#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è“è‰²åœ†å½¢æŠ“å–è„šæœ¬
=====================================
åŠŸèƒ½:
  1. æ£€æµ‹ç›´å¾„5cmçš„è“è‰²åœ†å½¢ï¼ˆè¿åŠ¨å½¢å˜é²æ£’ï¼‰
  2. ä½¿ç”¨PnPè®¡ç®—åœ†å½¢åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®
  3. æ§åˆ¶æœºæ¢°è‡‚æœ«ç«¯é è¿‘è“è‰²åœ†å½¢è¿›è¡ŒæŠ“å–

ä½¿ç”¨æ–¹æ³•:
  python vision/track_blue_circle.py
"""

import sys
import os
import cv2
import numpy as np
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof_gripper


class BlueCircleTracker:
    """è“è‰²åœ†å½¢è·Ÿè¸ªå™¨ï¼ˆè¿åŠ¨å½¢å˜é²æ£’ï¼‰"""
    
    def __init__(self, circle_diameter=0.05, intrinsic_file=None):
        """
        Parameters
        ----------
        circle_diameter : float
            åœ†å½¢ç›´å¾„ (ç±³)ï¼Œé»˜è®¤ 5cm
        intrinsic_file : str
            ç›¸æœºå†…å‚æ–‡ä»¶è·¯å¾„
        """
        self.circle_diameter = circle_diameter
        self.circle_radius = circle_diameter / 2.0
        
        # åŠ è½½ç›¸æœºå†…å‚
        if intrinsic_file is None:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            intrinsic_file = os.path.join(script_dir, 'camera_intrinsics.yaml')
        self.load_camera_intrinsics(intrinsic_file)
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ
        self.load_handeye_calibration()
        
        # åˆå§‹åŒ–æœºå™¨äºº
        self.robot = None
        self.controller = None
        
        # HSV è“è‰²èŒƒå›´
        self.hsv_lower1 = np.array([100, 80, 80])
        self.hsv_upper1 = np.array([120, 255, 255])
        self.hsv_lower2 = None
        self.hsv_upper2 = None
        
        print("="*60)
        print("ğŸ”µ è“è‰²åœ†å½¢æŠ“å–ç³»ç»Ÿï¼ˆè¿åŠ¨å½¢å˜é²æ£’ï¼‰")
        print("="*60)
        print(f"åœ†å½¢ç›´å¾„: {circle_diameter*1000:.0f} mm")
        print("="*60)
    
    def load_camera_intrinsics(self, yaml_path):
        """åŠ è½½ç›¸æœºå†…å‚"""
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
        
        fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
        self.K = fs.getNode('K').mat()
        self.dist = fs.getNode('distCoeffs').mat().flatten()
        fs.release()
        
        # ç„¦è·ä¿®æ­£
        correction_factor = 67/70
        self.K[0, 0] *= correction_factor
        self.K[1, 1] *= correction_factor
        
        print(f"ğŸ“· å·²åŠ è½½ç›¸æœºå†…å‚")
        print(f"   fx={self.K[0,0]:.1f}, fy={self.K[1,1]:.1f}")
    
    def load_handeye_calibration(self):
        """åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ"""
        calib_file = os.path.join(os.path.dirname(__file__), 'handeye_result.npy')
        if not os.path.exists(calib_file):
            print("âš ï¸ æœªæ‰¾åˆ°æ‰‹çœ¼æ ‡å®šæ–‡ä»¶ï¼Œå°†åªæ˜¾ç¤ºæ£€æµ‹ç»“æœ")
            self.T_cam_gripper = None
        else:
            self.T_cam_gripper = np.load(calib_file)
            print("âœ… å·²åŠ è½½æ‰‹çœ¼æ ‡å®šå‚æ•°")
    
    def init_robot(self, port="/dev/right_arm", baudrate=1_000_000):
        """åˆå§‹åŒ–æœºå™¨äºº"""
        print("\nğŸ¤– åˆå§‹åŒ–æœºå™¨äºº...")
        
        self.controller = ServoController(
            port=port, 
            baudrate=baudrate, 
            config_path=os.path.join(os.path.dirname(__file__), "../driver/servo_config.json")
        )
        self.robot = create_so101_5dof_gripper()
        self.robot.set_servo_controller(self.controller)
        
        print("âœ… æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def detect_blue_circle(self, frame):
        """
        æ£€æµ‹è“è‰²åœ†å½¢ï¼ˆå¢å¼ºé²æ£’æ€§ï¼Œæ”¯æŒå½¢å˜ï¼‰
        
        ä½¿ç”¨ä¸¤ç§æ–¹æ³•ï¼š
        1. éœå¤«åœ†æ£€æµ‹ï¼ˆä¼˜å…ˆï¼‰
        2. è½®å»“æ£€æµ‹+æ¤­åœ†æ‹Ÿåˆï¼ˆå¤‡é€‰ï¼Œå¤„ç†å½¢å˜ï¼‰
        
        Returns
        -------
        success : bool
            æ˜¯å¦æ£€æµ‹åˆ°åœ†å½¢
        center : tuple
            åœ†å¿ƒåƒç´ åæ ‡ (u, v)
        radius : float
            åœ†å½¢åƒç´ åŠå¾„ï¼ˆæ¤­åœ†æ—¶å–å¹³å‡ï¼‰
        mask : np.ndarray
            è“è‰²æ©ç å›¾åƒ
        """
        # è½¬æ¢åˆ°HSV
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # ç»¿è‰²æ©ç 
        mask = cv2.inRange(hsv, self.hsv_lower1, self.hsv_upper1)
        
        # å½¢æ€å­¦å¤„ç† - æ›´å¼ºçš„é™å™ª
        kernel = np.ones((7, 7), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        
        # æ–¹æ³•1: éœå¤«åœ†æ£€æµ‹ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼å‚æ•°ï¼‰
        blurred = cv2.GaussianBlur(mask, (9, 9), 2)
        circles = cv2.HoughCircles(
            blurred,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=50,
            param1=50,
            param2=30,       # æé«˜é˜ˆå€¼ï¼Œåªæ£€æµ‹æ˜æ˜¾çš„åœ†
            minRadius=15,
            maxRadius=250
        )
        
        if circles is not None:
            circles = np.round(circles[0, :]).astype(int)
            best_circle = max(circles, key=lambda c: c[2])
            center = (best_circle[0], best_circle[1])
            radius = best_circle[2]
            return True, center, radius, mask
        
        # æ–¹æ³•2: ä¸¥æ ¼çš„è½®å»“æ‹Ÿåˆåœ†å½¢
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return False, None, None, mask
        
        # æ‰¾æœ€å¤§è½®å»“
        largest_contour = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        
        # é¢ç§¯è¿‡æ»¤ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if area < 800:  # å¢åŠ æœ€å°é¢ç§¯è¦æ±‚
            return False, None, None, mask
        
        # è®¡ç®—åœ†åº¦ï¼ˆåˆ¤æ–­æ˜¯å¦æ¥è¿‘åœ†å½¢ï¼‰
        perimeter = cv2.arcLength(largest_contour, True)
        if perimeter > 0:
            circularity = 4 * np.pi * area / (perimeter * perimeter)
        else:
            circularity = 0
        
        # æ›´ä¸¥æ ¼çš„åœ†åº¦è¦æ±‚ï¼ˆ0.7 è¡¨ç¤ºæ¥è¿‘åœ†å½¢ï¼‰
        if circularity > 0.7:
            # å°è¯•åœ†æ‹Ÿåˆï¼ˆè€Œä¸æ˜¯æ¤­åœ†ï¼‰
            if len(largest_contour) >= 5:
                # ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ‹Ÿåˆåœ†
                (cx, cy), radius = cv2.minEnclosingCircle(largest_contour)
                center = (int(cx), int(cy))
                radius = int(radius)
                
                # éªŒè¯æ‹Ÿåˆè´¨é‡ï¼šè®¡ç®—è½®å»“ç‚¹åˆ°åœ†çš„è·ç¦»åå·®
                points = largest_contour.reshape(-1, 2).astype(np.float32)
                distances = np.abs(np.linalg.norm(points - np.array([cx, cy]), axis=1) - radius)
                mean_error = np.mean(distances)
                std_error = np.std(distances)
                
                # åªæœ‰å½“æ‹Ÿåˆè¯¯å·®è¾ƒå°æ—¶æ‰æ¥å—
                if mean_error < radius * 0.15 and std_error < radius * 0.2:  # è¯¯å·®åœ¨15%ä»¥å†…
                    return True, center, radius, mask
        
        # ä¸ç¬¦åˆä¸¥æ ¼æ¡ä»¶ï¼Œæ‹’ç»
        return False, None, None, mask
    
    def estimate_pose_from_circle(self, center, radius_px):
        """
        ä»åœ†å½¢çš„åƒç´ åæ ‡å’ŒåŠå¾„ä¼°ç®—3Dä½ç½®
        
        ä½¿ç”¨é’ˆå­”ç›¸æœºæ¨¡å‹ï¼š
        - å·²çŸ¥åœ†å½¢å®é™…åŠå¾„ R (ç±³)
        - å·²çŸ¥åƒç´ åŠå¾„ r (åƒç´ )
        - ç„¦è· f (åƒç´ )
        - æ·±åº¦ Z = f * R / r
        
        Returns
        -------
        pos_cam : np.ndarray
            åœ†å¿ƒåœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„3Dä½ç½® [X, Y, Z]
        """
        fx = self.K[0, 0]
        fy = self.K[1, 1]
        cx = self.K[0, 2]
        cy = self.K[1, 2]
        
        # ä¼°ç®—æ·±åº¦ (ä½¿ç”¨å¹³å‡ç„¦è·)
        f = (fx + fy) / 2
        Z = f * self.circle_radius / radius_px
        
        # åæŠ•å½±å¾—åˆ°3Dåæ ‡
        u, v = center
        X = (u - cx) * Z / fx
        Y = (v - cy) * Z / fy
        
        return np.array([X, Y, Z])
    
    def read_robot_pose(self, verbose=False):
        """è¯»å–æœºå™¨äººå½“å‰æœ«ç«¯ä½å§¿"""
        q = self.robot.read_joint_angles(
            joint_names=self.robot.joint_names,
            verbose=verbose
        )
        T_gripper_base = self.robot.fkine(q)
        return T_gripper_base, q
    
    def run(self):
        """è¿è¡Œè·Ÿè¸ªå¾ªç¯"""
        if not self.init_robot():
            return
        
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        print("\nğŸ® æ§åˆ¶è¯´æ˜:")
        print("  'f' - å¼€å¯/å…³é—­ è·Ÿéšæ¨¡å¼")
        print("  'h' - æœºæ¢°è‡‚å›ä¸­")
        print("  'q' - é€€å‡º")
        print("="*60)
        
        following = False
        gain = 0.8             # é«˜å¢ç›Šå¿«é€Ÿè·Ÿè¸ª
        step_limit = 0.080     # 80mm æ¯æ­¥æœ€å¤§ç§»åŠ¨
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                display = frame.copy()
                
                # æ£€æµ‹è“è‰²åœ†å½¢
                success, center, radius_px, mask = self.detect_blue_circle(frame)
                
                if success:
                    # ç»˜åˆ¶æ£€æµ‹ç»“æœï¼ˆç»¿è‰²åœ†åœˆï¼‰
                    cv2.circle(display, center, radius_px, (0, 255, 0), 2)
                    cv2.circle(display, center, 3, (0, 255, 255), -1)
                    
                    # ä¼°ç®—3Dä½ç½®
                    pos_cam = self.estimate_pose_from_circle(center, radius_px)
                    pos_cam_mm = pos_cam * 1000
                    
                    cv2.putText(display, f"Circle (Cam): [{pos_cam_mm[0]:.0f}, {pos_cam_mm[1]:.0f}, {pos_cam_mm[2]:.0f}] mm", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(display, f"Distance: {pos_cam[2]*1000:.0f} mm", 
                               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                    
                    if following and self.T_cam_gripper is not None:
                        # è·å–æœºæ¢°è‡‚ä½å§¿
                        T_gripper_base, q_curr = self.read_robot_pose(verbose=False)
                        
                        # è®¡ç®—ç›®æ ‡åœ¨Baseåæ ‡ç³»ä¸‹çš„ä½ç½®
                        # ^B p = ^B T_G @ ^G T_C @ ^C p
                        pos_cam_homo = np.array([pos_cam[0], pos_cam[1], pos_cam[2], 1.0])
                        T_base_cam = T_gripper_base @ self.T_cam_gripper
                        pos_target_base = (T_base_cam @ pos_cam_homo)[:3]
                        
                        pos_gripper_base = T_gripper_base[:3, 3]
                        
                        # ç›´æ¥å‘ç›®æ ‡ä½ç½®é è¿‘ï¼ˆæŠ“å–æ¨¡å¼ï¼‰
                        error_base = pos_target_base - pos_gripper_base
                        
                        # æ„é€ æ§åˆ¶é‡ - å…¨éƒ¨è½´éƒ½å‘ç›®æ ‡é è¿‘
                        delta_base = error_base * gain
                        
                        # é™å¹…
                        norm_delta = np.linalg.norm(delta_base)
                        if norm_delta > step_limit:
                            delta_base = delta_base / norm_delta * step_limit
                        
                        # è®¡ç®—ç›®æ ‡ä½ç½®
                        pos_gripper_des = pos_gripper_base + delta_base
                        
                        # æ˜¾ç¤ºä¿¡æ¯
                        pos_target_mm = pos_target_base * 1000
                        pos_curr_mm = pos_gripper_base * 1000  # å½“å‰ä½ç½®
                        pos_dest_mm = pos_gripper_des * 1000   # å³å°†åˆ°è¾¾çš„ç›®æ ‡ä½ç½®
                        error_mm = error_base * 1000
                        
                        cv2.putText(display, f"Target (Base): [{pos_target_mm[0]:.0f}, {pos_target_mm[1]:.0f}, {pos_target_mm[2]:.0f}] mm", 
                                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                        cv2.putText(display, f"Curr: [{pos_curr_mm[0]:.0f}, {pos_curr_mm[1]:.0f}, {pos_curr_mm[2]:.0f}] -> Dest: [{pos_dest_mm[0]:.0f}, {pos_dest_mm[1]:.0f}, {pos_dest_mm[2]:.0f}]", 
                                   (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
                        
                        # æ—¥å¿—è¾“å‡º
                        print(f"Target(Base): [{pos_target_mm[0]:7.1f}, {pos_target_mm[1]:7.1f}, {pos_target_mm[2]:7.1f}] | " +
                              f"Curr: [{pos_curr_mm[0]:7.1f}, {pos_curr_mm[1]:7.1f}, {pos_curr_mm[2]:7.1f}] | " +
                              f"Dest: [{pos_dest_mm[0]:7.1f}, {pos_dest_mm[1]:7.1f}, {pos_dest_mm[2]:7.1f}] | " +
                              f"Error: [{error_mm[0]:7.1f}, {error_mm[1]:7.1f}, {error_mm[2]:7.1f}]")
                        
                        # IKæ±‚è§£å¹¶æ‰§è¡Œ
                        R_gripper_des = T_gripper_base[:3, :3]
                        T_gripper_base_des = np.eye(4)
                        T_gripper_base_des[:3, :3] = R_gripper_des
                        T_gripper_base_des[:3, 3] = pos_gripper_des
                        
                        ik_res = self.robot.ikine_LM(
                            T_gripper_base_des, 
                            q0=q_curr,
                            ilimit=100,
                            slimit=3,
                            tol=1e-3,
                            mask=np.array([1, 1, 1, 0.5, 0.5, 0]),
                            k=0.1,
                            method="sugihara"
                        )
                        
                        if ik_res.success:
                            q_new = ik_res.q
                            diff = np.linalg.norm(q_new - q_curr)
                            if diff < 1.5:
                                targets = self.robot.q_to_servo_targets(q_new)
                                self.controller.fast_move_to_pose(targets, speed=200)
                                cv2.putText(display, "TRACKING", (10, 150), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                            else:
                                cv2.putText(display, f"Move too large: {diff:.2f}", (10, 150), 
                                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                        else:
                            cv2.putText(display, "IK Failed", (10, 150), 
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                else:
                    cv2.putText(display, "No blue circle detected", (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # çŠ¶æ€æ˜¾ç¤º
                status = "FOLLOW ON" if following else "FOLLOW OFF (Press 'f')"
                color = (0, 255, 0) if following else (0, 255, 255)
                cv2.putText(display, status, (10, display.shape[0]-30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
                
                # æ˜¾ç¤ºmask (è°ƒè¯•ç”¨)
                mask_small = cv2.resize(mask, (320, 180))
                mask_bgr = cv2.cvtColor(mask_small, cv2.COLOR_GRAY2BGR)
                display[0:180, display.shape[1]-320:display.shape[1]] = mask_bgr
                
                cv2.imshow("Blue Circle Tracking", display)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('f'):
                    following = not following
                    print(f"Follow mode: {following}")
                elif key == ord('h'):
                    print("Returning to home...")
                    self.controller.move_all_home()
                    self.controller.move_servo("gripper", 3050)
                    self.controller.move_servo("wrist_roll", 850)
                    following = False
                    
        except KeyboardInterrupt:
            pass
        finally:
            cap.release()
            cv2.destroyAllWindows()


def main():
    tracker = BlueCircleTracker(circle_diameter=0.05)  # 5cmç›´å¾„
    tracker.run()


if __name__ == "__main__":
    main()
