#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœ¼åœ¨æ‰‹å¤– (Eye-to-Hand) æ‰‹çœ¼æ ‡å®šè„šæœ¬
=====================================
åŠŸèƒ½:
  1. æ§åˆ¶æœºæ¢°è‡‚ç§»åŠ¨åˆ°ä¸åŒä½å§¿
  2. åœ¨æ¯ä¸ªä½å§¿é‡‡é›†å›¾åƒå¹¶æ£€æµ‹æ£‹ç›˜æ ¼
  3. ä½¿ç”¨PnPè®¡ç®—æ£‹ç›˜æ ¼åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿ (T_target_cam)
  4. ä½¿ç”¨æ­£è¿åŠ¨å­¦è®¡ç®—æœ«ç«¯åœ¨åŸºåº§æ ‡ç³»ä¸‹çš„ä½å§¿ (T_gripper_base)
  5. ä½¿ç”¨æ‰‹çœ¼æ ‡å®šç®—æ³•æ±‚è§£ç›¸æœºåˆ°åŸºåº§çš„å˜æ¢çŸ©é˜µ (T_cam_base)

ä½¿ç”¨æ–¹æ³•:
  python handeye_calibration_eyetohand.py --collect --camera 0 --port /dev/left_arm --output-dir ./calib_data
  python handeye_calibration_eyetohand.py --calibrate --output-dir ./calib_data
  python handeye_calibration_eyetohand.py --all --camera 0 --port /dev/left_arm --output-dir ./calib_data

åæ ‡ç³»å®šä¹‰:
  - base: æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
  - gripper/end-effector: æœºæ¢°è‡‚æœ«ç«¯åæ ‡ç³»
  - cam: ç›¸æœºåæ ‡ç³» (å›ºå®šåœ¨ç¯å¢ƒä¸­)
  - target: æ ‡å®šæ¿åæ ‡ç³»

çœ¼åœ¨æ‰‹å¤–æ–¹ç¨‹: AX = YB
  - A: ç›¸é‚»ä¸¤ä¸ªæœ«ç«¯ä½å§¿çš„ç›¸å¯¹å˜æ¢
  - B: ç›¸é‚»ä¸¤ä¸ªæ ‡å®šæ¿ä½å§¿çš„ç›¸å¯¹å˜æ¢
  - X: ç›¸æœºç›¸å¯¹äºåŸºåº§çš„å˜æ¢ (T_cam_base) - å¾…æ±‚
  - Y: æœ«ç«¯ç›¸å¯¹äºåŸºåº§çš„å˜æ¢ (T_gripper_base)
"""

import os
import sys
import cv2
import numpy as np
import time
import argparse
from datetime import datetime
from scipy.spatial.transform import Rotation as R

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof_gripper


class EyeToHandCalibrator:
    """çœ¼åœ¨æ‰‹å¤– (Eye-to-Hand) æ‰‹çœ¼æ ‡å®šå™¨

    ç»“æ„å¯¹é½ `handeye_calibration_eyeinhand.py`ï¼š
    - __init__ ä»…è´Ÿè´£å‚æ•°/å†…å‚/æ£‹ç›˜ç‚¹å‡†å¤‡
    - init_robot/collect_data_interactive è´Ÿè´£ç¡¬ä»¶ä¸é‡‡é›†
    - load_collected_data/calibrate/evaluate/save_result æä¾›ç¦»çº¿æµç¨‹
    """

    def __init__(
        self,
        board_size=(7, 5),
        square_size=0.018,  # meters
        intrinsic_file="./config_data/camera_intrinsics_environment.yaml",
        output_dir="./handeye_data_environment",
    ):
        self.board_size = board_size
        self.square_size = square_size
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # ç›¸æœºå†…å‚
        self.K = None
        self.dist = None
        self.load_camera_intrinsics(intrinsic_file)

        # æ£‹ç›˜æ ¼ 3D ç‚¹
        self.objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)
        self.objp[:, :2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1, 2)
        self.objp *= square_size

        # æœºå™¨äºº
        self.robot = None
        self.controller = None

        # é‡‡é›†æ•°æ®
        self.T_target_cam_list = []
        self.T_gripper_base_list = []
        self.images = []

        # PnP ç¨³å®šæ€§
        self.pose_buffer = []
        self.pose_buffer_size = 5

        print("=" * 70)
        print("ğŸ¤– çœ¼åœ¨æ‰‹å¤– (Eye-to-Hand) æ‰‹çœ¼æ ‡å®šå·¥å…·")
        print("=" * 70)
        print("\næ£‹ç›˜æ ¼å‚æ•°:")
        print(f"  å†…è§’ç‚¹: {board_size[0]} Ã— {board_size[1]}")
        print(f"  æ–¹æ ¼å¤§å°: {square_size * 1000:.2f} mm")
        print(f"\næ•°æ®ä¿å­˜ç›®å½•: {os.path.abspath(output_dir)}")
        print("=" * 70)
    
    def load_camera_intrinsics(self, yaml_path):
        """åŠ è½½ç›¸æœºå†…å‚ - ä»…è¯»å– K ä¸ distCoeffs (ä¸ Eye-in-Hand å¯¹é½)"""
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")

        fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
        if not fs.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")

        K_node = fs.getNode("K")
        if K_node.empty():
            K_node = fs.getNode("camera_matrix")

        dist_node = fs.getNode("distCoeffs")
        if dist_node.empty():
            dist_node = fs.getNode("distortion_coefficients")

        self.K = None if K_node.empty() else K_node.mat()
        self.dist = None if dist_node.empty() else dist_node.mat().flatten()
        fs.release()

        if self.K is None or self.dist is None:
            raise ValueError(f"ç›¸æœºå†…å‚æ–‡ä»¶ç¼ºå°‘ K/distCoeffs: {yaml_path}")

        print(f"\nğŸ“· å·²åŠ è½½ç›¸æœºå†…å‚: {yaml_path}")
        print(f"   fx={self.K[0, 0]:.1f}, fy={self.K[1, 1]:.1f}")
        print(f"   cx={self.K[0, 2]:.1f}, cy={self.K[1, 2]:.1f}")
        print(f"   æ£‹ç›˜æ ¼å°ºå¯¸: {self.board_size} (ç”±è„šæœ¬è®¾å®š)")
        print(f"   æ–¹æ ¼å¤§å°: {self.square_size * 1000:.2f} mm (ç”±è„šæœ¬è®¾å®š)")

    def init_robot(self, port="/dev/left_arm", baudrate=1_000_000):
        """åˆå§‹åŒ–æœºå™¨äººå’Œæ§åˆ¶å™¨"""
        print("\nğŸ¤– åˆå§‹åŒ–æœºå™¨äºº...")
        self.controller = ServoController(
            port=port,
            baudrate=baudrate,
            config_path=os.path.join(os.path.dirname(__file__), "../driver/servo_config.json"),
        )
        self.robot = create_so101_5dof_gripper()
        self.robot.set_servo_controller(self.controller)
        print("âœ… æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        return True

    def read_robot_pose(self, verbose=True):
        """è¯»å–æœºå™¨äººå½“å‰æœ«ç«¯ä½å§¿"""
        q = self.robot.read_joint_angles(joint_names=self.robot.joint_names, verbose=verbose)
        T_gripper_base = self.robot.fkine(q)

        if verbose:
            pos = T_gripper_base[:3, 3]
            euler = R.from_matrix(T_gripper_base[:3, :3]).as_euler("xyz", degrees=True)
            print("\nğŸ“ æœ«ç«¯ä½å§¿:")
            print(
                f"   ä½ç½®: x={pos[0] * 1000:.1f}mm, y={pos[1] * 1000:.1f}mm, z={pos[2] * 1000:.1f}mm"
            )
            print(
                f"   å§¿æ€: roll={euler[0]:.1f}Â°, pitch={euler[1]:.1f}Â°, yaw={euler[2]:.1f}Â°"
            )

        return T_gripper_base, q
    
    def detect_chessboard(self, frame, use_ransac=True, refine_pose=True):
        """æ£€æµ‹æ£‹ç›˜æ ¼å¹¶è®¡ç®—å…¶åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿ (ä¸ Eye-in-Hand å¯¹é½çš„é²æ£’ç‰ˆæœ¬)"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK
        found, corners = cv2.findChessboardCorners(gray, self.board_size, flags)
        if not found:
            return False, None, None, float("inf")

        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001)
        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
        imgp = corners.reshape(-1, 2).astype(np.float32)

        if use_ransac:
            success, rvec, tvec, inliers = cv2.solvePnPRansac(
                self.objp,
                imgp,
                self.K,
                self.dist,
                iterationsCount=1000,
                reprojectionError=2.0,
                flags=cv2.SOLVEPNP_ITERATIVE,
            )
            if inliers is not None and len(inliers) < len(self.objp) * 0.8:
                return False, None, corners, float("inf")
        else:
            success, rvec, tvec = cv2.solvePnP(
                self.objp,
                imgp,
                self.K,
                self.dist,
                flags=cv2.SOLVEPNP_ITERATIVE,
            )

        if not success:
            return False, None, corners, float("inf")

        if refine_pose:
            rvec, tvec = cv2.solvePnPRefineLM(self.objp, imgp, self.K, self.dist, rvec, tvec)

        reproj_pts, _ = cv2.projectPoints(self.objp, rvec, tvec, self.K, self.dist)
        reproj_error = np.sqrt(
            np.mean(np.sum((imgp - reproj_pts.reshape(-1, 2)) ** 2, axis=1))
        )
        if reproj_error > 2.0:
            return False, None, corners, reproj_error

        R_mat, _ = cv2.Rodrigues(rvec)
        T_target_cam = np.eye(4)
        T_target_cam[:3, :3] = R_mat
        T_target_cam[:3, 3] = tvec.squeeze()
        return True, T_target_cam, corners, reproj_error

    def update_pose_buffer(self, T):
        self.pose_buffer.append(T.copy())
        if len(self.pose_buffer) > self.pose_buffer_size:
            self.pose_buffer.pop(0)

    def get_averaged_pose(self):
        if len(self.pose_buffer) < 3:
            return None

        translations = np.array([T[:3, 3] for T in self.pose_buffer])
        t_avg = np.mean(translations, axis=0)

        quats = np.array([R.from_matrix(T[:3, :3]).as_quat() for T in self.pose_buffer])
        q_avg = np.mean(quats, axis=0)
        q_avg /= np.linalg.norm(q_avg)
        R_avg = R.from_quat(q_avg).as_matrix()

        T_avg = np.eye(4)
        T_avg[:3, :3] = R_avg
        T_avg[:3, 3] = t_avg
        return T_avg
    
    def collect_data_interactive(self, cam_id=0, width=1280, height=720):
        """äº¤äº’å¼é‡‡é›†æ ‡å®šæ•°æ® (ä¸ Eye-in-Hand å¯¹é½)

        æŒ‰é”®:
          SPACE - é‡‡é›†å½“å‰ä½å§¿
          'h'   - æœºæ¢°è‡‚å›ä¸­
          's'   - æ˜¾ç¤º/éšè—ç¨³å®šæ€§ä¿¡æ¯
          'q'   - é€€å‡ºé‡‡é›†
        """

        # æ¯æ¬¡é‡‡é›†åˆ›å»ºä¸€ä¸ªæ–°çš„ session ç›®å½•ï¼ˆä¸æ—§ç‰ˆä¸€è‡´ï¼‰
        session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = os.path.join(self.output_dir, f"session_{session_timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        self.session_dir = session_dir

        print(f"ğŸ“‚ æœ¬æ¬¡é‡‡é›† session: {os.path.abspath(session_dir)}")

        cap = cv2.VideoCapture(cam_id)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, int(width))
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, int(height))
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº")
            return False

        print("\nğŸ“¸ å¼€å§‹äº¤äº’å¼æ•°æ®é‡‡é›†")
        print("=" * 70)
        print("\nâŒ¨ï¸  å¿«æ·é”®:")
        print("   SPACE - é‡‡é›†å½“å‰ä½å§¿æ•°æ®")
        print("   'h'   - æœºæ¢°è‡‚å›ä¸­ä½")
        print("   's'   - æ˜¾ç¤º/éšè—ç¨³å®šæ€§ä¿¡æ¯")
        print("   'q'   - é€€å‡ºé‡‡é›†")
        print("\nğŸ“– é‡‡é›†æŒ‡å—:")
        print("   1. æ‰‹åŠ¨ç§»åŠ¨æœºæ¢°è‡‚åˆ°ä¸åŒä½å§¿")
        print("   2. ç¡®ä¿æ£‹ç›˜æ ¼åœ¨ç›¸æœºè§†é‡å†…")
        print("   3. ç­‰å¾…ä½å§¿ç¨³å®š(ç»¿è‰²)åæŒ‰SPACEé‡‡é›†")
        print("   4. å»ºè®®é‡‡é›† 10-20 ç»„æ•°æ®")
        print("   5. å°½é‡è®©æœºæ¢°è‡‚å§¿æ€å¤šæ ·åŒ–")
        print("=" * 70 + "\n")

        sample_count = 0
        show_stability = True
        self.pose_buffer = []

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            display = frame.copy()
            success, T_target_cam, corners, reproj_error = self.detect_chessboard(frame)

            is_stable = False
            stability_info = ""

            if success and corners is not None:
                cv2.drawChessboardCorners(display, self.board_size, corners, True)

                self.update_pose_buffer(T_target_cam)
                if len(self.pose_buffer) >= 3:
                    translations = np.array([T[:3, 3] for T in self.pose_buffer])
                    t_std = np.std(translations, axis=0) * 1000
                    t_std_norm = np.linalg.norm(t_std)
                    is_stable = t_std_norm < 3.0 and reproj_error < 1.0
                    if show_stability:
                        stability_info = f"Std: {t_std_norm:.1f}mm, ReprojErr: {reproj_error:.2f}px"

                distance = np.linalg.norm(T_target_cam[:3, 3]) * 1000
                color = (0, 255, 0) if is_stable else (0, 255, 255)
                status_text = "STABLE - Press SPACE" if is_stable else "Detecting..."
                cv2.putText(
                    display,
                    f"Distance: {distance:.0f}mm",
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    color,
                    2,
                )
                cv2.putText(
                    display,
                    status_text,
                    (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.7,
                    color,
                    2,
                )
                if show_stability and stability_info:
                    cv2.putText(
                        display,
                        stability_info,
                        (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,
                        (255, 255, 255),
                        2,
                    )
            else:
                cv2.putText(
                    display,
                    "Chessboard NOT FOUND",
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 0, 255),
                    2,
                )
                self.pose_buffer = []

            cv2.putText(
                display,
                f"Samples: {sample_count}",
                (display.shape[1] - 180, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                (255, 255, 255),
                2,
            )

            cv2.imshow("Hand-Eye Calibration (Eye-to-Hand)", display)
            key = cv2.waitKey(1) & 0xFF

            if key == ord("q"):
                print("\nğŸ‘‹ é€€å‡ºé‡‡é›†")
                break
            if key == ord("h"):
                print("\nğŸ  æœºæ¢°è‡‚å›ä¸­...")
                self.controller.move_all_home()
                time.sleep(1)
                continue
            if key == ord("s"):
                show_stability = not show_stability
                print(f"{'æ˜¾ç¤º' if show_stability else 'éšè—'}ç¨³å®šæ€§ä¿¡æ¯")
                continue
            if key == ord(" "):
                if not success:
                    print("âš ï¸  æœªæ£€æµ‹åˆ°æ£‹ç›˜æ ¼ï¼Œæ— æ³•é‡‡é›†")
                    continue
                if not is_stable:
                    print("âš ï¸  ä½å§¿ä¸ç¨³å®šï¼Œå»ºè®®ç­‰å¾…ç¨³å®šåå†é‡‡é›†")

                T_avg = self.get_averaged_pose()
                if T_avg is not None:
                    T_to_save = T_avg
                    print("   ä½¿ç”¨å¹³å‡ä½å§¿")
                else:
                    T_to_save = T_target_cam
                    print("   ä½¿ç”¨å•å¸§ä½å§¿")

                sample_count += 1
                print(f"\nğŸ“¸ é‡‡é›†æ•°æ® #{sample_count}")
                T_gripper_base, q = self.read_robot_pose(verbose=True)

                self.T_target_cam_list.append(T_to_save.copy())
                self.T_gripper_base_list.append(T_gripper_base.copy())
                self.images.append(frame.copy())

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                np.savez(
                    os.path.join(session_dir, f"pose_{sample_count:03d}.npz"),
                    T_target_cam=T_to_save,
                    T_gripper_base=T_gripper_base,
                    q=q,
                    reproj_error=reproj_error,
                )
                cv2.imwrite(
                    os.path.join(session_dir, f"image_{sample_count:03d}.jpg"),
                    frame,
                )

                # åŒæ—¶ä¿å­˜å¯è§†åŒ–å›¾ï¼Œä¾¿äºå›çœ‹
                cv2.imwrite(
                    os.path.join(session_dir, f"vis_{sample_count:03d}.jpg"),
                    display,
                )

                print(f"âœ… å·²ä¿å­˜æ•°æ® #{sample_count}")
                print(f"   æ ‡å®šæ¿è·ç¦»: {np.linalg.norm(T_to_save[:3, 3]) * 1000:.1f} mm")
                print(f"   é‡æŠ•å½±è¯¯å·®: {reproj_error:.2f} px")

                self.pose_buffer = []

        cap.release()
        cv2.destroyAllWindows()
        print(f"\nğŸ“Š å…±é‡‡é›† {sample_count} ç»„æ•°æ®")
        return sample_count >= 3
    
    def load_collected_data(self, session_dir=None):
        """ä»æ–‡ä»¶åŠ è½½å·²é‡‡é›†çš„æ•°æ® (ä¼˜å…ˆæœ€æ–° session_*/pose_*.npz)

        Parameters
        ----------
        session_dir : str | None
            æŒ‡å®š session ç›®å½•ï¼›ä¸º None æ—¶è‡ªåŠ¨é€‰æ‹©æœ€æ–° session_*/ï¼Œè‹¥ä¸å­˜åœ¨åˆ™å›é€€ output_dir æ ¹ç›®å½•ã€‚
        """
        import glob

        base_dir = self.output_dir
        if session_dir is None:
            session_dirs = sorted(glob.glob(os.path.join(self.output_dir, "session_*")))
            if session_dirs:
                base_dir = session_dirs[-1]
        else:
            base_dir = session_dir

        pose_files = sorted(glob.glob(os.path.join(base_dir, "pose_*.npz")))
        if not pose_files:
            print(f"âŒ æœªæ‰¾åˆ°æ ‡å®šæ•°æ®: {base_dir}")
            return False

        self.T_target_cam_list = []
        self.T_gripper_base_list = []

        print(f"\nğŸ“‚ åŠ è½½æ ‡å®šæ•°æ®: {base_dir}")
        for f in pose_files:
            data = np.load(f)
            self.T_target_cam_list.append(data["T_target_cam"])
            self.T_gripper_base_list.append(data["T_gripper_base"])
            print(f"   âœ… {os.path.basename(f)}")

        print(f"\nå…±åŠ è½½ {len(self.T_target_cam_list)} ç»„æ•°æ®")
        return True
    
    def calibrate(self):
        """æ‰§è¡Œçœ¼åœ¨æ‰‹å¤–æ ‡å®šï¼Œè¿”å› T_cam_base"""
        if len(self.T_gripper_base_list) < 3 or len(self.T_target_cam_list) < 3:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ 3 ç»„æ•°æ®")
            return None

        print("\nğŸ”§ å¼€å§‹çœ¼åœ¨æ‰‹å¤–æ ‡å®š...")
        print(f"   æ•°æ®ç»„æ•°: {len(self.T_gripper_base_list)}")

        # å‡†å¤‡æ•°æ®
        R_gripper2base = []
        t_gripper2base = []
        R_base2gripper = []
        t_base2gripper = []

        R_target2cam = []
        t_target2cam = []
        R_cam2target = []
        t_cam2target = []

        for T_gb, T_tc in zip(self.T_gripper_base_list, self.T_target_cam_list):
            R_gripper2base.append(T_gb[:3, :3])
            t_gripper2base.append(T_gb[:3, 3])
            T_bg = np.linalg.inv(T_gb)
            R_base2gripper.append(T_bg[:3, :3])
            t_base2gripper.append(T_bg[:3, 3])

            R_target2cam.append(T_tc[:3, :3])
            t_target2cam.append(T_tc[:3, 3])
            T_ct = np.linalg.inv(T_tc)
            R_cam2target.append(T_ct[:3, :3])
            t_cam2target.append(T_ct[:3, 3])

        print("\nğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥:")
        self.analyze_data_quality(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam)

        strategies = [
            {
                "name": "Strategy 1: Base2Gripper + Target2Cam",
                "R_gripper": R_base2gripper,
                "t_gripper": t_base2gripper,
                "R_target": R_target2cam,
                "t_target": t_target2cam,
            },
            {
                "name": "Strategy 2: Gripper2Base + Cam2Target",
                "R_gripper": R_gripper2base,
                "t_gripper": t_gripper2base,
                "R_target": R_cam2target,
                "t_target": t_cam2target,
            },
            {
                "name": "Strategy 3: Target2Cam (as Robot) + Gripper2Base (as Target)",
                "R_gripper": R_target2cam,
                "t_gripper": t_target2cam,
                "R_target": R_gripper2base,
                "t_target": t_gripper2base,
            },
        ]

        methods = [
            (cv2.CALIB_HAND_EYE_TSAI, "Tsai-Lenz"),
            (cv2.CALIB_HAND_EYE_PARK, "Park"),
            (cv2.CALIB_HAND_EYE_HORAUD, "Horaud"),
            (cv2.CALIB_HAND_EYE_ANDREFF, "Andreff"),
            (cv2.CALIB_HAND_EYE_DANIILIDIS, "Daniilidis"),
        ]

        best_result = None
        best_score = float("inf")

        for strategy in strategies:
            print(f"\nğŸ”„ å°è¯•ç­–ç•¥: {strategy['name']}")
            for method, method_name in methods:
                try:
                    R_calib, t_calib = cv2.calibrateHandEye(
                        strategy["R_gripper"],
                        strategy["t_gripper"],
                        strategy["R_target"],
                        strategy["t_target"],
                        method=method,
                    )

                    error = self.evaluate_calibration(
                        R_calib,
                        t_calib,
                        R_gripper2base,
                        t_gripper2base,
                        R_target2cam,
                        t_target2cam,
                    )
                    print(f"   {method_name}: {error:.6f} mm")

                    if error < best_score and not (np.isnan(error) or np.isinf(error)):
                        best_score = error
                        best_result = (R_calib, t_calib, method_name, strategy["name"])

                except Exception as e:
                    print(f"   {method_name} å¤±è´¥: {e}")

        if best_result is None:
            print("âŒ æ‰€æœ‰æ ‡å®šç®—æ³•éƒ½å¤±è´¥äº†")
            return None

        print(f"\nğŸ”„ å°è¯•éçº¿æ€§ä¼˜åŒ– (åŸºäº {best_result[2]})...")
        try:
            R_opt, t_opt, error_opt = self.optimize_calibration(
                best_result[0],
                best_result[1],
                R_gripper2base,
                t_gripper2base,
                R_target2cam,
                t_target2cam,
            )
            print(f"   Optimization: {error_opt:.6f} mm")
            if error_opt < best_score:
                best_score = error_opt
                best_result = (R_opt, t_opt, "Optimization", "Non-linear Least Squares")
        except Exception as e:
            print(f"   ä¼˜åŒ–å¤±è´¥: {e}")

        R_cam2base, t_cam2base, best_method, best_strategy = best_result
        T_cam2base = np.eye(4)
        T_cam2base[:3, :3] = R_cam2base
        T_cam2base[:3, 3] = t_cam2base.flatten()

        # è‡ªåŠ¨åˆ¤åˆ«æ˜¯å¦éœ€è¦å–é€†
        try:
            score_direct = self.evaluate_calibration(
                T_cam2base[:3, :3],
                T_cam2base[:3, 3].reshape(3, 1),
                R_gripper2base,
                t_gripper2base,
                R_target2cam,
                t_target2cam,
            )
            T_inv = np.linalg.inv(T_cam2base)
            score_inv = self.evaluate_calibration(
                T_inv[:3, :3],
                T_inv[:3, 3].reshape(3, 1),
                R_gripper2base,
                t_gripper2base,
                R_target2cam,
                t_target2cam,
            )
            if np.isfinite(score_inv) and (score_inv + 1e-9) < score_direct:
                print(
                    f"\nâ„¹ï¸  æ£€æµ‹åˆ°ç»“æœå¯èƒ½ä¸ºé€†å˜æ¢ï¼šä¸€è‡´æ€§ {score_direct:.6f} -> {score_inv:.6f} mmï¼Œå·²è‡ªåŠ¨å–é€†"
                )
                T_cam2base = T_inv
                best_strategy = f"{best_strategy} (auto-inverted)"
                best_score = score_inv
            else:
                best_score = score_direct
        except Exception:
            pass

        print("\nâœ… çœ¼åœ¨æ‰‹å¤–æ ‡å®šå®Œæˆ!")
        print(f"   æœ€ä½³ç­–ç•¥: {best_strategy}")
        print(f"   æœ€ä½³ç®—æ³•: {best_method}")
        print(f"   ä¸€è‡´æ€§è¯¯å·®: {best_score:.6f} mm")
        print("\nğŸ¯ ç›¸æœºåˆ°åŸºåº§å˜æ¢çŸ©é˜µ (T_cam_base):")
        print("-" * 70)
        print(T_cam2base)
        print("-" * 70)

        return T_cam2base

    def optimize_calibration(self, R_init, t_init, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """ä½¿ç”¨éçº¿æ€§æœ€å°äºŒä¹˜ä¼˜åŒ–æ ‡å®šç»“æœ"""
        from scipy.optimize import least_squares
        
        # 1. åˆå§‹åŒ– T_cam_base (X)
        T_cam_base = np.eye(4)
        T_cam_base[:3, :3] = R_init
        T_cam_base[:3, 3] = t_init.flatten()
        
        # 2. åˆå§‹åŒ– T_gripper_target (Z)
        # Z = mean( inv(T_base_gripper) * inv(T_cam_base) * T_cam_target ) ?
        # No, T_cam_target = T_cam_base * T_base_gripper * T_gripper_target
        # So T_gripper_target = inv(T_base_gripper) * inv(T_cam_base) * T_cam_target
        # Wait, T_base_gripper is T_gripper_base (in my code variable name)
        # My code: T_gripper_base variable holds T_base_gripper (Pose of gripper in base)
        
        T_gripper_targets = []
        for i in range(len(R_gripper2base)):
            T_bg = np.eye(4)
            T_bg[:3, :3] = R_gripper2base[i]
            T_bg[:3, 3] = t_gripper2base[i]
            
            T_tc = np.eye(4)
            T_tc[:3, :3] = R_target2cam[i]
            T_tc[:3, 3] = t_target2cam[i]
            
            # T_gt = inv(T_bg) * inv(T_cb) * T_tc
            T_gt = np.linalg.inv(T_bg) @ np.linalg.inv(T_cam_base) @ T_tc
            T_gripper_targets.append(T_gt)
        
        # Average T_gripper_target
        # Simple averaging for translation, proper averaging for rotation
        t_gt_mean = np.mean([T[:3, 3] for T in T_gripper_targets], axis=0)
        R_gt_mean_obj = R.from_matrix([T[:3, :3] for T in T_gripper_targets]).mean()
        T_gripper_target = np.eye(4)
        T_gripper_target[:3, :3] = R_gt_mean_obj.as_matrix()
        T_gripper_target[:3, 3] = t_gt_mean
        
        # 3. Optimization parameters: [rx_X, ry_X, rz_X, tx_X, ty_X, tz_X, rx_Z, ry_Z, rz_Z, tx_Z, ty_Z, tz_Z]
        x0 = np.concatenate([
            R.from_matrix(T_cam_base[:3, :3]).as_rotvec(),
            T_cam_base[:3, 3],
            R.from_matrix(T_gripper_target[:3, :3]).as_rotvec(),
            T_gripper_target[:3, 3]
        ])
        
        def residuals(params):
            # Reconstruct X and Z
            r_X = params[0:3]
            t_X = params[3:6]
            r_Z = params[6:9]
            t_Z = params[9:12]
            
            T_X = np.eye(4)
            T_X[:3, :3] = R.from_rotvec(r_X).as_matrix()
            T_X[:3, 3] = t_X
            
            T_Z = np.eye(4)
            T_Z[:3, :3] = R.from_rotvec(r_Z).as_matrix()
            T_Z[:3, 3] = t_Z
            
            res = []
            for i in range(len(R_gripper2base)):
                T_bg = np.eye(4)
                T_bg[:3, :3] = R_gripper2base[i]
                T_bg[:3, 3] = t_gripper2base[i]
                
                T_tc_obs = np.eye(4)
                T_tc_obs[:3, :3] = R_target2cam[i]
                T_tc_obs[:3, 3] = t_target2cam[i]
                
                # Predicted T_tc = X * T_bg * Z
                T_tc_pred = T_X @ T_bg @ T_Z
                
                # Error in translation
                diff_t = T_tc_pred[:3, 3] - T_tc_obs[:3, 3]
                res.extend(diff_t)
                
                # Error in rotation (angle-axis)
                diff_R = T_tc_pred[:3, :3] @ T_tc_obs[:3, :3].T
                diff_r = R.from_matrix(diff_R).as_rotvec()
                res.extend(diff_r * 0.1) # Weight rotation less (approx 0.1m per radian)
            
            return np.array(res)
            
        res = least_squares(residuals, x0, verbose=0)
        
        # Extract optimized X
        params_opt = res.x
        r_X_opt = params_opt[0:3]
        t_X_opt = params_opt[3:6]
        
        R_opt = R.from_rotvec(r_X_opt).as_matrix()
        t_opt = t_X_opt.reshape(3, 1)
        
        # Calculate error
        error = self.evaluate_calibration(R_opt, t_opt, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam)
        
        return R_opt, t_opt, error

    
    def analyze_data_quality(self, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("   æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        # æ£€æŸ¥å¹³ç§»é‡å˜åŒ–
        translations = np.array(t_gripper2base)
        translation_range = np.ptp(translations, axis=0)
        print(f"   æœºå™¨äººå¹³ç§»èŒƒå›´: X={translation_range[0]:.3f}m, Y={translation_range[1]:.3f}m, Z={translation_range[2]:.3f}m")
        
        # æ£€æŸ¥æ—‹è½¬é‡å˜åŒ–
        rotations = []
        for rot_matrix in R_gripper2base:
            r = R.from_matrix(rot_matrix)
            rotations.append(r.as_euler('xyz', degrees=True))
        
        rotations = np.array(rotations)
        rotation_range = np.ptp(rotations, axis=0)
        print(f"   æœºå™¨äººæ—‹è½¬èŒƒå›´: Roll={rotation_range[0]:.1f}Â°, Pitch={rotation_range[1]:.1f}Â°, Yaw={rotation_range[2]:.1f}Â°")
        
        # å»ºè®®
        if np.any(translation_range < 0.05):
            print("   âš ï¸  å»ºè®®å¢åŠ å¹³ç§»å˜åŒ–é‡ (>5cm)")
        if np.any(rotation_range < 10):
            print("   âš ï¸  å»ºè®®å¢åŠ æ—‹è½¬å˜åŒ–é‡ (>10Â°)")
    
    def evaluate_calibration(self, R_cam2base, t_cam2base, 
                           R_gripper2base, t_gripper2base,
                           R_target2cam, t_target2cam):
        """è¯„ä¼°æ ‡å®šç»“æœ (ä½¿ç”¨ä¸€è‡´æ€§æ ‡å‡†å·®ä½œä¸ºæŒ‡æ ‡)"""
        # æ„å»º T_cam_base
        T_cam_base = np.eye(4)
        T_cam_base[:3, :3] = R_cam2base
        T_cam_base[:3, 3] = t_cam2base.flatten()
        
        # è®¡ç®—æ‰€æœ‰ä½å§¿ä¸‹çš„ T_target_gripper
        # T_target_gripper = inv(T_gripper_base) * T_cam_base * T_target_cam
        
        target_gripper_translations = []
        
        for i in range(len(R_gripper2base)):
            T_gripper_base = np.eye(4)
            T_gripper_base[:3, :3] = R_gripper2base[i]
            T_gripper_base[:3, 3] = t_gripper2base[i]
            
            T_target_cam = np.eye(4)
            T_target_cam[:3, :3] = R_target2cam[i]
            T_target_cam[:3, 3] = t_target2cam[i]
            
            T_target_gripper = np.linalg.inv(T_gripper_base) @ T_cam_base @ T_target_cam
            target_gripper_translations.append(T_target_gripper[:3, 3])
            
        # è®¡ç®—å¹³ç§»çš„æ ‡å‡†å·® (mm)
        translations = np.array(target_gripper_translations)
        std_dev = np.std(translations, axis=0)
        mean_std_dev = np.mean(std_dev) * 1000.0  # è½¬æ¢ä¸ºmm
        
        return mean_std_dev
    
    def save_result(self, T_cam_base, filename="handeye_result_envir.yaml"):
        """ä¿å­˜æ ‡å®šç»“æœ (ä¸ Eye-in-Hand é£æ ¼å¯¹é½)"""
        if T_cam_base is None:
            return

        filepath = os.path.join(self.output_dir, filename)
        fs = cv2.FileStorage(filepath, cv2.FILE_STORAGE_WRITE)
        fs.write("T_cam_base", T_cam_base)

        R_mat = T_cam_base[:3, :3]
        t_vec = T_cam_base[:3, 3]
        euler = R.from_matrix(R_mat).as_euler("xyz", degrees=True)
        quat = R.from_matrix(R_mat).as_quat()

        fs.write("rotation_matrix", R_mat)
        fs.write("translation_vector", t_vec.reshape(3, 1))
        fs.write("euler_angles_deg", np.array(euler).reshape(3, 1))
        fs.write("quaternion_xyzw", np.array(quat).reshape(4, 1))
        fs.write("calibration_date", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        fs.write("num_samples", len(self.T_target_cam_list))
        fs.release()

        print(f"\nğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜: {filepath}")

        npy_path = os.path.join(self.output_dir, "handeye_result_envir.npy")
        np.save(npy_path, T_cam_base)
        print(f"ğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜: {npy_path}")
    
    def evaluate_calibration_consistency(self, T_cam_base):
        """è¯„ä¼°æ ‡å®šç»“æœçš„ä¸€è‡´æ€§ (ä»¿ç…§ Eye-in-Hand)"""
        if T_cam_base is None:
            return
        
        print("\nğŸ“Š æ ‡å®šç»“æœä¸€è‡´æ€§è¯„ä¼°")
        print("="*70)
        
        errors = []
        
        # å¯¹äºçœ¼åœ¨æ‰‹å¤– (Eye-to-Hand)ï¼Œæ ‡å®šæ¿å›ºå®šåœ¨æœºæ¢°è‡‚æœ«ç«¯
        # å› æ­¤ T_target_gripper åº”è¯¥æ˜¯æ’å®šçš„
        # T_target_gripper = inv(T_gripper_base) * T_cam_base * T_target_cam
        
        T_target_grippers = []

        n = min(len(self.T_gripper_base_list), len(self.T_target_cam_list))
        for i in range(n):
            T_gb = self.T_gripper_base_list[i]
            T_tc = self.T_target_cam_list[i]
            
            # è®¡ç®— T_target_gripper
            T_tg = np.linalg.inv(T_gb) @ T_cam_base @ T_tc
            T_target_grippers.append(T_tg)
            
        # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„è¯¯å·®
        for i in range(len(T_target_grippers)):
            for j in range(i + 1, len(T_target_grippers)):
                T_tg1 = T_target_grippers[i]
                T_tg2 = T_target_grippers[j]
                
                # ç›¸å¯¹è¯¯å·® T_diff = T_tg1 * inv(T_tg2)
                T_diff = T_tg1 @ np.linalg.inv(T_tg2)
                
                error_trans = np.linalg.norm(T_diff[:3, 3]) * 1000  # mm
                error_rot = np.linalg.norm(R.from_matrix(T_diff[:3, :3]).as_rotvec()) * 180 / np.pi  # deg
                
                errors.append({
                    'pair': (i, j),
                    'trans_error': error_trans,
                    'rot_error': error_rot
                })
        
        if not errors:
            print("   æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè¯„ä¼°")
            return

        # ç»Ÿè®¡
        trans_errors = [e['trans_error'] for e in errors]
        rot_errors = [e['rot_error'] for e in errors]
        
        print(f"\nä¸€è‡´æ€§è¯¯å·® (æ ‡å®šæ¿ç›¸å¯¹äºæœ«ç«¯çš„ä¸€è‡´æ€§):")
        print(f"   å¹³ç§»è¯¯å·®: å¹³å‡={np.mean(trans_errors):.2f}mm, æœ€å¤§={np.max(trans_errors):.2f}mm")
        print(f"   æ—‹è½¬è¯¯å·®: å¹³å‡={np.mean(rot_errors):.2f}Â°, æœ€å¤§={np.max(rot_errors):.2f}Â°")
        
        # è´¨é‡è¯„ä¼°
        if np.mean(trans_errors) < 10 and np.mean(rot_errors) < 2:
             print("\n   âœ… æ ‡å®šè´¨é‡: ä¼˜ç§€")
        elif np.mean(trans_errors) < 20 and np.mean(rot_errors) < 5:
             print("\n   âš ï¸  æ ‡å®šè´¨é‡: è‰¯å¥½")
        else:
             print("\n   âŒ æ ‡å®šè´¨é‡: ä¸€èˆ¬/è¾ƒå·®")
             
        print("="*70)
    
    def load_session_data(self, session_dir):
        """ä»ä¼šè¯ç›®å½•åŠ è½½æ•°æ®"""
        try:
            import glob
            pose_files = sorted(glob.glob(os.path.join(session_dir, "pose_*.npz")))
            
            if not pose_files:
                print(f"âŒ ä¼šè¯ç›®å½•ä¸­æ²¡æœ‰æ•°æ®æ–‡ä»¶: {session_dir}")
                return False
            
            self.robot_poses = []
            self.target_poses = []
            
            for f in pose_files:
                data = np.load(f)
                self.robot_poses.append(data['robot_pose'])
                self.target_poses.append(data['target_pose'])
            
            print(f"âœ… ä»ä¼šè¯ç›®å½•åŠ è½½æ•°æ®: {session_dir}")
            print(f"   æ•°æ®ç‚¹æ•°é‡: {len(self.robot_poses)}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ä¼šè¯æ•°æ®å¤±è´¥: {e}")
            return False

    def close(self):
        """å…³é—­æ§åˆ¶å™¨"""
        if self.controller:
            self.controller.close()
            print("ğŸ”Œ æ§åˆ¶å™¨å·²å…³é—­")


def main():
    parser = argparse.ArgumentParser(description="çœ¼åœ¨æ‰‹å¤–æ‰‹çœ¼æ ‡å®šå·¥å…·")
    parser.add_argument("--collect", action="store_true", help="é‡‡é›†æ ‡å®šæ•°æ®")
    parser.add_argument("--calibrate", action="store_true", help="æ‰§è¡Œæ ‡å®šè®¡ç®—")
    parser.add_argument("--all", action="store_true", help="é‡‡é›†+æ ‡å®š")

    parser.add_argument("--output-dir", default="./handeye_data_environment", help="æ•°æ®ä¿å­˜ç›®å½•")
    parser.add_argument(
        "--intrinsic",
        default="./config_data/camera_intrinsics_environment.yaml",
        help="ç›¸æœºå†…å‚æ–‡ä»¶ (OpenCV YAML, ä»…è¯» K/distCoeffs)",
    )
    parser.add_argument("--square-size", type=float, default=18.0, help="æ£‹ç›˜æ ¼æ–¹æ ¼å¤§å°(mm)")
    parser.add_argument("--port", default="/dev/left_arm", help="ä¸²å£")
    parser.add_argument("--video", type=int, default=0, help="è§†é¢‘è®¾å¤‡ID")
    parser.add_argument("--width", type=int, default=1280, help="ç›¸æœºé‡‡é›†å®½åº¦")
    parser.add_argument("--height", type=int, default=720, help="ç›¸æœºé‡‡é›†é«˜åº¦")

    # å…¼å®¹æ—§å‚æ•°
    parser.add_argument("--camera", type=int, help="(å…¼å®¹) ç›¸æœºè®¾å¤‡IDï¼Œç­‰åŒäº --video")
    parser.add_argument(
        "--camera-params",
        help="(å…¼å®¹) ç›¸æœºå‚æ•°æ–‡ä»¶ï¼Œç­‰åŒäº --intrinsic (æœ¬è„šæœ¬ä»…è¯»å–å†…å‚)",
    )

    args = parser.parse_args()

    if args.camera is not None:
        args.video = args.camera
    if args.camera_params is not None:
        args.intrinsic = args.camera_params

    calibrator = EyeToHandCalibrator(
        board_size=(7, 5),
        square_size=args.square_size / 1000.0,
        intrinsic_file=args.intrinsic,
        output_dir=args.output_dir,
    )

    try:
        if args.collect or args.all:
            calibrator.init_robot(port=args.port)
            print("\nğŸ  æœºæ¢°è‡‚å›ä¸­...")
            calibrator.controller.move_all_home()
            time.sleep(1)
            calibrator.collect_data_interactive(cam_id=args.video, width=args.width, height=args.height)

        if args.calibrate or args.all or (not args.collect and not args.all):
            if not calibrator.T_target_cam_list:
                calibrator.load_collected_data()

            if calibrator.T_target_cam_list:
                T_cam_base = calibrator.calibrate()
                if T_cam_base is not None:
                    calibrator.evaluate_calibration_consistency(T_cam_base)
                    calibrator.save_result(T_cam_base)

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    finally:
        calibrator.close()


if __name__ == "__main__":
    main()