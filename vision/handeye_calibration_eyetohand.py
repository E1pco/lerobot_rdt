#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœ¼åœ¨æ‰‹å¤– (Eye-to-Hand) æ‰‹çœ¼æ ‡å®šè„šæœ¬
=====================================
åŠŸèƒ½:
  1. æ§åˆ¶æœºæ¢°è‡‚ç§»åŠ¨åˆ°ä¸åŒä½å§¿
  2. åœ¨æ¯ä¸ªä½å§¿é‡‡é›†å›¾åƒå¹¶æ£€æµ‹æ£‹ç›˜æ ¼
  3. ä½¿ç”¨PnPè®¡ç®—æ£‹ç›˜æ ¼åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿ (T_target_cam)
  4. ä½¿ç”¨æ­£è¿åŠ¨å­¦è®¡ç®—æœ«ç«¯åœ¨åŸºåº§æ ‡ç³»ä¸‹çš„ä½å§¿ (T_gripper_base)
  5. ä½¿ç”¨æ‰‹çœ¼æ ‡å®šç®—æ³•æ±‚è§£ç›¸æœºåˆ°åŸºåº§çš„å˜æ¢çŸ©é˜µ (T_cam_base)

ä½¿ç”¨æ–¹æ³•:
  python handeye_calibration_eyetohand.py --collect --camera 0 --port /dev/left_arm --output-dir ./calib_data
  python handeye_calibration_eyetohand.py --calibrate --output-dir ./calib_data
  python handeye_calibration_eyetohand.py --all --camera 0 --port /dev/left_arm --output-dir ./calib_data

åæ ‡ç³»å®šä¹‰:
  - base: æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
  - gripper/end-effector: æœºæ¢°è‡‚æœ«ç«¯åæ ‡ç³»
  - cam: ç›¸æœºåæ ‡ç³» (å›ºå®šåœ¨ç¯å¢ƒä¸­)
  - target: æ ‡å®šæ¿åæ ‡ç³»

çœ¼åœ¨æ‰‹å¤–æ–¹ç¨‹: AX = YB
  - A: ç›¸é‚»ä¸¤ä¸ªæœ«ç«¯ä½å§¿çš„ç›¸å¯¹å˜æ¢
  - B: ç›¸é‚»ä¸¤ä¸ªæ ‡å®šæ¿ä½å§¿çš„ç›¸å¯¹å˜æ¢
  - X: ç›¸æœºç›¸å¯¹äºåŸºåº§çš„å˜æ¢ (T_cam_base) - å¾…æ±‚
  - Y: æœ«ç«¯ç›¸å¯¹äºåŸºåº§çš„å˜æ¢ (T_gripper_base)
"""

import os
import sys
import cv2
import numpy as np
import time
import yaml
import argparse
from datetime import datetime
from scipy.spatial.transform import Rotation as R

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof, create_so101_5dof_gripper


class EyeToHandCalibrator:
    """çœ¼åœ¨æ‰‹å¤–æ‰‹çœ¼æ ‡å®šå™¨"""
    
    def __init__(self, 
                 board_size=(4, 4),
                 square_size=0.00983,  # 25mm
                 camera_id=0,
                 port="/dev/left_arm",
                 camera_params_file="./config_data/camera_intrinsics_environment.yaml",
                 output_dir="./handeye_data_environment"):
        """
        Parameters
        ----------
        board_size : tuple
            æ£‹ç›˜æ ¼å†…è§’ç‚¹æ•°é‡ (cols-1, rows-1)
        square_size : float
            æ£‹ç›˜æ ¼æ–¹æ ¼è¾¹é•¿ (ç±³)
        camera_id : int
            ç›¸æœºè®¾å¤‡ID
        port : str
            æœºæ¢°è‡‚ä¸²å£è·¯å¾„
        camera_params_file : str
            ç›¸æœºå†…å¤–å‚æ–‡ä»¶è·¯å¾„ (OpenCV YAMLæ ¼å¼)
        output_dir : str
            æ•°æ®ä¿å­˜ç›®å½•
        """
        self.board_size = board_size
        self.square_size = square_size
        self.camera_id = camera_id
        self.port = port
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç›¸æœº
        self.cap = cv2.VideoCapture(camera_id)
        if not self.cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€ç›¸æœº {camera_id}")
        
        # è®¾ç½®ç›¸æœºåˆ†è¾¨ç‡
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        # åˆå§‹åŒ–æœºæ¢°è‡‚
        print(f"è¿æ¥æœºæ¢°è‡‚: {port}")
        self.controller = ServoController(
            port=port,
            baudrate=1_000_000,
            config_path="../driver/servo_config.json"
        )
        
        # åˆ›å»ºæœºå™¨äººæ¨¡å‹
        self.robot = create_so101_5dof_gripper()
        self.robot.set_servo_controller(self.controller)
        
        # åŠ è½½ç›¸æœºå‚æ•°
        self.camera_matrix = None
        self.dist_coeffs = None
        self.camera_extrinsics = None
        
        if camera_params_file:
            self.load_camera_params(camera_params_file)
        
        # ç”Ÿæˆæ£‹ç›˜æ ¼3Dç‚¹
        self.generate_board_corners()
        
        # æ•°æ®å­˜å‚¨
        self.robot_poses = []       # T_gripper_base
        self.target_poses = []      # T_target_cam
        self.images = []
        
        print(f"âœ… çœ¼åœ¨æ‰‹å¤–æ ‡å®šå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ£‹ç›˜æ ¼å°ºå¯¸: {board_size}")
        print(f"   æ–¹æ ¼å¤§å°: {square_size*1000:.1f}mm")
        print(f"   ç›¸æœºID: {camera_id}")
        print(f"   æœºæ¢°è‡‚: {port}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    def load_camera_params(self, camera_params_file):
        """åŠ è½½ç›¸æœºå†…å¤–å‚æ•° (OpenCV YAMLæ ¼å¼)"""
        try:
            if not os.path.exists(camera_params_file):
                print(f"âŒ ç›¸æœºå‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {camera_params_file}")
                return

            fs = cv2.FileStorage(camera_params_file, cv2.FILE_STORAGE_READ)
            
            if not fs.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€ç›¸æœºå‚æ•°æ–‡ä»¶: {camera_params_file}")
                return

            # 1. åŠ è½½å†…å‚çŸ©é˜µ K
            camera_matrix_node = fs.getNode('K')
            if camera_matrix_node.empty():
                camera_matrix_node = fs.getNode('camera_matrix')
            
            if not camera_matrix_node.empty():
                self.camera_matrix = camera_matrix_node.mat()
            
            # 2. åŠ è½½ç•¸å˜ç³»æ•° distCoeffs
            dist_coeffs_node = fs.getNode('distCoeffs')
            if dist_coeffs_node.empty():
                dist_coeffs_node = fs.getNode('distortion_coefficients')
            
            if not dist_coeffs_node.empty():
                self.dist_coeffs = dist_coeffs_node.mat().flatten()

            # 3. å°è¯•åŠ è½½æ£‹ç›˜æ ¼å‚æ•° (å¦‚æœæ–‡ä»¶ä¸­æœ‰)
            square_size_node = fs.getNode('square_size')
            if not square_size_node.empty():
                file_square_size = square_size_node.real()
                if abs(file_square_size - self.square_size) > 1e-6:
                    print(f"â„¹ï¸  ä½¿ç”¨æ–‡ä»¶ä¸­çš„æ–¹æ ¼å¤§å°: {file_square_size*1000:.2f}mm (åŸè®¾ç½®: {self.square_size*1000:.2f}mm)")
                    self.square_size = file_square_size
                    # é‡æ–°ç”Ÿæˆæ£‹ç›˜æ ¼3Dç‚¹
                    self.generate_board_corners()

            cols_node = fs.getNode('board_size_cols')
            rows_node = fs.getNode('board_size_rows')
            if not cols_node.empty() and not rows_node.empty():
                cols = int(cols_node.real())
                rows = int(rows_node.real())
                if (cols, rows) != self.board_size and (rows, cols) != self.board_size:
                     print(f"â„¹ï¸  ä½¿ç”¨æ–‡ä»¶ä¸­çš„æ£‹ç›˜æ ¼å°ºå¯¸: {cols}x{rows} (åŸè®¾ç½®: {self.board_size})")
                     self.board_size = (cols, rows)
                     self.generate_board_corners()

            # å°è¯•åŠ è½½å¤–å‚ (å¯é€‰)
            extrinsics_node = fs.getNode('camera_extrinsics')
            if not extrinsics_node.empty():
                self.camera_extrinsics = extrinsics_node.mat()
            
            fs.release()
            
            print(f"âœ… åŠ è½½ç›¸æœºå‚æ•°: {camera_params_file}")
            if self.camera_matrix is not None:
                print(f"   ç›¸æœºçŸ©é˜µ:\\n{self.camera_matrix}")
            if self.dist_coeffs is not None:
                print(f"   ç•¸å˜ç³»æ•°: {self.dist_coeffs}")
            if self.camera_extrinsics is not None:
                print(f"   å¤–å‚çŸ©é˜µ:\\n{self.camera_extrinsics}")
                
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ç›¸æœºå‚æ•°: {e}")
            print("   å°†ä½¿ç”¨è‡ªåŠ¨æ ‡å®šæˆ–é»˜è®¤å‚æ•°")
    
    def load_camera_intrinsics(self, intrinsic_file):
        """åŠ è½½ç›¸æœºå†…å‚ (å…¼å®¹æ€§æ–¹æ³•)"""
        try:
            if intrinsic_file.endswith('.yaml') or intrinsic_file.endswith('.yml'):
                # OpenCV YAMLæ ¼å¼
                fs = cv2.FileStorage(intrinsic_file, cv2.FILE_STORAGE_READ)
                self.camera_matrix = fs.getNode('camera_matrix').mat()
                self.dist_coeffs = fs.getNode('distortion_coefficients').mat().flatten()
                fs.release()
            else:
                # NumPyæ ¼å¼
                data = np.load(intrinsic_file, allow_pickle=True).item()
                self.camera_matrix = data['camera_matrix']
                self.dist_coeffs = data['dist_coeffs']
            
            print(f"âœ… åŠ è½½ç›¸æœºå†…å‚: {intrinsic_file}")
            print(f"   ç›¸æœºçŸ©é˜µ:\\n{self.camera_matrix}")
            print(f"   ç•¸å˜ç³»æ•°: {self.dist_coeffs}")
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ç›¸æœºå†…å‚: {e}")
            print("   å°†ä½¿ç”¨è‡ªåŠ¨æ ‡å®šæˆ–é»˜è®¤å‚æ•°")
    
    def load_camera_extrinsics(self, extrinsic_file):
        """åŠ è½½ç›¸æœºå¤–å‚ (å…¼å®¹æ€§æ–¹æ³•)"""
        try:
            if extrinsic_file.endswith('.yaml') or extrinsic_file.endswith('.yml'):
                with open(extrinsic_file, 'r') as f:
                    data = yaml.safe_load(f)
                self.camera_extrinsics = np.array(data['camera_extrinsics']).reshape(4, 4)
            else:
                self.camera_extrinsics = np.load(extrinsic_file)
            
            print(f"âœ… åŠ è½½ç›¸æœºå¤–å‚: {extrinsic_file}")
            print(f"   å¤–å‚çŸ©é˜µ:\\n{self.camera_extrinsics}")
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ç›¸æœºå¤–å‚: {e}")
    
    def generate_board_corners(self):
        """ç”Ÿæˆæ£‹ç›˜æ ¼3Dè§’ç‚¹"""
        self.board_corners = np.zeros((self.board_size[0] * self.board_size[1], 3), np.float32)
        self.board_corners[:, :2] = np.mgrid[0:self.board_size[0], 0:self.board_size[1]].T.reshape(-1, 2)
        self.board_corners *= self.square_size
    
    def detect_chessboard(self, image):
        """æ£€æµ‹æ£‹ç›˜æ ¼"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹è§’ç‚¹
        ret, corners = cv2.findChessboardCorners(
            gray, self.board_size, 
            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE
        )
        
        if ret:
            # äºšåƒç´ ç²¾åº¦ä¼˜åŒ–
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
            corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
            
            return True, corners.reshape(-1, 2)
        else:
            return False, None
    
    def calculate_target_pose(self, corners):
        """ä½¿ç”¨PnPè®¡ç®—æ ‡å®šæ¿åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿"""
        if self.camera_matrix is None:
            print("âŒ ç›¸æœºå†…å‚æœªåŠ è½½ï¼Œæ— æ³•è®¡ç®—ç›®æ ‡ä½å§¿")
            return None
        
        # è§£PnP
        success, rvec, tvec = cv2.solvePnP(
            self.board_corners, corners, 
            self.camera_matrix, self.dist_coeffs
        )
        
        if success:
            # è½¬æ¢ä¸ºå˜æ¢çŸ©é˜µ
            rotation_matrix, _ = cv2.Rodrigues(rvec)
            transform = np.eye(4)
            transform[:3, :3] = rotation_matrix
            transform[:3, 3] = tvec.flatten()
            
            return transform
        else:
            return None
    
    def get_robot_pose(self):
        """è·å–æœºå™¨äººæœ«ç«¯ä½å§¿"""
        try:
            # è¯»å–å½“å‰å…³èŠ‚è§’åº¦
            current_q = self.robot.read_joint_angles(verbose=False)
            
            # è®¡ç®—æ­£è¿åŠ¨å­¦
            T_gripper_base = self.robot.fkine(current_q)
            
            return T_gripper_base
            
        except Exception as e:
            print(f"âŒ è·å–æœºå™¨äººä½å§¿å¤±è´¥: {e}")
            return None
    
    def capture_calibration_data(self):
        """é‡‡é›†æ ‡å®šæ•°æ®"""
        print("ğŸ¯ å¼€å§‹é‡‡é›†æ‰‹çœ¼æ ‡å®šæ•°æ®")
        print("æ“ä½œè¯´æ˜:")
        print("  ç©ºæ ¼é”® - é‡‡é›†å½“å‰ä½å§¿çš„æ•°æ®")
        print("  ré”® - åˆ é™¤æœ€åä¸€ä¸ªæ•°æ®ç‚¹")
        print("  sé”® - ä¿å­˜æ•°æ®")
        print("  qé”® - é€€å‡ºé‡‡é›†")
        print("  hé”® - æœºæ¢°è‡‚å›åˆ°åˆå§‹ä½ç½®")
        print()
        print("è¯·ç§»åŠ¨æœºæ¢°è‡‚åˆ°ä¸åŒä½ç½®ï¼Œç¡®ä¿ç›¸æœºèƒ½çœ‹åˆ°æ ‡å®šæ¿...")
        
        # åˆ›å»ºæœ¬æ¬¡é‡‡é›†çš„ä¼šè¯ç›®å½•
        session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_dir = os.path.join(self.output_dir, f"session_{session_timestamp}")
        os.makedirs(session_dir, exist_ok=True)
        print(f"ğŸ“‚ æ•°æ®å°†å®æ—¶ä¿å­˜åˆ°: {session_dir}")
        
        pose_count = 0
        
        try:
            while True:
                # è¯»å–å›¾åƒ
                ret, frame = self.cap.read()
                if not ret:
                    print("âŒ ç›¸æœºè¯»å–å¤±è´¥")
                    break
                
                # æ£€æµ‹æ£‹ç›˜æ ¼
                found, corners = self.detect_chessboard(frame)
                
                # å¯è§†åŒ–
                display_frame = frame.copy()
                
                if found:
                    # ç»˜åˆ¶æ£€æµ‹åˆ°çš„è§’ç‚¹
                    cv2.drawChessboardCorners(display_frame, self.board_size, corners, found)
                    
                    # å¦‚æœæœ‰ç›¸æœºå†…å‚ï¼Œè®¡ç®—å¹¶æ˜¾ç¤ºåæ ‡è½´
                    if self.camera_matrix is not None:
                        target_pose = self.calculate_target_pose(corners)
                        if target_pose is not None:
                            # ç»˜åˆ¶åæ ‡è½´
                            axis_points = np.array([
                                [0, 0, 0],
                                [0.05, 0, 0],  # Xè½´ - çº¢è‰²
                                [0, 0.05, 0],  # Yè½´ - ç»¿è‰²
                                [0, 0, -0.05]  # Zè½´ - è“è‰²
                            ], dtype=np.float32)
                            
                            axis_2d, _ = cv2.projectPoints(
                                axis_points, 
                                cv2.Rodrigues(target_pose[:3, :3])[0],
                                target_pose[:3, 3],
                                self.camera_matrix, 
                                self.dist_coeffs
                            )
                            
                            # è½¬æ¢ä¸ºæ•´æ•°åæ ‡å¹¶ç»˜åˆ¶åæ ‡è½´
                            axis_2d = axis_2d.reshape(-1, 2)
                            pts = np.int32(axis_2d).reshape(-1, 2)
                            origin = tuple(pts[0].tolist())
                            pt_x = tuple(pts[1].tolist())
                            pt_y = tuple(pts[2].tolist())
                            pt_z = tuple(pts[3].tolist())
                            
                            # ä½¿ç”¨lineä»£æ›¿arrowedLineä»¥é¿å…ç±»å‹é—®é¢˜
                            cv2.line(display_frame, origin, pt_x, (0, 0, 255), 3)  # X - çº¢
                            cv2.line(display_frame, origin, pt_y, (0, 255, 0), 3)  # Y - ç»¿
                            cv2.line(display_frame, origin, pt_z, (255, 0, 0), 3)  # Z - è“
                    
                    # æ˜¾ç¤ºçŠ¶æ€
                    cv2.putText(display_frame, f"Chessboard: FOUND", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                else:
                    cv2.putText(display_frame, "Chessboard: NOT FOUND", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # æ˜¾ç¤ºå·²é‡‡é›†çš„æ•°æ®ç‚¹æ•°é‡
                cv2.putText(display_frame, f"Poses: {pose_count}", (10, 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                cv2.imshow('Hand-Eye Calibration (Eye-to-Hand)', display_frame)
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord(' '):  # ç©ºæ ¼ - é‡‡é›†æ•°æ®
                    if found and self.camera_matrix is not None:
                        # è®¡ç®—æ ‡å®šæ¿ä½å§¿
                        target_pose = self.calculate_target_pose(corners)
                        robot_pose = self.get_robot_pose()
                        
                        if target_pose is not None and robot_pose is not None:
                            # ä¿å­˜æ•°æ®åˆ°å†…å­˜
                            self.target_poses.append(target_pose)
                            self.robot_poses.append(robot_pose)
                            self.images.append(frame.copy())
                            
                            # ç«‹å³ä¿å­˜åˆ°ç£ç›˜
                            img_filename = os.path.join(session_dir, f"image_{pose_count:03d}.jpg")
                            
                            # ä¿å­˜å¸¦æœ‰åæ ‡è½´çš„å›¾åƒ (å¯é€‰ï¼Œå¦‚æœç”¨æˆ·æƒ³è¦ä¿å­˜å¸¦è½´çš„å›¾)
                            # ä½†é€šå¸¸æ ‡å®šéœ€è¦åŸå§‹å›¾ã€‚ç”¨æˆ·è¯´"åœ¨é‡‡é›†çš„æ—¶å€™éƒ½åœ¨å›¾åƒåŠ ä¸Šä¸‰ç»´åæ ‡è½´"ï¼Œå¯èƒ½æ˜¯æŒ‡æ˜¾ç¤ºï¼Œä¹Ÿå¯èƒ½æ˜¯æŒ‡ä¿å­˜ã€‚
                            # å¦‚æœæ˜¯æŒ‡ä¿å­˜ï¼Œæˆ‘ä»¬åº”è¯¥ä¿å­˜ display_frameã€‚
                            # ä½†ä¸ºäº†æ ‡å®šå‡†ç¡®æ€§ï¼ŒåŸå§‹å›¾åƒå¿…é¡»æ˜¯å¹²å‡€çš„ã€‚
                            # ä¹Ÿè®¸ç”¨æˆ·åªæ˜¯æƒ³åœ¨ç•Œé¢ä¸Šçœ‹åˆ°ã€‚
                            # æ—¢ç„¶ç•Œé¢ä¸Šå·²ç»æœ‰äº†ï¼Œé‚£å¯èƒ½æ˜¯ç”¨æˆ·è§‰å¾—ä¸å¤Ÿæ˜æ˜¾æˆ–è€…æ²¡çœ‹åˆ°ï¼Ÿ
                            # æˆ–è€…ç”¨æˆ·å¸Œæœ›ä¿å­˜ä¸‹æ¥çš„å›¾ç‰‡ä¹Ÿæœ‰åæ ‡è½´ç”¨äºæ£€æŸ¥ï¼Ÿ
                            # è®©æˆ‘ä»¬ä¿å­˜ä¸€ä»½å¸¦åæ ‡è½´çš„å‰¯æœ¬ç”¨äºè°ƒè¯•ã€‚
                            
                            cv2.imwrite(img_filename, frame) # ä¿å­˜åŸå§‹å›¾ç”¨äºæ ‡å®š
                            cv2.imwrite(os.path.join(session_dir, f"vis_{pose_count:03d}.jpg"), display_frame) # ä¿å­˜å¯è§†åŒ–å›¾
                            
                            pose_filename = os.path.join(session_dir, f"pose_{pose_count:03d}.npz")
                            np.savez(pose_filename, 
                                     robot_pose=robot_pose, 
                                     target_pose=target_pose)
                            
                            pose_count += 1
                            
                            # è®¡ç®—æ¬§æ‹‰è§’ä»¥ä¾¿æ˜¾ç¤º
                            r_robot = R.from_matrix(robot_pose[:3, :3])
                            euler_robot = r_robot.as_euler('xyz', degrees=True)
                            
                            print(f"âœ… é‡‡é›†ä½å§¿ {pose_count}")
                            print(f"   æœºå™¨äººä½ç½®: {robot_pose[:3, 3]}")
                            print(f"   æœºå™¨äººå§¿æ€(Euler XYZ): {euler_robot}")
                            print(f"   æ ‡å®šæ¿ä½å§¿: {target_pose[:3, 3]}")
                            print(f"   å·²ä¿å­˜: {img_filename}")
                        else:
                            print("âŒ ä½å§¿è®¡ç®—å¤±è´¥")
                    else:
                        if not found:
                            print("âŒ æœªæ£€æµ‹åˆ°æ£‹ç›˜æ ¼")
                        if self.camera_matrix is None:
                            print("âŒ ç›¸æœºå†…å‚æœªåŠ è½½")
                
                elif key == ord('r'):  # r - åˆ é™¤æœ€åä¸€ä¸ªæ•°æ®ç‚¹
                    if pose_count > 0:
                        self.target_poses.pop()
                        self.robot_poses.pop()
                        self.images.pop()
                        pose_count -= 1
                        print(f"ğŸ—‘ï¸  åˆ é™¤æœ€åä¸€ä¸ªæ•°æ®ç‚¹ï¼Œå‰©ä½™: {pose_count}")
                    else:
                        print("âŒ æ²¡æœ‰æ•°æ®ç‚¹å¯åˆ é™¤")
                
                elif key == ord('s'):  # s - ä¿å­˜æ•°æ®
                    if pose_count > 0:
                        self.save_calibration_data()
                        print(f"ğŸ’¾ å·²ä¿å­˜ {pose_count} ä¸ªæ•°æ®ç‚¹")
                    else:
                        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                
                elif key == ord('h'):  # h - å›åˆ°åˆå§‹ä½ç½®
                    print("ğŸ  æœºæ¢°è‡‚å›åˆ°åˆå§‹ä½ç½®...")
                    self.controller.move_all_home()
                    time.sleep(2)
                
                elif key == ord('q'):  # q - é€€å‡º
                    print("ğŸ›‘ é€€å‡ºé‡‡é›†")
                    break
        
        except KeyboardInterrupt:
            print("\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        
        finally:
            cv2.destroyAllWindows()
        
        print(f"ğŸ“Š é‡‡é›†å®Œæˆï¼Œå…±è·å¾— {pose_count} ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹")
        return pose_count > 0
    
    def save_calibration_data(self):
        """ä¿å­˜æ ‡å®šæ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä½å§¿æ•°æ®
        poses_file = os.path.join(self.output_dir, f"calibration_poses_{timestamp}.npz")
        np.savez(poses_file,
                robot_poses=np.array(self.robot_poses),
                target_poses=np.array(self.target_poses))
        
        # ä¿å­˜å›¾åƒ
        images_dir = os.path.join(self.output_dir, f"images_{timestamp}")
        os.makedirs(images_dir, exist_ok=True)
        
        for i, img in enumerate(self.images):
            img_file = os.path.join(images_dir, f"image_{i:03d}.jpg")
            cv2.imwrite(img_file, img)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°:")
        print(f"   ä½å§¿æ•°æ®: {poses_file}")
        print(f"   å›¾åƒæ•°æ®: {images_dir}")
    
    def load_calibration_data(self, poses_file):
        """åŠ è½½æ ‡å®šæ•°æ®"""
        try:
            data = np.load(poses_file)
            self.robot_poses = data['robot_poses'].tolist()
            self.target_poses = data['target_poses'].tolist()
            
            print(f"âœ… åŠ è½½æ ‡å®šæ•°æ®: {poses_file}")
            print(f"   æ•°æ®ç‚¹æ•°é‡: {len(self.robot_poses)}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ ‡å®šæ•°æ®å¤±è´¥: {e}")
            return False
    
    def calibrate_eye_to_hand(self):
        """æ‰§è¡Œçœ¼åœ¨æ‰‹å¤–æ ‡å®š"""
        if len(self.robot_poses) < 3:
            print("âŒ æ•°æ®ç‚¹ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ä¸ªä½å§¿")
            return None
        
        print(f"ğŸ”§ å¼€å§‹çœ¼åœ¨æ‰‹å¤–æ ‡å®šï¼Œæ•°æ®ç‚¹æ•°é‡: {len(self.robot_poses)}")
        
        try:
            # å‡†å¤‡æ•°æ®
            R_gripper2base = []
            t_gripper2base = []
            R_target2cam = []
            t_target2cam = []
            
            R_base2gripper = []
            t_base2gripper = []
            
            R_cam2target = []
            t_cam2target = []
            
            for i in range(len(self.robot_poses)):
                # 1. æœºå™¨äººæœ«ç«¯åˆ°åŸºåº§ (Standard FK)
                T_gripper_base = self.robot_poses[i]
                R_gripper2base.append(T_gripper_base[:3, :3])
                t_gripper2base.append(T_gripper_base[:3, 3])
                
                # 2. åŸºåº§åˆ°æœºå™¨äººæœ«ç«¯ (Inverted FK)
                T_base_gripper = np.linalg.inv(T_gripper_base)
                R_base2gripper.append(T_base_gripper[:3, :3])
                t_base2gripper.append(T_base_gripper[:3, 3])
                
                # 3. æ ‡å®šæ¿åˆ°ç›¸æœº (Standard PnP)
                T_target_cam = self.target_poses[i]
                R_target2cam.append(T_target_cam[:3, :3])
                t_target2cam.append(T_target_cam[:3, 3])
                
                # 4. ç›¸æœºåˆ°æ ‡å®šæ¿ (Inverted PnP)
                T_cam_target = np.linalg.inv(T_target_cam)
                R_cam2target.append(T_cam_target[:3, :3])
                t_cam2target.append(T_cam_target[:3, 3])
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            print("\\nğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥:")
            self.analyze_data_quality(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam)
            
            # å®šä¹‰ä¸åŒçš„è¾“å…¥ç»„åˆç­–ç•¥
            strategies = [
                {
                    "name": "Strategy 1: Base2Gripper + Target2Cam",
                    "R_gripper": R_base2gripper, "t_gripper": t_base2gripper,
                    "R_target": R_target2cam, "t_target": t_target2cam
                },
                {
                    "name": "Strategy 2: Gripper2Base + Cam2Target",
                    "R_gripper": R_gripper2base, "t_gripper": t_gripper2base,
                    "R_target": R_cam2target, "t_target": t_cam2target
                },
                {
                    "name": "Strategy 3: Target2Cam (as Robot) + Gripper2Base (as Target)",
                    "R_gripper": R_target2cam, "t_gripper": t_target2cam,
                    "R_target": R_gripper2base, "t_target": t_gripper2base
                }
            ]
            
            methods = [
                (cv2.CALIB_HAND_EYE_TSAI, "Tsai-Lenz"),
                (cv2.CALIB_HAND_EYE_PARK, "Park"),
                (cv2.CALIB_HAND_EYE_HORAUD, "Horaud"),
                (cv2.CALIB_HAND_EYE_ANDREFF, "Andreff"),
                (cv2.CALIB_HAND_EYE_DANIILIDIS, "Daniilidis")
            ]
            
            best_result = None
            best_score = float('inf')
            
            for strategy in strategies:
                print(f"\\nğŸ”„ å°è¯•ç­–ç•¥: {strategy['name']}")
                
                for method, method_name in methods:
                    try:
                        # æ‰§è¡Œæ ‡å®š
                        R_calib, t_calib = cv2.calibrateHandEye(
                            strategy["R_gripper"], strategy["t_gripper"],
                            strategy["R_target"], strategy["t_target"],
                            method=method
                        )
                        
                        # éªŒè¯ç»“æœ
                        error = self.evaluate_calibration(R_calib, t_calib, 
                                                        R_gripper2base, t_gripper2base,
                                                        R_target2cam, t_target2cam)
                        
                        print(f"   {method_name}: {error:.6f} mm")
                        
                        if error < best_score and not (np.isnan(error) or np.isinf(error)):
                            best_score = error
                            best_result = (R_calib, t_calib, method_name, strategy['name'])
                        
                    except Exception as e:
                        print(f"   {method_name} å¤±è´¥: {e}")
                        continue
            
            # å°è¯•éçº¿æ€§ä¼˜åŒ–
            if best_result is not None:
                print(f"\\nğŸ”„ å°è¯•éçº¿æ€§ä¼˜åŒ– (åŸºäº {best_result[2]})...")
                try:
                    R_opt, t_opt, error_opt = self.optimize_calibration(
                        best_result[0], best_result[1],
                        R_gripper2base, t_gripper2base,
                        R_target2cam, t_target2cam
                    )
                    print(f"   Optimization: {error_opt:.6f} mm")
                    
                    if error_opt < best_score:
                        best_score = error_opt
                        best_result = (R_opt, t_opt, "Optimization", "Non-linear Least Squares")
                except Exception as e:
                    print(f"   ä¼˜åŒ–å¤±è´¥: {e}")

            if best_result is None:
                print("âŒ æ‰€æœ‰æ ‡å®šç®—æ³•éƒ½å¤±è´¥äº†")
                return None
            
            R_cam2base, t_cam2base, best_method, best_strategy = best_result
            
            # æ„å»ºå˜æ¢çŸ©é˜µ
            T_cam2base = np.eye(4)
            T_cam2base[:3, :3] = R_cam2base
            T_cam2base[:3, 3] = t_cam2base.flatten()

            # æœ‰äº›è¾“å…¥ç»„åˆ/æ–‡çŒ®å®šä¹‰ä¼šè¿”å›â€œé€†â€çš„å¤–å‚ï¼ˆä¾‹å¦‚å¾—åˆ° T_base_cam è€Œä¸æ˜¯ T_cam_baseï¼‰ã€‚
            # è¿™é‡Œç”¨æ•°æ®ä¸€è‡´æ€§è‡ªåŠ¨åˆ¤åˆ«ï¼šé€‰æ‹©èƒ½è®© T_target_gripper æ›´ç¨³å®šçš„é‚£ä¸ªæ–¹å‘ã€‚
            try:
                score_direct = self.evaluate_calibration(
                    T_cam2base[:3, :3],
                    T_cam2base[:3, 3].reshape(3, 1),
                    R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam,
                )
                T_inv = np.linalg.inv(T_cam2base)
                score_inv = self.evaluate_calibration(
                    T_inv[:3, :3],
                    T_inv[:3, 3].reshape(3, 1),
                    R_gripper2base, t_gripper2base,
                    R_target2cam, t_target2cam,
                )
                if np.isfinite(score_inv) and (score_inv + 1e-9) < score_direct:
                    print(f"\nâ„¹ï¸  æ£€æµ‹åˆ°ç»“æœå¯èƒ½ä¸ºé€†å˜æ¢ï¼šä¸€è‡´æ€§ {score_direct:.6f} -> {score_inv:.6f} mmï¼Œå·²è‡ªåŠ¨å–é€†")
                    T_cam2base = T_inv
                    best_strategy = f"{best_strategy} (auto-inverted)"
                    best_score = score_inv
                else:
                    best_score = score_direct
            except Exception as _e:
                # è‹¥è¯„ä¼°å¤±è´¥ï¼Œä¸é˜»æ–­ä¸»æµç¨‹
                pass
            
            print(f"\\nâœ… çœ¼åœ¨æ‰‹å¤–æ ‡å®šå®Œæˆ")
            print(f"   æœ€ä½³ç­–ç•¥: {best_strategy}")
            print(f"   æœ€ä½³ç®—æ³•: {best_method}")
            print(f"   ä¸€è‡´æ€§è¯¯å·®: {best_score:.6f} mm")
            print(f"\\nğŸ¯ ç›¸æœºåˆ°åŸºåº§å˜æ¢çŸ©é˜µ (T_cam_base):")
            print(T_cam2base)
            
            # åˆ†æç»“æœ
            self.analyze_calibration_result(T_cam2base)
            
            # ä¸€è‡´æ€§è¯„ä¼°
            self.evaluate_calibration_consistency(T_cam2base)
            
            # ä¿å­˜ç»“æœ
            self.save_calibration_result(T_cam2base, best_method, best_score)
            
            return T_cam2base
            
        except Exception as e:
            print(f"âŒ æ ‡å®šå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def optimize_calibration(self, R_init, t_init, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """ä½¿ç”¨éçº¿æ€§æœ€å°äºŒä¹˜ä¼˜åŒ–æ ‡å®šç»“æœ"""
        from scipy.optimize import least_squares
        
        # 1. åˆå§‹åŒ– T_cam_base (X)
        T_cam_base = np.eye(4)
        T_cam_base[:3, :3] = R_init
        T_cam_base[:3, 3] = t_init.flatten()
        
        # 2. åˆå§‹åŒ– T_gripper_target (Z)
        # Z = mean( inv(T_base_gripper) * inv(T_cam_base) * T_cam_target ) ?
        # No, T_cam_target = T_cam_base * T_base_gripper * T_gripper_target
        # So T_gripper_target = inv(T_base_gripper) * inv(T_cam_base) * T_cam_target
        # Wait, T_base_gripper is T_gripper_base (in my code variable name)
        # My code: T_gripper_base variable holds T_base_gripper (Pose of gripper in base)
        
        T_gripper_targets = []
        for i in range(len(R_gripper2base)):
            T_bg = np.eye(4)
            T_bg[:3, :3] = R_gripper2base[i]
            T_bg[:3, 3] = t_gripper2base[i]
            
            T_tc = np.eye(4)
            T_tc[:3, :3] = R_target2cam[i]
            T_tc[:3, 3] = t_target2cam[i]
            
            # T_gt = inv(T_bg) * inv(T_cb) * T_tc
            T_gt = np.linalg.inv(T_bg) @ np.linalg.inv(T_cam_base) @ T_tc
            T_gripper_targets.append(T_gt)
        
        # Average T_gripper_target
        # Simple averaging for translation, proper averaging for rotation
        t_gt_mean = np.mean([T[:3, 3] for T in T_gripper_targets], axis=0)
        R_gt_mean_obj = R.from_matrix([T[:3, :3] for T in T_gripper_targets]).mean()
        T_gripper_target = np.eye(4)
        T_gripper_target[:3, :3] = R_gt_mean_obj.as_matrix()
        T_gripper_target[:3, 3] = t_gt_mean
        
        # 3. Optimization parameters: [rx_X, ry_X, rz_X, tx_X, ty_X, tz_X, rx_Z, ry_Z, rz_Z, tx_Z, ty_Z, tz_Z]
        x0 = np.concatenate([
            R.from_matrix(T_cam_base[:3, :3]).as_rotvec(),
            T_cam_base[:3, 3],
            R.from_matrix(T_gripper_target[:3, :3]).as_rotvec(),
            T_gripper_target[:3, 3]
        ])
        
        def residuals(params):
            # Reconstruct X and Z
            r_X = params[0:3]
            t_X = params[3:6]
            r_Z = params[6:9]
            t_Z = params[9:12]
            
            T_X = np.eye(4)
            T_X[:3, :3] = R.from_rotvec(r_X).as_matrix()
            T_X[:3, 3] = t_X
            
            T_Z = np.eye(4)
            T_Z[:3, :3] = R.from_rotvec(r_Z).as_matrix()
            T_Z[:3, 3] = t_Z
            
            res = []
            for i in range(len(R_gripper2base)):
                T_bg = np.eye(4)
                T_bg[:3, :3] = R_gripper2base[i]
                T_bg[:3, 3] = t_gripper2base[i]
                
                T_tc_obs = np.eye(4)
                T_tc_obs[:3, :3] = R_target2cam[i]
                T_tc_obs[:3, 3] = t_target2cam[i]
                
                # Predicted T_tc = X * T_bg * Z
                T_tc_pred = T_X @ T_bg @ T_Z
                
                # Error in translation
                diff_t = T_tc_pred[:3, 3] - T_tc_obs[:3, 3]
                res.extend(diff_t)
                
                # Error in rotation (angle-axis)
                diff_R = T_tc_pred[:3, :3] @ T_tc_obs[:3, :3].T
                diff_r = R.from_matrix(diff_R).as_rotvec()
                res.extend(diff_r * 0.1) # Weight rotation less (approx 0.1m per radian)
            
            return np.array(res)
            
        res = least_squares(residuals, x0, verbose=0)
        
        # Extract optimized X
        params_opt = res.x
        r_X_opt = params_opt[0:3]
        t_X_opt = params_opt[3:6]
        
        R_opt = R.from_rotvec(r_X_opt).as_matrix()
        t_opt = t_X_opt.reshape(3, 1)
        
        # Calculate error
        error = self.evaluate_calibration(R_opt, t_opt, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam)
        
        return R_opt, t_opt, error

    
    def analyze_data_quality(self, R_gripper2base, t_gripper2base, R_target2cam, t_target2cam):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("   æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        # æ£€æŸ¥å¹³ç§»é‡å˜åŒ–
        translations = np.array(t_gripper2base)
        translation_range = np.ptp(translations, axis=0)
        print(f"   æœºå™¨äººå¹³ç§»èŒƒå›´: X={translation_range[0]:.3f}m, Y={translation_range[1]:.3f}m, Z={translation_range[2]:.3f}m")
        
        # æ£€æŸ¥æ—‹è½¬é‡å˜åŒ–
        rotations = []
        for rot_matrix in R_gripper2base:
            r = R.from_matrix(rot_matrix)
            rotations.append(r.as_euler('xyz', degrees=True))
        
        rotations = np.array(rotations)
        rotation_range = np.ptp(rotations, axis=0)
        print(f"   æœºå™¨äººæ—‹è½¬èŒƒå›´: Roll={rotation_range[0]:.1f}Â°, Pitch={rotation_range[1]:.1f}Â°, Yaw={rotation_range[2]:.1f}Â°")
        
        # å»ºè®®
        if np.any(translation_range < 0.05):
            print("   âš ï¸  å»ºè®®å¢åŠ å¹³ç§»å˜åŒ–é‡ (>5cm)")
        if np.any(rotation_range < 10):
            print("   âš ï¸  å»ºè®®å¢åŠ æ—‹è½¬å˜åŒ–é‡ (>10Â°)")
    
    def evaluate_calibration(self, R_cam2base, t_cam2base, 
                           R_gripper2base, t_gripper2base,
                           R_target2cam, t_target2cam):
        """è¯„ä¼°æ ‡å®šç»“æœ (ä½¿ç”¨ä¸€è‡´æ€§æ ‡å‡†å·®ä½œä¸ºæŒ‡æ ‡)"""
        # æ„å»º T_cam_base
        T_cam_base = np.eye(4)
        T_cam_base[:3, :3] = R_cam2base
        T_cam_base[:3, 3] = t_cam2base.flatten()
        
        # è®¡ç®—æ‰€æœ‰ä½å§¿ä¸‹çš„ T_target_gripper
        # T_target_gripper = inv(T_gripper_base) * T_cam_base * T_target_cam
        
        target_gripper_translations = []
        
        for i in range(len(R_gripper2base)):
            T_gripper_base = np.eye(4)
            T_gripper_base[:3, :3] = R_gripper2base[i]
            T_gripper_base[:3, 3] = t_gripper2base[i]
            
            T_target_cam = np.eye(4)
            T_target_cam[:3, :3] = R_target2cam[i]
            T_target_cam[:3, 3] = t_target2cam[i]
            
            T_target_gripper = np.linalg.inv(T_gripper_base) @ T_cam_base @ T_target_cam
            target_gripper_translations.append(T_target_gripper[:3, 3])
            
        # è®¡ç®—å¹³ç§»çš„æ ‡å‡†å·® (mm)
        translations = np.array(target_gripper_translations)
        std_dev = np.std(translations, axis=0)
        mean_std_dev = np.mean(std_dev) * 1000.0  # è½¬æ¢ä¸ºmm
        
        return mean_std_dev
    
    def analyze_calibration_result(self, T_cam2base):
        """åˆ†ææ ‡å®šç»“æœ"""
        print("\\nğŸ“‹ æ ‡å®šç»“æœåˆ†æ:")
        
        # ç›¸æœºä½ç½®
        cam_pos = T_cam2base[:3, 3]
        print(f"   ç›¸æœºä½ç½®: [{cam_pos[0]:.3f}, {cam_pos[1]:.3f}, {cam_pos[2]:.3f}] m")
        
        # ç›¸æœºå§¿æ€
        r = R.from_matrix(T_cam2base[:3, :3])
        cam_euler = r.as_euler('xyz', degrees=True)
        print(f"   ç›¸æœºå§¿æ€: Roll={cam_euler[0]:.1f}Â°, Pitch={cam_euler[1]:.1f}Â°, Yaw={cam_euler[2]:.1f}Â°")
        
        # ä¸é¢„æœŸçš„æ¯”è¾ƒ (å¦‚æœæœ‰å¤–å‚å‚è€ƒ)
        if self.camera_extrinsics is not None:
            pos_diff = np.linalg.norm(cam_pos - self.camera_extrinsics[:3, 3])
            print(f"   ä¸å‚è€ƒå¤–å‚ä½ç½®å·®å¼‚: {pos_diff*1000:.1f}mm")
    
    def save_calibration_result(self, T_cam2base, method, error):
        """ä¿å­˜æ ‡å®šç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜numpyæ ¼å¼
        result_file = os.path.join(self.output_dir, f"camera_extrinsics_{timestamp}.npy")
        np.save(result_file, T_cam2base)
        
        # ä¿å­˜YAMLæ ¼å¼
        yaml_file = os.path.join(self.output_dir, f"camera_extrinsics_{timestamp}.yaml")
        result_data = {
            'calibration_info': {
                'method': method,
                'error': float(error),
                'timestamp': timestamp,
                'n_poses': len(self.robot_poses)
            },
            'camera_extrinsics': T_cam2base.tolist()
        }
        
        with open(yaml_file, 'w') as f:
            yaml.dump(result_data, f, default_flow_style=False)
        
        print(f"\\nğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜:")
        print(f"   {result_file}")
        print(f"   {yaml_file}")
    
    def evaluate_calibration_consistency(self, T_cam_base):
        """è¯„ä¼°æ ‡å®šç»“æœçš„ä¸€è‡´æ€§ (ä»¿ç…§ Eye-in-Hand)"""
        if T_cam_base is None:
            return
        
        print("\nğŸ“Š æ ‡å®šç»“æœä¸€è‡´æ€§è¯„ä¼°")
        print("="*70)
        
        errors = []
        
        # å¯¹äºçœ¼åœ¨æ‰‹å¤– (Eye-to-Hand)ï¼Œæ ‡å®šæ¿å›ºå®šåœ¨æœºæ¢°è‡‚æœ«ç«¯
        # å› æ­¤ T_target_gripper åº”è¯¥æ˜¯æ’å®šçš„
        # T_target_gripper = inv(T_gripper_base) * T_cam_base * T_target_cam
        
        T_target_grippers = []
        
        for i in range(len(self.robot_poses)):
            T_gb = self.robot_poses[i]
            T_tc = self.target_poses[i]
            
            # è®¡ç®— T_target_gripper
            T_tg = np.linalg.inv(T_gb) @ T_cam_base @ T_tc
            T_target_grippers.append(T_tg)
            
        # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„è¯¯å·®
        for i in range(len(T_target_grippers)):
            for j in range(i + 1, len(T_target_grippers)):
                T_tg1 = T_target_grippers[i]
                T_tg2 = T_target_grippers[j]
                
                # ç›¸å¯¹è¯¯å·® T_diff = T_tg1 * inv(T_tg2)
                T_diff = T_tg1 @ np.linalg.inv(T_tg2)
                
                error_trans = np.linalg.norm(T_diff[:3, 3]) * 1000  # mm
                error_rot = np.linalg.norm(R.from_matrix(T_diff[:3, :3]).as_rotvec()) * 180 / np.pi  # deg
                
                errors.append({
                    'pair': (i, j),
                    'trans_error': error_trans,
                    'rot_error': error_rot
                })
        
        if not errors:
            print("   æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè¯„ä¼°")
            return

        # ç»Ÿè®¡
        trans_errors = [e['trans_error'] for e in errors]
        rot_errors = [e['rot_error'] for e in errors]
        
        print(f"\nä¸€è‡´æ€§è¯¯å·® (æ ‡å®šæ¿ç›¸å¯¹äºæœ«ç«¯çš„ä¸€è‡´æ€§):")
        print(f"   å¹³ç§»è¯¯å·®: å¹³å‡={np.mean(trans_errors):.2f}mm, æœ€å¤§={np.max(trans_errors):.2f}mm")
        print(f"   æ—‹è½¬è¯¯å·®: å¹³å‡={np.mean(rot_errors):.2f}Â°, æœ€å¤§={np.max(rot_errors):.2f}Â°")
        
        # è´¨é‡è¯„ä¼°
        if np.mean(trans_errors) < 10 and np.mean(rot_errors) < 2:
             print("\n   âœ… æ ‡å®šè´¨é‡: ä¼˜ç§€")
        elif np.mean(trans_errors) < 20 and np.mean(rot_errors) < 5:
             print("\n   âš ï¸  æ ‡å®šè´¨é‡: è‰¯å¥½")
        else:
             print("\n   âŒ æ ‡å®šè´¨é‡: ä¸€èˆ¬/è¾ƒå·®")
             
        print("="*70)
    
    def load_session_data(self, session_dir):
        """ä»ä¼šè¯ç›®å½•åŠ è½½æ•°æ®"""
        try:
            import glob
            pose_files = sorted(glob.glob(os.path.join(session_dir, "pose_*.npz")))
            
            if not pose_files:
                print(f"âŒ ä¼šè¯ç›®å½•ä¸­æ²¡æœ‰æ•°æ®æ–‡ä»¶: {session_dir}")
                return False
            
            self.robot_poses = []
            self.target_poses = []
            
            for f in pose_files:
                data = np.load(f)
                self.robot_poses.append(data['robot_pose'])
                self.target_poses.append(data['target_pose'])
            
            print(f"âœ… ä»ä¼šè¯ç›®å½•åŠ è½½æ•°æ®: {session_dir}")
            print(f"   æ•°æ®ç‚¹æ•°é‡: {len(self.robot_poses)}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ä¼šè¯æ•°æ®å¤±è´¥: {e}")
            return False

    def run_calibration_workflow(self, mode="all"):
        """è¿è¡Œå®Œæ•´çš„æ ‡å®šæµç¨‹"""
        if mode in ["collect", "all"]:
            print("ğŸ¬ æ­¥éª¤1: é‡‡é›†æ ‡å®šæ•°æ®")
            success = self.capture_calibration_data()
            if not success:
                print("âŒ æ•°æ®é‡‡é›†å¤±è´¥")
                return False
        
        if mode in ["calibrate", "all"]:
            print("\\nğŸ¬ æ­¥éª¤2: æ‰§è¡Œæ ‡å®šè®¡ç®—")
            
            # å¦‚æœæ˜¯ä»…æ ‡å®šæ¨¡å¼ï¼Œå°è¯•åŠ è½½æœ€æ–°çš„æ•°æ®
            if mode == "calibrate" and not self.robot_poses:
                # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
                import glob
                
                # 1. å°è¯•æŸ¥æ‰¾èšåˆæ–‡ä»¶ (æ—§æ ¼å¼)
                pose_files = glob.glob(os.path.join(self.output_dir, "calibration_poses_*.npz"))
                
                # 2. å°è¯•æŸ¥æ‰¾ä¼šè¯ç›®å½• (æ–°æ ¼å¼)
                session_dirs = sorted(glob.glob(os.path.join(self.output_dir, "session_*")))
                
                if pose_files:
                    latest_file = max(pose_files, key=os.path.getctime)
                    print(f"â„¹ï¸  å‘ç°èšåˆæ•°æ®æ–‡ä»¶: {latest_file}")
                    if not self.load_calibration_data(latest_file):
                        print("âŒ æ— æ³•åŠ è½½æ ‡å®šæ•°æ®")
                        return False
                elif session_dirs:
                    latest_session = session_dirs[-1]
                    print(f"â„¹ï¸  å‘ç°æœ€æ–°ä¼šè¯ç›®å½•: {latest_session}")
                    if not self.load_session_data(latest_session):
                        print("âŒ æ— æ³•åŠ è½½ä¼šè¯æ•°æ®")
                        return False
                else:
                    print("âŒ æœªæ‰¾åˆ°æ ‡å®šæ•°æ®æ–‡ä»¶ (æ—¢æ— èšåˆæ–‡ä»¶ä¹Ÿæ— ä¼šè¯ç›®å½•)")
                    return False
            
            result = self.calibrate_eye_to_hand()
            if result is None:
                print("âŒ æ ‡å®šè®¡ç®—å¤±è´¥")
                return False
            
            print("\\nâœ… çœ¼åœ¨æ‰‹å¤–æ ‡å®šå®Œæˆ!")
        
        return True
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'cap') and self.cap is not None:
            self.cap.release()
        if hasattr(self, 'controller'):
            self.controller.close()
        cv2.destroyAllWindows()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çœ¼åœ¨æ‰‹å¤–æ‰‹çœ¼æ ‡å®š")
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--collect", action="store_true", help="ä»…é‡‡é›†æ•°æ®")
    parser.add_argument("--calibrate", action="store_true", help="ä»…æ‰§è¡Œæ ‡å®š")
    parser.add_argument("--all", action="store_true", help="é‡‡é›†æ•°æ®+æ‰§è¡Œæ ‡å®š")
    
    # ç¡¬ä»¶é…ç½®
    parser.add_argument("--camera", type=int, default=0, help="ç›¸æœºè®¾å¤‡ID")
    parser.add_argument("--port", default="/dev/left_arm", help="æœºæ¢°è‡‚ä¸²å£")
    parser.add_argument("--output-dir", default="./handeye_data_environment", help="è¾“å‡ºç›®å½•")
    # æ–‡ä»¶é…ç½®
    parser.add_argument("--camera-params", default="./config_data/camera_intrinsics_environment.yaml", help="ç›¸æœºå†…å¤–å‚æ–‡ä»¶ (OpenCV YAMLæ ¼å¼)")

    args = parser.parse_args()
    
    # ç¡®å®šè¿è¡Œæ¨¡å¼
    if not any([args.collect, args.calibrate, args.all]):
        args.all = True  # é»˜è®¤è¿è¡Œå®Œæ•´æµç¨‹
    
    if args.collect:
        mode = "collect"
    elif args.calibrate:
        mode = "calibrate"
    else:
        mode = "all"
    
    try:
        # åˆ›å»ºæ ‡å®šå™¨
        calibrator = EyeToHandCalibrator(
            camera_id=args.camera,
            port=args.port,
            camera_params_file=args.camera_params,
            output_dir=args.output_dir
        )
        
        # è¿è¡Œæ ‡å®šæµç¨‹
        success = calibrator.run_calibration_workflow(mode)
        
        if success:
            print("\\nğŸ‰ çœ¼åœ¨æ‰‹å¤–æ ‡å®šæˆåŠŸå®Œæˆ!")
        else:
            print("\\nâŒ çœ¼åœ¨æ‰‹å¤–æ ‡å®šå¤±è´¥!")
            return 1
    
    except KeyboardInterrupt:
        print("\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\\nâŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())