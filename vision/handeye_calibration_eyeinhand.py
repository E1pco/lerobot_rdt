#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœ¼åœ¨æ‰‹ä¸Š (Eye-in-Hand) æ‰‹çœ¼æ ‡å®šè„šæœ¬
=====================================
åŠŸèƒ½:
  1. æ§åˆ¶æœºæ¢°è‡‚ç§»åŠ¨åˆ°ä¸åŒä½å§¿
  2. åœ¨æ¯ä¸ªä½å§¿é‡‡é›†å›¾åƒå¹¶æ£€æµ‹æ£‹ç›˜æ ¼
  3. ä½¿ç”¨PnPè®¡ç®—æ£‹ç›˜æ ¼åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿ (T_target_cam)
  4. ä½¿ç”¨æ­£è¿åŠ¨å­¦è®¡ç®—æœ«ç«¯åœ¨åŸºåº§æ ‡ç³»ä¸‹çš„ä½å§¿ (T_gripper_base)
  5. ä½¿ç”¨Tsai-Lenzæ–¹æ³•æ±‚è§£æ‰‹çœ¼å˜æ¢çŸ©é˜µ (T_cam_gripper)

ä½¿ç”¨æ–¹æ³•:
  python handeye_calibration_eyeinhand.py --collect   # é‡‡é›†æ•°æ®
  python handeye_calibration_eyeinhand.py --calibrate # æ ‡å®šè®¡ç®—
  python handeye_calibration_eyeinhand.py --all       # é‡‡é›†+æ ‡å®š

åæ ‡ç³»å®šä¹‰:
  - base: æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
  - gripper/end-effector: æœºæ¢°è‡‚æœ«ç«¯åæ ‡ç³»
  - cam: ç›¸æœºåæ ‡ç³»
  - target: æ ‡å®šæ¿åæ ‡ç³»

çœ¼åœ¨æ‰‹ä¸Šæ–¹ç¨‹: AX = XB
  - A: ç›¸é‚»ä¸¤ä¸ªæœ«ç«¯ä½å§¿çš„ç›¸å¯¹å˜æ¢
  - B: ç›¸é‚»ä¸¤ä¸ªæ ‡å®šæ¿ä½å§¿çš„ç›¸å¯¹å˜æ¢  
  - X: ç›¸æœºç›¸å¯¹äºæœ«ç«¯çš„å˜æ¢ (T_cam_gripper)
"""

import os
import sys
import cv2
import numpy as np
import time
import argparse
from datetime import datetime
from scipy.spatial.transform import Rotation as R

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof, create_so101_5dof_gripper


class HandEyeCalibrator:
    """çœ¼åœ¨æ‰‹ä¸Šæ‰‹çœ¼æ ‡å®šå™¨"""
    
    def __init__(self, 
                 board_size=(11, 8),
                 square_size=0.02073,  # 20.73mm
                 intrinsic_file=None,  # é»˜è®¤ä½¿ç”¨è„šæœ¬ç›®å½•ä¸‹çš„æ–‡ä»¶
                 output_dir='./handeye_data'):
        """
        Parameters
        ----------
        board_size : tuple
            æ£‹ç›˜æ ¼å†…è§’ç‚¹æ•° (cols, rows)
        square_size : float
            æ£‹ç›˜æ ¼æ–¹æ ¼å¤§å° (ç±³)
        intrinsic_file : str
            ç›¸æœºå†…å‚æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨è„šæœ¬ç›®å½•ä¸‹çš„ camera_intrinsics.yaml
        output_dir : str
            æ•°æ®ä¿å­˜ç›®å½•
        """
        self.board_size = board_size
        self.square_size = square_size
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å¦‚æœæœªæŒ‡å®šå†…å‚æ–‡ä»¶ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„æ–‡ä»¶
        if intrinsic_file is None:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            intrinsic_file = os.path.join(script_dir, 'camera_intrinsics.yaml')
        
        # åŠ è½½ç›¸æœºå†…å‚
        self.load_camera_intrinsics(intrinsic_file)
        
        # æ„é€ æ£‹ç›˜æ ¼3Dç‚¹
        self.objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)
        self.objp[:, :2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1, 2)
        self.objp *= square_size
        
        # åˆå§‹åŒ–æœºå™¨äººå’Œæ§åˆ¶å™¨
        self.robot = None
        self.controller = None
        
        # å­˜å‚¨æ ‡å®šæ•°æ®
        self.T_target_cam_list = []  # æ ‡å®šæ¿åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿
        self.T_gripper_base_list = []  # æœ«ç«¯åœ¨åŸºåº§æ ‡ç³»ä¸‹çš„ä½å§¿
        self.images = []
        
        # PnPç¨³å®šæ€§ç›¸å…³
        self.pose_buffer = []  # ç”¨äºå¤šå¸§å¹³å‡
        self.pose_buffer_size = 5  # ç¼“å†²åŒºå¤§å°
        
        print("="*70)
        print("ğŸ¤– çœ¼åœ¨æ‰‹ä¸Š (Eye-in-Hand) æ‰‹çœ¼æ ‡å®šå·¥å…·")
        print("="*70)
        print(f"\næ£‹ç›˜æ ¼å‚æ•°:")
        print(f"  å†…è§’ç‚¹: {board_size[0]} Ã— {board_size[1]}")
        print(f"  æ–¹æ ¼å¤§å°: {square_size*1000:.2f} mm")
        print(f"\næ•°æ®ä¿å­˜ç›®å½•: {os.path.abspath(output_dir)}")
        print("="*70)
    
    def load_camera_intrinsics(self, yaml_path):
        """åŠ è½½ç›¸æœºå†…å‚"""
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
        
        fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
        self.K = fs.getNode('K').mat()
        self.dist = fs.getNode('distCoeffs').mat().flatten()
        
        # ç„¦è·ä¿®æ­£ - æ ¹æ®å®é™…æµ‹é‡ç»“æœä¿®æ­£
        # åŸå§‹æµ‹é‡ 647mmï¼Œå®é™… 600mmï¼Œä¿®æ­£ç³»æ•° = 600/647
        correction_factor = 67/70
        K_original_fx = self.K[0, 0]
        K_original_fy = self.K[1, 1]
        self.K[0, 0] *= correction_factor  # fx
        self.K[1, 1] *= correction_factor  # fy
        
        print(f"\nğŸ“· ç„¦è·ä¿®æ­£:")
        print(f"   åŸå§‹: fx={K_original_fx:.1f}, fy={K_original_fy:.1f}")
        print(f"   ä¿®æ­£ç³»æ•°: {correction_factor:.4f} (600/647)")
        print(f"   ä¿®æ­£å: fx={self.K[0,0]:.1f}, fy={self.K[1,1]:.1f}")
        
        # å°è¯•è¯»å–æ–‡ä»¶ä¸­ä¿å­˜çš„æ–¹æ ¼å¤§å°
        square_size_node = fs.getNode('square_size')
        if not square_size_node.empty():
            file_square_size = square_size_node.real()
            if abs(file_square_size - self.square_size) > 0.0001:
                print(f"\nâš ï¸  è­¦å‘Š: æ–¹æ ¼å¤§å°ä¸ä¸€è‡´!")
                print(f"   å†…å‚æ–‡ä»¶ä¸­: {file_square_size*1000:.2f} mm")
                print(f"   å½“å‰è®¾ç½®: {self.square_size*1000:.2f} mm")
                print(f"   ä½¿ç”¨å†…å‚æ–‡ä»¶ä¸­çš„å€¼...")
                self.square_size = file_square_size
                # é‡æ–°æ„é€ objp
                self.objp = np.zeros((self.board_size[0] * self.board_size[1], 3), np.float32)
                self.objp[:, :2] = np.mgrid[0:self.board_size[0], 0:self.board_size[1]].T.reshape(-1, 2)
                self.objp *= self.square_size
        
        fs.release()
        
        print(f"\nğŸ“· å·²åŠ è½½ç›¸æœºå†…å‚: {os.path.abspath(yaml_path)}")
        print(f"   fx={self.K[0,0]:.1f}, fy={self.K[1,1]:.1f}")
        print(f"   cx={self.K[0,2]:.1f}, cy={self.K[1,2]:.1f}")
        print(f"   æ–¹æ ¼å¤§å°: {self.square_size*1000:.2f} mm")
    
    def init_robot(self, port="/dev/ttyACM0", baudrate=1_000_000):
        """åˆå§‹åŒ–æœºå™¨äººå’Œæ§åˆ¶å™¨"""
        print("\nğŸ¤– åˆå§‹åŒ–æœºå™¨äºº...")
        
        self.controller = ServoController(
            port=port, 
            baudrate=baudrate, 
            config_path=os.path.join(os.path.dirname(__file__), "../driver/servo_config.json")
        )
        self.robot = create_so101_5dof_gripper()
        self.robot.set_servo_controller(self.controller)
        
        print("âœ… æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
        return True
    
    def read_robot_pose(self, verbose=True):
        """
        è¯»å–æœºå™¨äººå½“å‰æœ«ç«¯ä½å§¿
        
        Returns
        -------
        T_gripper_base : np.ndarray
            4x4 æœ«ç«¯åœ¨åŸºåº§æ ‡ç³»ä¸‹çš„ä½å§¿çŸ©é˜µ
        q : np.ndarray
            å…³èŠ‚è§’åº¦ (å¼§åº¦)
        """
        # è¯»å–å…³èŠ‚è§’åº¦
        q = self.robot.read_joint_angles(
            joint_names=self.robot.joint_names,
            verbose=verbose
        )
        
        # æ­£è¿åŠ¨å­¦è®¡ç®—æœ«ç«¯ä½å§¿
        T_gripper_base = self.robot.fkine(q)
        
        if verbose:
            pos = T_gripper_base[:3, 3]
            euler = R.from_matrix(T_gripper_base[:3, :3]).as_euler('xyz', degrees=True)
            print(f"\nğŸ“ æœ«ç«¯ä½å§¿:")
            print(f"   ä½ç½®: x={pos[0]*1000:.1f}mm, y={pos[1]*1000:.1f}mm, z={pos[2]*1000:.1f}mm")
            print(f"   å§¿æ€: roll={euler[0]:.1f}Â°, pitch={euler[1]:.1f}Â°, yaw={euler[2]:.1f}Â°")
        
        return T_gripper_base, q
    
    def detect_chessboard(self, frame, refine_pose=True):
        """
        æ£€æµ‹æ£‹ç›˜æ ¼å¹¶è®¡ç®—å…¶åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿ (ä½¿ç”¨RANSAC PnPæ–¹æ³•)
        
        ç»è¿‡æµ‹è¯•éªŒè¯ï¼ŒRANSAC PnPæ–¹æ³•åœ¨ç¨³å®šæ€§å’Œç²¾åº¦ä¸Šè¡¨ç°æœ€ä½³ã€‚
        
        Parameters
        ----------
        frame : np.ndarray
            è¾“å…¥å›¾åƒ
        refine_pose : bool
            æ˜¯å¦ä½¿ç”¨LMä¼˜åŒ–ç²¾åŒ–ä½å§¿
        
        Returns
        -------
        success : bool
            æ˜¯å¦æˆåŠŸæ£€æµ‹åˆ°æ£‹ç›˜æ ¼
        T_target_cam : np.ndarray or None
            4x4 æ ‡å®šæ¿åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿çŸ©é˜µ
        corners : np.ndarray or None
            æ£€æµ‹åˆ°çš„è§’ç‚¹
        reproj_error : float
            é‡æŠ•å½±è¯¯å·® (åƒç´ )
        """
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹æ£‹ç›˜æ ¼ - å¢åŠ FAST_CHECKåŠ é€Ÿ
        flags = (cv2.CALIB_CB_ADAPTIVE_THRESH + 
                cv2.CALIB_CB_NORMALIZE_IMAGE + 
                cv2.CALIB_CB_FAST_CHECK)
        found, corners = cv2.findChessboardCorners(gray, self.board_size, flags)
        
        if not found:
            return False, None, None, float('inf')
        
        # äºšåƒç´ ç²¾åŒ– - ä½¿ç”¨æ›´ä¸¥æ ¼çš„ç»ˆæ­¢æ¡ä»¶
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001)
        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
        
        # PnPæ±‚è§£ - ä½¿ç”¨å¤šç§æ–¹æ³•ï¼Œé€‰æ‹©æœ€ä½³ç»“æœ
        imgp = corners.reshape(-1, 2).astype(np.float32)
        
        best_result = None
        best_reproj_error = float('inf')
        
        # å°è¯•å¤šç§PnPæ–¹æ³•
        pnp_methods = [
            ('RANSAC', None),
            ('ITERATIVE', cv2.SOLVEPNP_ITERATIVE),
            ('EPNP', cv2.SOLVEPNP_EPNP),
            ('IPPE', cv2.SOLVEPNP_IPPE),
            ('SQPNP', cv2.SOLVEPNP_SQPNP),
        ]
        
        for method_name, method_flag in pnp_methods:
            try:
                if method_name == 'RANSAC':
                    success, rvec, tvec, inliers = cv2.solvePnPRansac(
                        self.objp, imgp, self.K, self.dist,
                        iterationsCount=1000,
                        reprojectionError=2.0,
                        flags=cv2.SOLVEPNP_ITERATIVE
                    )
                else:
                    success, rvec, tvec = cv2.solvePnP(
                        self.objp, imgp, self.K, self.dist, flags=method_flag
                    )
                
                if not success:
                    continue
                
                # LMä¼˜åŒ–ç²¾åŒ–ä½å§¿
                if refine_pose:
                    try:
                        rvec, tvec = cv2.solvePnPRefineLM(
                            self.objp, imgp, self.K, self.dist, rvec, tvec
                        )
                    except:
                        pass
                
                # è®¡ç®—é‡æŠ•å½±è¯¯å·®
                reproj_pts, _ = cv2.projectPoints(self.objp, rvec, tvec, self.K, self.dist)
                reproj_error = np.sqrt(np.mean(np.sum((imgp - reproj_pts.reshape(-1, 2))**2, axis=1)))
                
                # é€‰æ‹©é‡æŠ•å½±è¯¯å·®æœ€å°çš„ç»“æœ
                if reproj_error < best_reproj_error:
                    best_reproj_error = reproj_error
                    best_result = (rvec.copy(), tvec.copy())
                    
            except Exception as e:
                continue
        
        # å¦‚æœæ²¡æœ‰æˆåŠŸçš„ç»“æœ
        if best_result is None:
            return False, None, corners, float('inf')
        
        rvec, tvec = best_result
        
        # æ„é€ 4x4å˜æ¢çŸ©é˜µ
        R_mat, _ = cv2.Rodrigues(rvec)
        T_target_cam = np.eye(4)
        T_target_cam[:3, :3] = R_mat
        T_target_cam[:3, 3] = tvec.squeeze()
        
        return True, T_target_cam, corners, best_reproj_error
    
    def get_stable_pose(self, frame, num_samples=5, max_std_trans=5.0, max_std_rot=2.0):
        """
        è·å–ç¨³å®šçš„PnPä½å§¿ (å¤šæ¬¡é‡‡æ ·å–å¹³å‡)
        
        Parameters
        ----------
        frame : np.ndarray
            è¾“å…¥å›¾åƒ (å®é™…ä¼šé‡æ–°ä»ç›¸æœºé‡‡é›†å¤šå¸§)
        num_samples : int
            é‡‡æ ·æ¬¡æ•°
        max_std_trans : float
            å…è®¸çš„æœ€å¤§å¹³ç§»æ ‡å‡†å·® (mm)
        max_std_rot : float
            å…è®¸çš„æœ€å¤§æ—‹è½¬æ ‡å‡†å·® (åº¦)
        
        Returns
        -------
        success : bool
            æ˜¯å¦æˆåŠŸè·å–ç¨³å®šä½å§¿
        T_avg : np.ndarray
            å¹³å‡ä½å§¿çŸ©é˜µ
        std_info : dict
            æ ‡å‡†å·®ä¿¡æ¯
        """
        # ç›´æ¥ç”¨ä¼ å…¥çš„å¸§è¿›è¡Œå¤šæ¬¡æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿå¤šå¸§ï¼Œå®é™…åº”è¯¥ç”¨capå¤šæ¬¡è¯»å–ï¼‰
        poses = []
        
        success, T, corners, err = self.detect_chessboard(frame)
        if not success:
            return False, None, {'error': 'detection_failed'}
        
        # ç”±äºæ˜¯é™æ€åœºæ™¯ï¼Œæˆ‘ä»¬è¿›è¡Œå¤šæ¬¡PnPæ±‚è§£æ¥è¯„ä¼°ç¨³å®šæ€§
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        imgp = corners.reshape(-1, 2).astype(np.float32)
        
        for _ in range(num_samples):
            # ä½¿ç”¨ä¸åŒçš„PnPæ–¹æ³•æ±‚è§£
            methods = [
                cv2.SOLVEPNP_ITERATIVE,
                cv2.SOLVEPNP_EPNP,
                cv2.SOLVEPNP_IPPE,
                cv2.SOLVEPNP_SQPNP,
            ]
            
            for method in methods:
                try:
                    success, rvec, tvec = cv2.solvePnP(
                        self.objp, imgp, self.K, self.dist, flags=method
                    )
                    if success:
                        # LMä¼˜åŒ–
                        rvec, tvec = cv2.solvePnPRefineLM(
                            self.objp, imgp, self.K, self.dist, rvec, tvec
                        )
                        poses.append((rvec.copy(), tvec.copy()))
                except:
                    continue
        
        if len(poses) < 3:
            return False, None, {'error': 'insufficient_samples'}
        
        # è®¡ç®—å¹³å‡ä½å§¿
        tvecs = np.array([p[1].flatten() for p in poses])
        rvecs = np.array([p[0].flatten() for p in poses])
        
        # å¹³ç§»çš„å‡å€¼å’Œæ ‡å‡†å·®
        t_mean = np.mean(tvecs, axis=0)
        t_std = np.std(tvecs, axis=0) * 1000  # è½¬ä¸ºmm
        
        # æ—‹è½¬çš„æ ‡å‡†å·® (ç®€åŒ–å¤„ç†)
        r_std = np.std(rvecs, axis=0) * 180 / np.pi  # è½¬ä¸ºåº¦
        
        std_info = {
            't_std_mm': t_std,
            'r_std_deg': r_std,
            't_std_norm': np.linalg.norm(t_std),
            'r_std_norm': np.linalg.norm(r_std)
        }
        
        # æ£€æŸ¥ç¨³å®šæ€§
        if np.linalg.norm(t_std) > max_std_trans or np.linalg.norm(r_std) > max_std_rot:
            return False, None, std_info
        
        # æ„é€ å¹³å‡ä½å§¿çŸ©é˜µ
        r_mean = np.mean(rvecs, axis=0)
        R_mat, _ = cv2.Rodrigues(r_mean)
        T_avg = np.eye(4)
        T_avg[:3, :3] = R_mat
        T_avg[:3, 3] = t_mean
        
        return True, T_avg, std_info
    
    def update_pose_buffer(self, T):
        """æ›´æ–°ä½å§¿ç¼“å†²åŒºç”¨äºæ»‘åŠ¨å¹³å‡"""
        self.pose_buffer.append(T.copy())
        if len(self.pose_buffer) > self.pose_buffer_size:
            self.pose_buffer.pop(0)
    
    def get_averaged_pose(self):
        """ä»ç¼“å†²åŒºè·å–å¹³å‡ä½å§¿"""
        if len(self.pose_buffer) < 3:
            return None
        
        # å¹³å‡å¹³ç§»
        translations = np.array([T[:3, 3] for T in self.pose_buffer])
        t_avg = np.mean(translations, axis=0)
        
        # å¹³å‡æ—‹è½¬ (ä½¿ç”¨å››å…ƒæ•°)
        quats = []
        for T in self.pose_buffer:
            q = R.from_matrix(T[:3, :3]).as_quat()
            quats.append(q)
        quats = np.array(quats)
        
        # ç®€å•å¹³å‡å››å…ƒæ•° (å¯¹äºå°è§’åº¦å˜åŒ–è¶³å¤Ÿ)
        q_avg = np.mean(quats, axis=0)
        q_avg /= np.linalg.norm(q_avg)  # å½’ä¸€åŒ–
        
        R_avg = R.from_quat(q_avg).as_matrix()
        
        T_avg = np.eye(4)
        T_avg[:3, :3] = R_avg
        T_avg[:3, 3] = t_avg
        
        return T_avg
    
    def collect_data_interactive(self, cam_id=0):
        """
        äº¤äº’å¼é‡‡é›†æ ‡å®šæ•°æ® (å¢å¼ºç‰ˆ - å¸¦PnPç¨³å®šæ€§æ£€æµ‹)
        
        æŒ‰é”®:
          SPACE - é‡‡é›†å½“å‰ä½å§¿
          'h'   - æœºæ¢°è‡‚å›ä¸­
          's'   - æ˜¾ç¤º/éšè—ç¨³å®šæ€§ä¿¡æ¯
          'q'   - é€€å‡ºé‡‡é›†
        """
        cap = cv2.VideoCapture(cam_id)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº")
            return False
        
        print("\nğŸ“¸ å¼€å§‹äº¤äº’å¼æ•°æ®é‡‡é›† (å¢å¼ºç‰ˆ)")
        print("="*70)
        print("\nâŒ¨ï¸  å¿«æ·é”®:")
        print("   SPACE - é‡‡é›†å½“å‰ä½å§¿æ•°æ®")
        print("   'h'   - æœºæ¢°è‡‚å›ä¸­ä½")
        print("   's'   - æ˜¾ç¤º/éšè—ç¨³å®šæ€§ä¿¡æ¯")
        print("   'q'   - é€€å‡ºé‡‡é›†")
        print("\nğŸ“– é‡‡é›†æŒ‡å—:")
        print("   1. æ‰‹åŠ¨ç§»åŠ¨æœºæ¢°è‡‚åˆ°ä¸åŒä½å§¿")
        print("   2. ç¡®ä¿æ£‹ç›˜æ ¼åœ¨ç›¸æœºè§†é‡å†…")
        print("   3. ç­‰å¾…ä½å§¿ç¨³å®š(ç»¿è‰²)åæŒ‰SPACEé‡‡é›†")
        print("   4. å»ºè®®é‡‡é›† 10-20 ç»„æ•°æ®")
        print("   5. å°½é‡è®©æœºæ¢°è‡‚å§¿æ€å¤šæ ·åŒ–")
        print("="*70 + "\n")
        
        sample_count = 0
        show_stability = True
        
        # æ¸…ç©ºä½å§¿ç¼“å†²åŒº
        self.pose_buffer = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            display = frame.copy()
            
            # æ£€æµ‹æ£‹ç›˜æ ¼å¹¶è·å–é‡æŠ•å½±è¯¯å·®
            success, T_target_cam, corners, reproj_error = self.detect_chessboard(frame)
            
            # åˆ¤æ–­ç¨³å®šæ€§
            is_stable = False
            stability_info = ""
            
            if success and corners is not None:
                cv2.drawChessboardCorners(display, self.board_size, corners, True)
                
                # æ›´æ–°ä½å§¿ç¼“å†²åŒº
                self.update_pose_buffer(T_target_cam)
                
                # è®¡ç®—ä½å§¿ç¨³å®šæ€§
                if len(self.pose_buffer) >= 3:
                    translations = np.array([T[:3, 3] for T in self.pose_buffer])
                    t_std = np.std(translations, axis=0) * 1000  # mm
                    t_std_norm = np.linalg.norm(t_std)
                    
                    # åˆ¤æ–­æ˜¯å¦ç¨³å®š
                    is_stable = t_std_norm < 3.0 and reproj_error < 1.0
                    
                    if show_stability:
                        stability_info = f"Std: {t_std_norm:.1f}mm, ReprojErr: {reproj_error:.2f}px"
                
                # æ˜¾ç¤ºæ ‡å®šæ¿è·ç¦»
                distance = np.linalg.norm(T_target_cam[:3, 3]) * 1000
                
                # æ ¹æ®ç¨³å®šæ€§é€‰æ‹©é¢œè‰²
                color = (0, 255, 0) if is_stable else (0, 255, 255)
                status_text = "STABLE - Press SPACE" if is_stable else "Detecting..."
                
                cv2.putText(display, f"Distance: {distance:.0f}mm", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
                cv2.putText(display, status_text, (10, 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                
                if show_stability and stability_info:
                    cv2.putText(display, stability_info, (10, 90),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            else:
                cv2.putText(display, "Chessboard NOT FOUND", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                # æ¸…ç©ºç¼“å†²åŒº
                self.pose_buffer = []
            
            # æ˜¾ç¤ºé‡‡é›†æ•°é‡
            cv2.putText(display, f"Samples: {sample_count}", (display.shape[1]-150, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            cv2.imshow('Hand-Eye Calibration', display)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("\nğŸ‘‹ é€€å‡ºé‡‡é›†")
                break
            
            elif key == ord('h'):
                print("\nğŸ  æœºæ¢°è‡‚å›ä¸­...")
                self.controller.move_all_home()
                time.sleep(1)
            
            elif key == ord('s'):
                show_stability = not show_stability
                print(f"{'æ˜¾ç¤º' if show_stability else 'éšè—'}ç¨³å®šæ€§ä¿¡æ¯")
            
            elif key == ord(' '):
                if not success:
                    print("âš ï¸  æœªæ£€æµ‹åˆ°æ£‹ç›˜æ ¼ï¼Œæ— æ³•é‡‡é›†")
                    continue
                
                if not is_stable:
                    print("âš ï¸  ä½å§¿ä¸ç¨³å®šï¼Œå»ºè®®ç­‰å¾…ç¨³å®šåå†é‡‡é›†")
                    # ä»ç„¶å…è®¸é‡‡é›†ï¼Œä½†ç»™å‡ºè­¦å‘Š
                
                # ä½¿ç”¨å¹³å‡ä½å§¿ (æ›´ç¨³å®š)
                T_avg = self.get_averaged_pose()
                if T_avg is not None:
                    T_to_save = T_avg
                    print("   ä½¿ç”¨å¹³å‡ä½å§¿")
                else:
                    T_to_save = T_target_cam
                    print("   ä½¿ç”¨å•å¸§ä½å§¿")
                
                sample_count += 1
                print(f"\nğŸ“¸ é‡‡é›†æ•°æ® #{sample_count}")
                
                # è¯»å–æœºå™¨äººä½å§¿
                T_gripper_base, q = self.read_robot_pose(verbose=True)
                
                # ä¿å­˜æ•°æ®
                self.T_target_cam_list.append(T_to_save.copy())
                self.T_gripper_base_list.append(T_gripper_base.copy())
                self.images.append(frame.copy())
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                np.savez(
                    os.path.join(self.output_dir, f"pose_{sample_count:02d}_{timestamp}.npz"),
                    T_target_cam=T_to_save,
                    T_gripper_base=T_gripper_base,
                    q=q,
                    reproj_error=reproj_error
                )
                cv2.imwrite(
                    os.path.join(self.output_dir, f"image_{sample_count:02d}_{timestamp}.jpg"),
                    frame
                )
                
                print(f"âœ… å·²ä¿å­˜æ•°æ® #{sample_count}")
                print(f"   æ ‡å®šæ¿è·ç¦»: {np.linalg.norm(T_to_save[:3,3])*1000:.1f} mm")
                print(f"   é‡æŠ•å½±è¯¯å·®: {reproj_error:.2f} px")
                
                # æ¸…ç©ºç¼“å†²åŒºï¼Œé¿å…ä¸‹ä¸€å¸§ä½¿ç”¨æ—§æ•°æ®
                self.pose_buffer = []
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\nğŸ“Š å…±é‡‡é›† {sample_count} ç»„æ•°æ®")
        return sample_count >= 3
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\nğŸ“Š å…±é‡‡é›† {sample_count} ç»„æ•°æ®")
        return sample_count >= 3
    
    def load_collected_data(self):
        """ä»æ–‡ä»¶åŠ è½½å·²é‡‡é›†çš„æ•°æ®"""
        import glob
        
        pose_files = sorted(glob.glob(os.path.join(self.output_dir, "pose_*.npz")))
        
        if not pose_files:
            print(f"âŒ æœªæ‰¾åˆ°æ ‡å®šæ•°æ®: {self.output_dir}")
            return False
        
        self.T_target_cam_list = []
        self.T_gripper_base_list = []
        
        # ä¸´æ—¶åˆ›å»ºæœºå™¨äººæ¨¡å‹ç”¨äºé‡ç®—FK
        temp_robot = create_so101_5dof_gripper()
        
        print(f"\nğŸ“‚ åŠ è½½æ ‡å®šæ•°æ®...")
        for f in pose_files:
            data = np.load(f)
            self.T_target_cam_list.append(data['T_target_cam'])
            
            # å¦‚æœæœ‰ä¿å­˜å…³èŠ‚è§’åº¦ï¼Œé‡æ–°è®¡ç®—FK (ä»¥é˜²è¿åŠ¨å­¦å‚æ•°æœ‰æ›´æ–°)
            if 'q' in data:
                q = data['q']
                T_gb = temp_robot.fkine(q)
                self.T_gripper_base_list.append(T_gb)
                # print(f"   âœ… {os.path.basename(f)} (Re-computed FK)")
            else:
                self.T_gripper_base_list.append(data['T_gripper_base'])
                # print(f"   âœ… {os.path.basename(f)}")
            print(f"   âœ… {os.path.basename(f)}")
        
        print(f"\nå…±åŠ è½½ {len(self.T_target_cam_list)} ç»„æ•°æ®")
        return True
    
    def calibrate(self, method=cv2.CALIB_HAND_EYE_PARK):
        """
        æ‰§è¡Œæ‰‹çœ¼æ ‡å®š
        
        Parameters
        ----------
        method : int
            æ‰‹çœ¼æ ‡å®šæ–¹æ³•ï¼Œå¯é€‰:
            - cv2.CALIB_HAND_EYE_TSAI
            - cv2.CALIB_HAND_EYE_PARK (é»˜è®¤)
            - cv2.CALIB_HAND_EYE_HORAUD
            - cv2.CALIB_HAND_EYE_ANDREFF
            - cv2.CALIB_HAND_EYE_DANIILIDIS
        
        Returns
        -------
        T_cam_gripper : np.ndarray
            4x4 ç›¸æœºåœ¨æœ«ç«¯åæ ‡ç³»ä¸‹çš„ä½å§¿çŸ©é˜µ
        """
        if len(self.T_target_cam_list) < 3:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ 3 ç»„æ•°æ®")
            return None
        
        print("\nğŸ”„ å¼€å§‹æ‰‹çœ¼æ ‡å®š...")
        print(f"   æ•°æ®ç»„æ•°: {len(self.T_target_cam_list)}")
        
        # å‡†å¤‡æ•°æ®
        R_gripper2base = []
        t_gripper2base = []
        R_target2cam = []
        t_target2cam = []
        
        for T_gb, T_tc in zip(self.T_gripper_base_list, self.T_target_cam_list):
            R_gripper2base.append(T_gb[:3, :3])
            t_gripper2base.append(T_gb[:3, 3].reshape(3, 1))
            R_target2cam.append(T_tc[:3, :3])
            t_target2cam.append(T_tc[:3, 3].reshape(3, 1))
        
        # æ‰§è¡Œæ‰‹çœ¼æ ‡å®š
        R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
            R_gripper2base, t_gripper2base,
            R_target2cam, t_target2cam,
            method=method
        )
        
        # æ„é€ 4x4å˜æ¢çŸ©é˜µ
        T_cam_gripper = np.eye(4)
        T_cam_gripper[:3, :3] = R_cam2gripper
        T_cam_gripper[:3, 3] = t_cam2gripper.squeeze()
        
        print("\nâœ… æ‰‹çœ¼æ ‡å®šå®Œæˆ!")
        print("\nğŸ“Š ç»“æœ (T_cam_gripper - ç›¸æœºç›¸å¯¹äºæœ«ç«¯çš„å˜æ¢):")
        print("-"*70)
        
        # å¹³ç§»
        t = t_cam2gripper.squeeze() * 1000  # è½¬æ¢ä¸ºmm
        print(f"å¹³ç§»å‘é‡ (mm):")
        print(f"   tx = {t[0]:8.2f}")
        print(f"   ty = {t[1]:8.2f}")
        print(f"   tz = {t[2]:8.2f}")
        
        # æ—‹è½¬
        euler = R.from_matrix(R_cam2gripper).as_euler('xyz', degrees=True)
        quat = R.from_matrix(R_cam2gripper).as_quat()
        print(f"\næ—‹è½¬ (æ¬§æ‹‰è§’, åº¦):")
        print(f"   roll  = {euler[0]:8.2f}")
        print(f"   pitch = {euler[1]:8.2f}")
        print(f"   yaw   = {euler[2]:8.2f}")
        print(f"\nå››å…ƒæ•° (x, y, z, w):")
        print(f"   {quat}")
        
        print("-"*70)
        
        return T_cam_gripper
    
    def evaluate_calibration(self, T_cam_gripper):
        """è¯„ä¼°æ ‡å®šç»“æœçš„ä¸€è‡´æ€§"""
        if T_cam_gripper is None:
            return
        
        print("\nğŸ“Š æ ‡å®šç»“æœè¯„ä¼°")
        print("="*70)
        
        errors = []
        
        # è®¡ç®—AX=XBçš„ä¸€è‡´æ€§è¯¯å·®
        for i in range(len(self.T_gripper_base_list)):
            for j in range(i + 1, len(self.T_gripper_base_list)):
                # ç›¸é‚»ä¸¤å¸§çš„ç›¸å¯¹è¿åŠ¨
                T_gb1 = self.T_gripper_base_list[i]
                T_gb2 = self.T_gripper_base_list[j]
                T_tc1 = self.T_target_cam_list[i]
                T_tc2 = self.T_target_cam_list[j]
                
                # A = T_g2_g1 = inv(T_b_g2) * T_b_g1 (åœ¨Gripperåæ ‡ç³»ä¸‹çš„ç›¸å¯¹è¿åŠ¨)
                A = np.linalg.inv(T_gb2) @ T_gb1
                
                # B = T_c2_c1 = T_c2_t * T_t_c1 = T_c_t2 * inv(T_c_t1) (åœ¨Cameraåæ ‡ç³»ä¸‹çš„ç›¸å¯¹è¿åŠ¨)
                B = T_tc2 @ np.linalg.inv(T_tc1)
                
                # AX å’Œ XB åº”è¯¥ç›¸ç­‰
                AX = A @ T_cam_gripper
                XB = T_cam_gripper @ B
                
                # è®¡ç®—è¯¯å·®
                error_T = AX @ np.linalg.inv(XB)
                error_trans = np.linalg.norm(error_T[:3, 3]) * 1000  # mm
                error_rot = np.linalg.norm(R.from_matrix(error_T[:3, :3]).as_rotvec()) * 180 / np.pi  # deg
                
                errors.append({
                    'pair': (i, j),
                    'trans_error': error_trans,
                    'rot_error': error_rot
                })
        
        # ç»Ÿè®¡
        trans_errors = [e['trans_error'] for e in errors]
        rot_errors = [e['rot_error'] for e in errors]
        
        print(f"\nä¸€è‡´æ€§è¯¯å·® (AX=XB):")
        print(f"   å¹³ç§»è¯¯å·®: å¹³å‡={np.mean(trans_errors):.2f}mm, æœ€å¤§={np.max(trans_errors):.2f}mm")
        print(f"   æ—‹è½¬è¯¯å·®: å¹³å‡={np.mean(rot_errors):.2f}Â°, æœ€å¤§={np.max(rot_errors):.2f}Â°")
        
        # è´¨é‡è¯„ä¼°
        if np.mean(trans_errors) < 30 and np.mean(rot_errors) < 5:
            print("\n   âœ… æ ‡å®šè´¨é‡: ä¼˜ç§€")
        elif np.mean(trans_errors) < 50 and np.mean(rot_errors) < 10:
            print("\n   âš ï¸  æ ‡å®šè´¨é‡: ä¸€èˆ¬")
        else:
            print("\n   âŒ æ ‡å®šè´¨é‡: è¾ƒå·®ï¼Œå»ºè®®é‡æ–°é‡‡é›†æ•°æ®")
        
        print("="*70)
    
    def save_result(self, T_cam_gripper, filename='handeye_result.yaml'):
        """ä¿å­˜æ ‡å®šç»“æœ"""
        if T_cam_gripper is None:
            return
        
        # ä¿å­˜ä¸ºYAML
        filepath = os.path.join(self.output_dir, filename)
        fs = cv2.FileStorage(filepath, cv2.FILE_STORAGE_WRITE)
        fs.write('T_cam_gripper', T_cam_gripper)
        
        # åˆ†è§£ä¿å­˜
        R_mat = T_cam_gripper[:3, :3]
        t_vec = T_cam_gripper[:3, 3]
        euler = R.from_matrix(R_mat).as_euler('xyz', degrees=True)
        quat = R.from_matrix(R_mat).as_quat()
        
        fs.write('rotation_matrix', R_mat)
        fs.write('translation_vector', t_vec.reshape(3, 1))
        fs.write('euler_angles_deg', np.array(euler).reshape(3, 1))
        fs.write('quaternion_xyzw', np.array(quat).reshape(4, 1))
        fs.write('calibration_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        fs.write('num_samples', len(self.T_target_cam_list))
        fs.release()
        
        print(f"\nğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜: {filepath}")
        
        # åŒæ—¶ä¿å­˜ä¸ºnpy (å…¼å®¹)
        npy_path = os.path.join(self.output_dir, 'handeye_result.npy')
        np.save(npy_path, T_cam_gripper)
        print(f"ğŸ’¾ æ ‡å®šç»“æœå·²ä¿å­˜: {npy_path}")
        
        # å¤åˆ¶åˆ°visionç›®å½•æ ¹ç›®å½•
        root_yaml = os.path.join(os.path.dirname(__file__), 'handeye_result.yaml')
        root_npy = os.path.join(os.path.dirname(__file__), 'handeye_result.npy')
        
        import shutil
        shutil.copy(filepath, root_yaml)
        shutil.copy(npy_path, root_npy)
        print(f"ğŸ’¾ å·²å¤åˆ¶åˆ°: {root_yaml}")
    
    def close(self):
        """å…³é—­æ§åˆ¶å™¨"""
        if self.controller:
            self.controller.close()
            print("ğŸ”Œ æ§åˆ¶å™¨å·²å…³é—­")


def main():
    parser = argparse.ArgumentParser(description='çœ¼åœ¨æ‰‹ä¸Šæ‰‹çœ¼æ ‡å®šå·¥å…·')
    parser.add_argument('--collect', action='store_true', help='é‡‡é›†æ ‡å®šæ•°æ®')
    parser.add_argument('--calibrate', action='store_true', help='æ‰§è¡Œæ ‡å®šè®¡ç®—')
    parser.add_argument('--all', action='store_true', help='é‡‡é›†+æ ‡å®š')
    parser.add_argument('--output-dir', default='./handeye_data', help='æ•°æ®ä¿å­˜ç›®å½•')
    parser.add_argument('--intrinsic', default='camera_intrinsics.yaml', help='ç›¸æœºå†…å‚æ–‡ä»¶')
    parser.add_argument('--square-size', type=float, default=20.73, help='æ£‹ç›˜æ ¼æ–¹æ ¼å¤§å°(mm)')
    parser.add_argument('--port', default='/dev/ttyACM0', help='ä¸²å£')
    
    args = parser.parse_args()
    
    calibrator = HandEyeCalibrator(
        board_size=(11, 8),
        square_size=args.square_size / 1000.0,
        intrinsic_file=args.intrinsic,
        output_dir=args.output_dir
    )
    
    try:
        if args.collect or args.all:
            # åˆå§‹åŒ–æœºå™¨äºº
            calibrator.init_robot(port=args.port)
            
            # å›ä¸­
            print("\nğŸ  æœºæ¢°è‡‚å›ä¸­...")
            # é‡‡é›†æ•°æ®
            calibrator.collect_data_interactive()
        
        if args.calibrate or args.all or (not args.collect and not args.all):
            # åŠ è½½æ•°æ®
            if not calibrator.T_target_cam_list:
                calibrator.load_collected_data()
            
            if calibrator.T_target_cam_list:
                # æ‰§è¡Œæ ‡å®š
                T_cam_gripper = calibrator.calibrate()
                
                if T_cam_gripper is not None:
                    # è¯„ä¼°
                    calibrator.evaluate_calibration(T_cam_gripper)
                    
                    # ä¿å­˜
                    calibrator.save_result(T_cam_gripper)
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    finally:
        calibrator.close()


if __name__ == '__main__':
    main()
