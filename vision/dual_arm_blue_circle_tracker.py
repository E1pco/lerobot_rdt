#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒè‡‚è“è‰²åœ†å½¢è¯†åˆ«è·Ÿè¸ªè„šæœ¬
=====================================
åŠŸèƒ½:
  1. åŒæ—¶ä½¿ç”¨å·¦å³ä¸¤ä¸ªç›¸æœºæ£€æµ‹è“è‰²åœ†å½¢
  2. ä½¿ç”¨æ‰‹çœ¼æ ‡å®šçŸ©é˜µå°†ç›¸æœºåæ ‡è½¬æ¢ä¸ºæœºå™¨äººåæ ‡
  3. æ§åˆ¶å¯¹åº”çš„æœºæ¢°è‡‚è¿›è¡Œè·Ÿè¸ª
  4. æ”¯æŒåŒè‡‚ååŒæ“ä½œ

ç›¸æœºé…ç½®:
  - video0: å·¦æ‰‹ç›¸æœº
  - video2: å³æ‰‹ç›¸æœº
  
æœºæ¢°è‡‚é…ç½®:
  - /dev/left_arm: å·¦è‡‚ä¸²å£
  - /dev/right_arm: å³è‡‚ä¸²å£

ä½¿ç”¨æ–¹æ³•:
  python vision/dual_arm_blue_circle_tracker.py
"""

import sys
import os
import cv2
import numpy as np
import time
import yaml
import threading
from scipy.spatial.transform import Rotation as R

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof_gripper


class DualArmBlueCircleTracker:
    """åŒè‡‚è“è‰²åœ†å½¢è·Ÿè¸ªå™¨"""
    
    def __init__(self, circle_diameter=0.05):
        """
        Parameters
        ----------
        circle_diameter : float
            åœ†å½¢ç›´å¾„ (ç±³)ï¼Œé»˜è®¤ 5cm
        """
        self.circle_diameter = circle_diameter
        self.circle_radius = circle_diameter / 2.0
        
        # ç›¸æœºå’Œæœºæ¢°è‡‚é…ç½®
        self.camera_configs = {
            'left': {
                'camera_id': 0,
                'device_path': '/dev/left_arm',
                'intrinsic_file': 'config_data/camera_intrinsics_left.yaml',
                'handeye_file': 'config_data/handeye_result_left.npy'
            },
            'right': {
                'camera_id': 2,
                'device_path': '/dev/right_arm', 
                'intrinsic_file': 'config_data/camera_intrinsics_right.yaml',
                'handeye_file': 'config_data/handeye_result_right.npy'
            }
        }
        
        # åˆå§‹åŒ–åŒè‡‚ç³»ç»Ÿ
        self.arms = {}
        self.cameras = {}
        self.intrinsics = {}
        self.handeye_matrices = {}
        
        self.setup_dual_arm_system()
        
        # æ§åˆ¶å‚æ•°
        self.tracking_enabled = True
        self.detection_results = {'left': None, 'right': None}
        
    def setup_dual_arm_system(self):
        """åˆå§‹åŒ–åŒè‡‚ç³»ç»Ÿ"""
        print("ğŸ”§ åˆå§‹åŒ–åŒè‡‚ç³»ç»Ÿ...")
        
        for arm_name, config in self.camera_configs.items():
            print(f"\n--- åˆå§‹åŒ– {arm_name.upper()} è‡‚ ---")
            
            try:
                # åˆå§‹åŒ–æœºæ¢°è‡‚æ§åˆ¶å™¨
                print(f"è¿æ¥æœºæ¢°è‡‚: {config['device_path']}")
                controller = ServoController(
                    port=config['device_path'],
                    baudrate=1_000_000,
                    config_path="../driver/servo_config.json"
                )
                
                # åˆ›å»ºæœºå™¨äººæ¨¡å‹
                robot = create_so101_5dof_gripper()
                robot.set_servo_controller(controller)
                
                self.arms[arm_name] = {
                    'controller': controller,
                    'robot': robot
                }
                
                # åˆå§‹åŒ–ç›¸æœº
                print(f"è¿æ¥ç›¸æœº: /dev/video{config['camera_id']}")
                cap = cv2.VideoCapture(config['camera_id'])
                if not cap.isOpened():
                    raise RuntimeError(f"æ— æ³•æ‰“å¼€ç›¸æœº {config['camera_id']}")
                
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                self.cameras[arm_name] = cap
                
                # åŠ è½½ç›¸æœºå†…å‚
                print(f"åŠ è½½ç›¸æœºå†…å‚: {config['intrinsic_file']}")
                self.load_camera_intrinsics(arm_name, config['intrinsic_file'])
                
                # åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ
                print(f"åŠ è½½æ‰‹çœ¼çŸ©é˜µ: {config['handeye_file']}")
                self.load_handeye_calibration(arm_name, config['handeye_file'])
                
                print(f"âœ… {arm_name.upper()} è‡‚åˆå§‹åŒ–æˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ {arm_name.upper()} è‡‚åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
    
    def load_camera_intrinsics(self, arm_name, intrinsic_file):
        """åŠ è½½ç›¸æœºå†…å‚"""
        try:
            # ä½¿ç”¨OpenCVè¯»å–YAMLæ–‡ä»¶ï¼ˆæ”¯æŒOpenCVæ ¼å¼ï¼‰
            fs = cv2.FileStorage(intrinsic_file, cv2.FILE_STORAGE_READ)
            
            # è¯»å–ç›¸æœºçŸ©é˜µå’Œç•¸å˜ç³»æ•°
            camera_matrix = fs.getNode('K').mat()
            dist_coeffs = fs.getNode('distCoeffs').mat().flatten()
            
            fs.release()
            
            self.intrinsics[arm_name] = {
                'camera_matrix': camera_matrix,
                'dist_coeffs': dist_coeffs
            }
            
            print(f"   ç›¸æœºå†…å‚çŸ©é˜µ:")
            print(f"{camera_matrix}")
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ {arm_name} ç›¸æœºå†…å‚: {e}")
            raise
    
    def load_handeye_calibration(self, arm_name, handeye_file):
        """åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ"""
        try:
            handeye_matrix = np.load(handeye_file)
            self.handeye_matrices[arm_name] = handeye_matrix
            
            print(f"   æ‰‹çœ¼æ ‡å®šçŸ©é˜µ:")
            print(f"{handeye_matrix}")
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ {arm_name} æ‰‹çœ¼æ ‡å®šç»“æœ: {e}")
            raise
    
    def detect_blue_circle(self, frame):
        """æ£€æµ‹è“è‰²åœ†å½¢"""
        # è½¬æ¢åˆ°HSVé¢œè‰²ç©ºé—´
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è“è‰²HSVèŒƒå›´ (è°ƒæ•´ä¸ºæ›´å®½æ¾çš„èŒƒå›´)
        lower_blue = np.array([100, 50, 50])
        upper_blue = np.array([130, 255, 255])
        
        # åˆ›å»ºè“è‰²æ©ç 
        mask = cv2.inRange(hsv, lower_blue, upper_blue)
        
        # å½¢æ€å­¦æ“ä½œå»å™ª
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        best_circle = None
        best_score = 0
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area < 100:  # è¿‡æ»¤å°è½®å»“
                continue
            
            # è®¡ç®—è½®å»“çš„åœ†åº¦
            perimeter = cv2.arcLength(contour, True)
            if perimeter == 0:
                continue
            
            circularity = 4 * np.pi * area / (perimeter * perimeter)
            
            if circularity > 0.5:  # åœ†åº¦é˜ˆå€¼
                # è®¡ç®—æœ€å°å¤–æ¥åœ†
                (x, y), radius = cv2.minEnclosingCircle(contour)
                center = (int(x), int(y))
                radius = int(radius)
                
                # è¯„åˆ†: åœ†åº¦ + å¤§å°åˆç†æ€§
                size_score = 1.0 - abs(radius - 30) / 30  # æœŸæœ›åŠå¾„çº¦30åƒç´ 
                total_score = circularity * 0.7 + max(0, size_score) * 0.3
                
                if total_score > best_score:
                    best_score = total_score
                    best_circle = {
                        'center': center,
                        'radius': radius,
                        'score': total_score,
                        'contour': contour
                    }
        
        return best_circle, mask
    
    def calculate_3d_position(self, arm_name, circle_center, circle_radius_pixels):
        """ä½¿ç”¨PnPç®—æ³•è®¡ç®—3Dä½ç½®"""
        if arm_name not in self.intrinsics:
            return None
        
        # 3Dåœ†å½¢å…³é”®ç‚¹ (åœ†å¿ƒåœ¨z=0å¹³é¢)
        object_points = np.array([
            [0, 0, 0],                              # åœ†å¿ƒ
            [self.circle_radius, 0, 0],             # å³
            [0, self.circle_radius, 0],             # ä¸Š
            [-self.circle_radius, 0, 0],            # å·¦
            [0, -self.circle_radius, 0]             # ä¸‹
        ], dtype=np.float32)
        
        # 2Då›¾åƒç‚¹ (å‡è®¾åœ†å½¢å‚ç›´äºç›¸æœº)
        cx, cy = circle_center
        image_points = np.array([
            [cx, cy],                               # åœ†å¿ƒ
            [cx + circle_radius_pixels, cy],        # å³
            [cx, cy - circle_radius_pixels],        # ä¸Š
            [cx - circle_radius_pixels, cy],        # å·¦
            [cx, cy + circle_radius_pixels]         # ä¸‹
        ], dtype=np.float32)
        
        camera_matrix = self.intrinsics[arm_name]['camera_matrix']
        dist_coeffs = self.intrinsics[arm_name]['dist_coeffs']
        
        # è§£PnP
        success, rvec, tvec = cv2.solvePnP(
            object_points, image_points, camera_matrix, dist_coeffs
        )
        
        if success:
            return {
                'translation': tvec.flatten(),
                'rotation': rvec.flatten(),
                'success': True
            }
        else:
            return {'success': False}
    
    def camera_to_robot_coords(self, arm_name, camera_position):
        """å°†ç›¸æœºåæ ‡è½¬æ¢ä¸ºæœºå™¨äººåæ ‡"""
        if arm_name not in self.handeye_matrices:
            return None
        
        # ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½å§¿
        camera_point = np.array([camera_position[0], camera_position[1], camera_position[2], 1])
        
        # ä½¿ç”¨æ‰‹çœ¼æ ‡å®šçŸ©é˜µè½¬æ¢
        handeye_matrix = self.handeye_matrices[arm_name]
        robot_point = handeye_matrix @ camera_point
        
        return robot_point[:3]  # è¿”å›xyzåæ ‡
    
    def process_arm_detection(self, arm_name):
        """å¤„ç†å•ä¸ªæœºæ¢°è‡‚çš„æ£€æµ‹"""
        if arm_name not in self.cameras:
            return
        
        cap = self.cameras[arm_name]
        ret, frame = cap.read()
        
        if not ret:
            print(f"âŒ {arm_name} ç›¸æœºè¯»å–å¤±è´¥")
            return
        
        # æ£€æµ‹è“è‰²åœ†å½¢
        circle_result, mask = self.detect_blue_circle(frame)
        
        # å¯è§†åŒ–
        display_frame = frame.copy()
        
        if circle_result:
            center = circle_result['center']
            radius = circle_result['radius']
            score = circle_result['score']
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœ
            cv2.circle(display_frame, center, radius, (0, 255, 0), 2)
            cv2.circle(display_frame, center, 2, (0, 255, 0), -1)
            
            # è®¡ç®—3Dä½ç½®
            pos_3d = self.calculate_3d_position(arm_name, center, radius)
            
            if pos_3d and pos_3d['success']:
                camera_pos = pos_3d['translation']
                
                # è½¬æ¢ä¸ºæœºå™¨äººåæ ‡
                robot_pos = self.camera_to_robot_coords(arm_name, camera_pos)
                
                if robot_pos is not None:
                    # æ˜¾ç¤ºä¿¡æ¯
                    info_text = f"{arm_name.upper()}: ({robot_pos[0]:.3f}, {robot_pos[1]:.3f}, {robot_pos[2]:.3f})"
                    cv2.putText(display_frame, info_text, (10, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # ä¿å­˜æ£€æµ‹ç»“æœ
                    self.detection_results[arm_name] = {
                        'camera_pos': camera_pos,
                        'robot_pos': robot_pos,
                        'pixel_center': center,
                        'pixel_radius': radius,
                        'score': score
                    }
                else:
                    cv2.putText(display_frame, f"{arm_name.upper()}: åæ ‡è½¬æ¢å¤±è´¥", (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            else:
                cv2.putText(display_frame, f"{arm_name.upper()}: PnPå¤±è´¥", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        else:
            cv2.putText(display_frame, f"{arm_name.upper()}: æœªæ£€æµ‹åˆ°åœ†å½¢", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            self.detection_results[arm_name] = None
        
        # æ˜¾ç¤ºå›¾åƒ
        cv2.imshow(f'{arm_name.upper()} Camera', display_frame)
        cv2.imshow(f'{arm_name.upper()} Mask', mask)
    
    def control_arm_movement(self, arm_name, target_position, approach_height=0.1):
        """æ§åˆ¶æœºæ¢°è‡‚ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®"""
        if arm_name not in self.arms:
            return False
        
        try:
            robot = self.arms[arm_name]['robot']
            
            # ç›®æ ‡ä½ç½®ï¼ˆç¨å¾®æŠ¬é«˜ä¸€äº›æ¥è¿‘ï¼‰
            target_pos = target_position.copy()
            target_pos[2] += approach_height  # åœ¨ç›®æ ‡ä¸Šæ–¹approach_heightç±³
            
            # æ„å»ºç›®æ ‡å˜æ¢çŸ©é˜µï¼ˆæœ«ç«¯å‚ç›´å‘ä¸‹ï¼‰
            target_transform = np.eye(4)
            target_transform[:3, 3] = target_pos
            target_transform[:3, :3] = R.from_euler('xyz', [np.pi, 0, 0]).as_matrix()
            
            # æ±‚è§£é€†è¿åŠ¨å­¦
            current_q = robot.read_joint_angles(verbose=False)
            result = robot.ikine_LM(target_transform, q0=current_q)
            
            if result.success:
                # æ‰§è¡Œç§»åŠ¨
                home_pose = {name: robot.servo.get_home_position(name) 
                           for name in robot.joint_names}
                
                servo_targets = robot.q_to_servo_targets(
                    q_rad=result.q,
                    home_pose=home_pose
                )
                
                robot.servo.soft_move_to_pose(servo_targets, step_count=8, interval=0.05)
                
                print(f"âœ… {arm_name.upper()} è‡‚ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®: {target_pos}")
                return True
            else:
                print(f"âŒ {arm_name.upper()} è‡‚é€†è¿åŠ¨å­¦æ±‚è§£å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ {arm_name.upper()} è‡‚ç§»åŠ¨å¤±è´¥: {e}")
            return False
    
    def run_dual_arm_tracking(self):
        """è¿è¡ŒåŒè‡‚è·Ÿè¸ª"""
        print("ğŸš€ å¼€å§‹åŒè‡‚è“è‰²åœ†å½¢è·Ÿè¸ª")
        print("æŒ‰é”®æ“ä½œ:")
        print("  q - é€€å‡ºç¨‹åº")
        print("  l - å·¦è‡‚ç§»åŠ¨åˆ°æ£€æµ‹ä½ç½®")
        print("  r - å³è‡‚ç§»åŠ¨åˆ°æ£€æµ‹ä½ç½®")
        print("  b - åŒè‡‚åŒæ—¶ç§»åŠ¨")
        print("  h - åŒè‡‚å›åˆ°åˆå§‹ä½ç½®")
        
        try:
            while self.tracking_enabled:
                # å¤„ç†å·¦å³ä¸¤ä¸ªç›¸æœº
                self.process_arm_detection('left')
                self.process_arm_detection('right')
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):
                    print("ğŸ›‘ é€€å‡ºç¨‹åº")
                    break
                elif key == ord('l'):
                    # å·¦è‡‚ç§»åŠ¨
                    if self.detection_results['left'] is not None:
                        target = self.detection_results['left']['robot_pos']
                        self.control_arm_movement('left', target)
                    else:
                        print("âš ï¸  å·¦è‡‚æœªæ£€æµ‹åˆ°ç›®æ ‡")
                elif key == ord('r'):
                    # å³è‡‚ç§»åŠ¨
                    if self.detection_results['right'] is not None:
                        target = self.detection_results['right']['robot_pos']
                        self.control_arm_movement('right', target)
                    else:
                        print("âš ï¸  å³è‡‚æœªæ£€æµ‹åˆ°ç›®æ ‡")
                elif key == ord('b'):
                    # åŒè‡‚åŒæ—¶ç§»åŠ¨
                    left_ok = right_ok = False
                    
                    if self.detection_results['left'] is not None:
                        target = self.detection_results['left']['robot_pos']
                        left_ok = self.control_arm_movement('left', target)
                    
                    if self.detection_results['right'] is not None:
                        target = self.detection_results['right']['robot_pos']
                        right_ok = self.control_arm_movement('right', target)
                    
                    if not (left_ok or right_ok):
                        print("âš ï¸  åŒè‡‚éƒ½æœªæ£€æµ‹åˆ°ç›®æ ‡")
                elif key == ord('h'):
                    # åŒè‡‚å›åˆ°åˆå§‹ä½ç½®
                    print("ğŸ  åŒè‡‚å›åˆ°åˆå§‹ä½ç½®...")
                    for arm_name in ['left', 'right']:
                        if arm_name in self.arms:
                            self.arms[arm_name]['controller'].move_all_home()
                
        except KeyboardInterrupt:
            print("\\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        # å…³é—­ç›¸æœº
        for arm_name, cap in self.cameras.items():
            if cap is not None:
                cap.release()
        
        # å…³é—­æœºæ¢°è‡‚æ§åˆ¶å™¨
        for arm_name, arm in self.arms.items():
            if 'controller' in arm:
                arm['controller'].close()
        
        cv2.destroyAllWindows()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    def __del__(self):
        self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåŒè‡‚è·Ÿè¸ªå™¨
        tracker = DualArmBlueCircleTracker(circle_diameter=0.05)
        
        # è¿è¡Œè·Ÿè¸ª
        tracker.run_dual_arm_tracking()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()