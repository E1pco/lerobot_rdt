#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2
import os
import sys
import time
import numpy as np
from scipy.spatial.transform import Rotation as R

# æ·»åŠ ä¸Šä¸€çº§ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from joyconrobotics import JoyconRobotics
from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof


# ==============================================================
# å·¥å…·å‡½æ•°
# ==============================================================

def build_T(x, y, z, roll, pitch, yaw):
    """æ„é€ é½æ¬¡å˜æ¢çŸ©é˜µ"""
    T = np.eye(4)
    T[:3, :3] = R.from_euler('xyz', [roll, pitch, yaw]).as_matrix()
    T[:3, 3] = [x, y, z]
    return T


def solve_pnp(img, K, distCoeffs, pattern_size=(11, 8), square_size=0.022):
    """æ£€æµ‹æ£‹ç›˜æ ¼è§’ç‚¹å¹¶æ±‚è§£å¤–å‚"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE
    ret, corners = cv2.findChessboardCorners(gray, pattern_size, flags)
    if not ret:
        print("âš ï¸ æ£‹ç›˜æ ¼æ£€æµ‹å¤±è´¥ï¼Œè¯·è°ƒæ•´è§’åº¦/å…‰ç…§ã€‚")
        return None, None, None

    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
    objp *= square_size

    corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1),
                                (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))

    ret, rvec, tvec = cv2.solvePnP(objp, corners2, K, distCoeffs)
    if not ret:
        print("âš ï¸ solvePnPå¤±è´¥ã€‚")
        return None, None, None

    R_cam, _ = cv2.Rodrigues(rvec)
    T_target_cam = np.eye(4)
    T_target_cam[:3, :3] = R_cam
    T_target_cam[:3, 3] = tvec.squeeze()

    # å¯è§†åŒ–è§’ç‚¹
    vis = img.copy()
    cv2.drawChessboardCorners(vis, pattern_size, corners2, True)
    cv2.imshow("Chessboard Detected", vis)
    cv2.waitKey(200)

    return ret, corners2, T_target_cam


# ==============================================================
# ä¸»ç±»ï¼šæ§åˆ¶ + æ‹ç…§
# ==============================================================

class JoyConCapture:
    def __init__(self, port='/dev/ttyACM0', baudrate=1_000_000,
                 config_path='servo_config.json', cam_id=0,
                 save_dir='dataset'):

        # ç›¸æœºå†…å‚ï¼ˆä» camera_intrinsics.yaml ç¡¬ç¼–ç ï¼‰
        self.K = np.array([[664.44701044,0.,658.891941  ],
                           [0.,654.89004383, 406.58738455],
                           [0.,0.,1.]], dtype=np.float32)
        
        # ç•¸å˜ç³»æ•°
        self.distCoeffs = np.array([
            -0.22848866657422115, -0.24286465556211895,
            -0.0041375613727195667, -0.0214093589304933, 0.67109732798343458
        ], dtype=np.float32)

        # èˆµæœºæ§åˆ¶åˆå§‹åŒ–
        self.controller = ServoController(port=port, baudrate=baudrate, config_path=config_path)
        self.robot = create_so101_5dof()
        # å…³é”®ï¼šç»‘å®šèˆµæœºæ§åˆ¶å™¨åˆ°æœºå™¨äººå¯¹è±¡
        self.robot.set_servo_controller(self.controller)

        # JoyCon åˆå§‹åŒ–
        self.joycon = JoyconRobotics(device='right', without_rest_init=False)

        # ç›¸æœºåˆå§‹åŒ–
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            raise IOError("âŒ æ— æ³•æ‰“å¼€ç›¸æœº")

        # æ•°æ®ä¿å­˜è·¯å¾„
        os.makedirs(save_dir, exist_ok=True)
        self.save_dir = save_dir
        self.img_idx = 0

        print("\nâœ… åˆå§‹åŒ–å®Œæˆï¼š")
        print(f"ç›¸æœºå†…å‚:\n{self.K}")
        print(f"ç•¸å˜ç³»æ•°: {self.distCoeffs}")
        print(f"æ•°æ®ä¿å­˜ç›®å½•: {save_dir}")
        print("\nğŸ“‹ é‡‡é›†å»ºè®®:")
        print("  1. é‡‡é›† 8-12 ç»„æ•°æ®ï¼Œç¡®ä¿è¶³å¤Ÿå¤šæ ·æ€§")
        print("  2. åœ¨ä¸åŒä½ç½®å’Œå§¿æ€ç§»åŠ¨æœºæ¢°è‡‚ï¼ˆX/Y/Z/Rx/Ry/Rz å„æ–¹å‘è‡³å°‘å˜åŒ– 5-10cm æˆ– 30-45Â°ï¼‰")
        print("  3. ç¡®ä¿æ£‹ç›˜æ ¼åœ¨å›¾åƒä¸­æ¸…æ™°å¯è§")
        print("  4. é¿å…é¢‘ç¹ç§»åŠ¨å¯¼è‡´çš„éœ‡åŠ¨å½±å“")
        print("  5. ç¡®ä¿æœºæ¢°è‡‚ç¨³å®šåå†æ‹ç…§\n")
        print("æŒ‰ [A] æ‹ç…§ä¿å­˜ï¼ŒæŒ‰ [X] é€€å‡ºã€‚\n")

    def _get_robot_pose(self):
        """è¯»å–å½“å‰èˆµæœºä½ç½®å¹¶è®¡ç®— FK"""
        
        # è¯»å–å…³èŠ‚è§’åº¦ï¼ˆä¼ å…¥å¿…è¦å‚æ•°ç¡®ä¿ ServoController å¯ç”¨ï¼‰
        q = self.robot.read_joint_angles(
            joint_names=self.robot.joint_names,
            verbose=False
        )
        
        # âœ… ä½¿ç”¨ fk() æ–¹æ³•è¿”å› [X, Y, Z, roll, pitch, yaw]
        pose_6d = self.robot.fk(q)
        # fk() è¿”å› [X, Y, Z, gamma, beta, alpha] = [X, Y, Z, Yaw, Pitch, Roll]
        x, y, z = pose_6d[0:3]
        yaw, pitch, roll = pose_6d[3:6]
        
        # æ„é€  4x4 é½æ¬¡å˜æ¢çŸ©é˜µ
        T_gripper_base = np.eye(4)
        T_gripper_base[:3, :3] = R.from_euler('xyz', [roll, pitch, yaw]).as_matrix()
        T_gripper_base[:3, 3] = [x, y, z]
        print(f"\nâœ… FK è®¡ç®—ç»“æœ:")
        print(f"   æœ«ç«¯ä½ç½®: ({x:.4f}, {y:.4f}, {z:.4f}) m")
        print(f"   æœ«ç«¯å§¿æ€: R={np.degrees(roll):.2f}Â° P={np.degrees(pitch):.2f}Â° Y={np.degrees(yaw):.2f}Â°")
        print(f"   T_gripper^base =\n{T_gripper_base}")
        
        return T_gripper_base, q

    def _normalize_transform(self, T):
        """è§„èŒƒåŒ–å˜æ¢çŸ©é˜µï¼ˆç¡®ä¿æ—‹è½¬çŸ©é˜µæ­£äº¤ï¼‰"""
        T_norm = T.copy()
        U, _, Vt = np.linalg.svd(T[:3, :3])
        T_norm[:3, :3] = U @ Vt
        return T_norm

    def run(self):
        print("ğŸ® å¼€å§‹é‡‡é›†ï¼Œç§»åŠ¨æœºæ¢°è‡‚å¯¹å‡†æ£‹ç›˜æ ¼...")

        while True:
            ret, frame = self.cap.read()
            if not ret:
                continue
            display = frame.copy()
            cv2.putText(display, f"Image #{self.img_idx}", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
            cv2.imshow("Camera", display)

            self.joycon.update()
            # A é”®æ‹ç…§
            if self.joycon.button.a == 1:
                print(f"\nğŸ“¸ æ‹ç…§ #{self.img_idx} ä¸­...")
                time.sleep(0.5)  # ç­‰å¾…æœºæ¢°è‡‚ç¨³å®šï¼Œé¿å…æŒ¯åŠ¨å½±å“
                
                # ã€å…³é”®ã€‘å…ˆè¯»å–æœºæ¢°è‡‚ä½å§¿ç¡®ä¿åŒæ­¥ï¼
                print("ğŸ“ è¯»å–æœºæ¢°è‡‚ä½å§¿ä¸­...")
                poses = []
                qs = []
                for _ in range(3):
                    T, q = self._get_robot_pose()
                    poses.append(T)
                    qs.append(q)
                    time.sleep(0.05)
                
                # å¯¹å¤šæ¬¡è¯»å–çš„ä½å§¿è¿›è¡Œå¹³å‡
                T_gripper_base = np.mean(poses, axis=0)
                T_gripper_base = self._normalize_transform(T_gripper_base)
                q = np.mean(qs, axis=0)  # å…³èŠ‚è§’åº¦ä¹Ÿå–å¹³å‡
                
                # ã€ç„¶åã€‘ç«‹å³æ‹ç…§
                print("ğŸ“· ç«‹å³æ‹ç…§...")
                time.sleep(0.1)
                
                # è¿ç»­æ‹æ‘„å¤šå¸§ï¼Œå–æœ€æ¸…æ™°çš„
                frames_to_capture = 3
                best_frame = None
                best_sharpness = 0
                
                for _ in range(frames_to_capture):
                    ret, frame = self.cap.read()
                    if not ret:
                        continue
                    
                    # è®¡ç®—å›¾åƒæ¸…æ™°åº¦ï¼ˆLaplacian æ–¹å·®ï¼‰
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
                    
                    if sharpness > best_sharpness:
                        best_sharpness = sharpness
                        best_frame = frame
                    
                    time.sleep(0.05)
                
                if best_frame is None:
                    continue
                
                frame = best_frame
                ret_pnp, corners, T_target_cam = solve_pnp(frame, self.K, self.distCoeffs)
                if not ret_pnp:
                    print("âš ï¸ æœªæ£€æµ‹åˆ°æ£‹ç›˜æ ¼ï¼Œè¯·é‡è¯•ã€‚")
                    continue
                
                # ä¿å­˜æ•°æ®
                np.savez(os.path.join(self.save_dir, f"pose_{self.img_idx:02d}.npz"),
                         T_target_cam=T_target_cam,
                         T_gripper_base=T_gripper_base,
                         q=q,  # ä¿å­˜åŸå§‹å…³èŠ‚è§’åº¦ä»¥ä¾¿è¯Šæ–­
                         image=frame)
                cv2.imwrite(os.path.join(self.save_dir, f"img_{self.img_idx:02d}.jpg"), frame)

                print(f"âœ… å·²ä¿å­˜ pose_{self.img_idx:02d}.npz å’Œå¯¹åº”å›¾ç‰‡")
                print(f"   å›¾åƒæ¸…æ™°åº¦: {best_sharpness:.2f}")
                self.img_idx += 1
                time.sleep(0.5)

            # X é”®é€€å‡º
            if self.joycon.button.x == 1:
                print("\nğŸ›‘ é€€å‡ºé‡‡é›†ã€‚")
                break

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        self.cap.release()
        cv2.destroyAllWindows()


# ==============================================================
# ä¸»å…¥å£
# ==============================================================

if __name__ == "__main__":
    collector = JoyConCapture(
        port='/dev/ttyACM0',
        baudrate=1_000_000,
        config_path='servo_config.json',
        cam_id=0,
        save_dir='dataset_eyeinhand'
    )
    collector.run()
