#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è“è‰²åœ†å½¢æŠ“å–è„šæœ¬ (çœ¼åœ¨æ‰‹å¤– Eye-to-Hand)
=====================================
åŠŸèƒ½:
  1. æ£€æµ‹è“è‰²åœ†å½¢
  2. ä½¿ç”¨PnPè®¡ç®—åœ†å½¢åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®
  3. ç»“åˆçœ¼åœ¨æ‰‹å¤–æ ‡å®šç»“æœï¼Œè®¡ç®—åœ†å½¢åœ¨åŸºåº§åæ ‡ç³»ä¸‹çš„ä½ç½®
  4. æ§åˆ¶æœºæ¢°è‡‚æœ«ç«¯é è¿‘è“è‰²åœ†å½¢

ä½¿ç”¨æ–¹æ³•:
  python vision/track_blue_circle_eyetohand.py
"""

import sys
import os
import cv2
import numpy as np
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof_gripper


class BlueCircleTrackerEyeToHand:
    """è“è‰²åœ†å½¢è·Ÿè¸ªå™¨ï¼ˆçœ¼åœ¨æ‰‹å¤–ç‰ˆï¼‰"""
    
    def __init__(self, circle_diameter=0.05):
        """
        Parameters
        ----------
        circle_diameter : float
            åœ†å½¢ç›´å¾„ (ç±³)ï¼Œé»˜è®¤ 5cm
        """
        self.circle_diameter = circle_diameter
        self.circle_radius = circle_diameter / 2.0
        
        # è·¯å¾„é…ç½®
        self.config_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config_data')
        self.intrinsic_file = os.path.join(self.config_dir, 'camera_intrinsics_environment.yaml')
        # æ‰‹çœ¼æ ‡å®šç»“æœä¼˜å…ˆä½¿ç”¨ handeye_result_envir.npyï¼ˆæ–°æµç¨‹äº§ç‰©ï¼‰ï¼Œå›é€€ camera_extrinsics.npyï¼ˆæ—§å‘½åï¼Œå†…å®¹å¯èƒ½æ˜¯ T_cam_baseï¼‰
        self.handeye_candidates = [
            os.path.join(self.config_dir, 'handeye_result_envir.npy'),
            os.path.join(self.config_dir, 'camera_extrinsics.npy'),
        ]
        
        # åŠ è½½å‚æ•°
        self.load_camera_intrinsics(self.intrinsic_file)
        self.load_handeye_calibration(self.handeye_candidates)
        
        # åˆå§‹åŒ–æœºå™¨äºº
        self.robot = None
        self.controller = None
        
        # HSV è“è‰²èŒƒå›´
        self.hsv_lower1 = np.array([100, 80, 80])
        self.hsv_upper1 = np.array([120, 255, 255])

        # è¿è¡Œæ—¶çŠ¶æ€
        self._k_scaled = False
        
        print("="*60)
        print("ğŸ”µ è“è‰²åœ†å½¢æŠ“å–ç³»ç»Ÿ (Eye-to-Hand)")
        print("="*60)
        print(f"åœ†å½¢ç›´å¾„: {circle_diameter*1000:.0f} mm")
        print("="*60)
    
    def load_camera_intrinsics(self, yaml_path):
        """åŠ è½½ç›¸æœºå†…å‚"""
        if not os.path.exists(yaml_path):
            print(f"âŒ æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
            # å°è¯•ä½¿ç”¨é»˜è®¤å€¼æˆ–æŠ›å‡ºå¼‚å¸¸
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
        
        try:
            fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
            
            # å°è¯•ä¸åŒçš„é”®å
            camera_matrix_node = fs.getNode('camera_matrix')
            if camera_matrix_node.empty():
                camera_matrix_node = fs.getNode('K')
            
            dist_coeffs_node = fs.getNode('distortion_coefficients')
            if dist_coeffs_node.empty():
                dist_coeffs_node = fs.getNode('distCoeffs')
            
            self.K = camera_matrix_node.mat()
            self.dist = dist_coeffs_node.mat().flatten()

            # è®°å½•æ ‡å®šåˆ†è¾¨ç‡ï¼ˆè‹¥yamlé‡Œæœ‰ï¼‰
            w_node = fs.getNode('image_width')
            h_node = fs.getNode('image_height')
            self.intrinsic_width = int(w_node.real()) if not w_node.empty() else None
            self.intrinsic_height = int(h_node.real()) if not h_node.empty() else None

            self.K0 = self.K.copy()
            fs.release()
            
            print(f"ğŸ“· å·²åŠ è½½ç›¸æœºå†…å‚: {os.path.basename(yaml_path)}")
            print(f"   fx={self.K[0,0]:.1f}, fy={self.K[1,1]:.1f}")
            if self.intrinsic_width and self.intrinsic_height:
                print(f"   æ ‡å®šåˆ†è¾¨ç‡: {self.intrinsic_width}x{self.intrinsic_height}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç›¸æœºå†…å‚å¤±è´¥: {e}")
            raise
    
    def load_handeye_calibration(self, npy_path):
        """åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ (T_cam_base)

        Parameters
        ----------
        npy_path : str | list[str]
            å•ä¸ªè·¯å¾„æˆ–å€™é€‰è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰ã€‚
        """
        candidates = npy_path if isinstance(npy_path, (list, tuple)) else [npy_path]

        self.T_cam_base = None
        self.handeye_path = None

        for path in candidates:
            if path and os.path.exists(path):
                try:
                    self.T_cam_base = np.load(path)
                    self.handeye_path = path
                    print(f"âœ… å·²åŠ è½½æ‰‹çœ¼æ ‡å®š: {os.path.basename(path)}")
                    break
                except Exception as e:
                    print(f"âŒ åŠ è½½æ‰‹çœ¼æ ‡å®šå‚æ•°å¤±è´¥: {path} ({e})")

        if self.T_cam_base is None:
            print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æ‰‹çœ¼æ ‡å®šæ–‡ä»¶ï¼Œå°†æ— æ³•è¿›è¡Œåæ ‡è½¬æ¢")

    def _maybe_scale_K_to_frame(self, frame):
        """è‹¥ç›¸æœºå®é™…åˆ†è¾¨ç‡ä¸æ ‡å®šåˆ†è¾¨ç‡ä¸åŒï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾ Kã€‚"""
        if self._k_scaled:
            return
        if frame is None:
            return
        h, w = frame.shape[:2]
        if not self.intrinsic_width or not self.intrinsic_height:
            print(f"â„¹ï¸  å½“å‰ç›¸æœºåˆ†è¾¨ç‡: {w}x{h} (yamlæœªæä¾›æ ‡å®šåˆ†è¾¨ç‡ï¼Œè·³è¿‡Kç¼©æ”¾)")
            self._k_scaled = True
            return
        print(f"â„¹ï¸  å½“å‰ç›¸æœºåˆ†è¾¨ç‡: {w}x{h}")
        if (w, h) == (self.intrinsic_width, self.intrinsic_height):
            self._k_scaled = True
            return
        sx = w / float(self.intrinsic_width)
        sy = h / float(self.intrinsic_height)
        K = self.K0.copy()
        K[0, 0] *= sx
        K[1, 1] *= sy
        K[0, 2] *= sx
        K[1, 2] *= sy
        self.K = K
        self._k_scaled = True
        print(f"âš ï¸  åˆ†è¾¨ç‡ä¸ä¸€è‡´ï¼Œå·²ç¼©æ”¾K: sx={sx:.4f}, sy={sy:.4f}")
    
    def init_robot(self, port="/dev/left_arm", baudrate=1_000_000):
        """åˆå§‹åŒ–æœºå™¨äºº"""
        print("\nğŸ¤– åˆå§‹åŒ–æœºå™¨äºº...")
        try:
            self.controller = ServoController(
                port=port, 
                baudrate=baudrate, 
                config_path=os.path.join(os.path.dirname(__file__), "../driver/servo_config.json")
            )
            self.robot = create_so101_5dof_gripper()
            self.robot.set_servo_controller(self.controller)
            print("âœ… æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ æœºå™¨äººåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_blue_circle(self, frame):
        """æ£€æµ‹è“è‰²åœ†å½¢"""
        # è½¬æ¢åˆ°HSV
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è“è‰²æ©ç 
        mask = cv2.inRange(hsv, self.hsv_lower1, self.hsv_upper1)
        
        # å½¢æ€å­¦å¤„ç†
        kernel = np.ones((7, 7), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        
        # éœå¤«åœ†æ£€æµ‹
        blurred = cv2.GaussianBlur(mask, (9, 9), 2)
        circles = cv2.HoughCircles(
            blurred,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=50,
            param1=50,
            param2=30,
            minRadius=15,
            maxRadius=250
        )
        
        if circles is not None:
            circles = np.round(circles[0, :]).astype(int)
            best_circle = max(circles, key=lambda c: c[2])
            center = (best_circle[0], best_circle[1])
            radius = best_circle[2]
            return True, center, radius, mask
        
        # è½®å»“æ£€æµ‹å¤‡é€‰
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)
            if area > 800:
                perimeter = cv2.arcLength(largest_contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    if circularity > 0.7:
                        (cx, cy), radius = cv2.minEnclosingCircle(largest_contour)
                        return True, (int(cx), int(cy)), int(radius), mask
        
        return False, None, None, mask
    
    def estimate_pose_from_circle(self, center, radius_px):
        """ä»åœ†å½¢çš„åƒç´ åæ ‡å’ŒåŠå¾„ä¼°ç®—3Dä½ç½® (ç›¸æœºåæ ‡ç³»)"""
        fx = float(self.K[0, 0])
        fy = float(self.K[1, 1])
        # ä¼°ç®—æ·±åº¦ Z = f * R / r
        f = (fx + fy) / 2.0
        Z = f * self.circle_radius / float(radius_px)

        # å»ç•¸å˜åæŠ•å½±ï¼šundistortPoints è¾“å‡ºå½’ä¸€åŒ–åæ ‡ (x, y)ï¼Œä½¿ X=x*Z, Y=y*Z
        u, v = float(center[0]), float(center[1])
        pts = np.array([[[u, v]]], dtype=np.float32)
        und = cv2.undistortPoints(pts, self.K, self.dist)  # (1,1,2), normalized
        x_n, y_n = float(und[0, 0, 0]), float(und[0, 0, 1])
        X = x_n * Z
        Y = y_n * Z
        return np.array([X, Y, Z])
    
    def read_robot_pose(self, verbose=False):
        """è¯»å–æœºå™¨äººå½“å‰æœ«ç«¯ä½å§¿"""
        q = self.robot.read_joint_angles(
            joint_names=self.robot.joint_names,
            verbose=verbose
        )
        T_gripper_base = self.robot.fkine(q)
        return T_gripper_base, q
    
    def run(self):
        """è¿è¡Œè·Ÿè¸ªå¾ªç¯"""
        if not self.init_robot():
            return
        
        # æ‰“å¼€ç›¸æœº (é€šå¸¸çœ¼åœ¨æ‰‹å¤–ä½¿ç”¨ä¸åŒçš„ç›¸æœºIDï¼Œè¿™é‡Œå‡è®¾ä¸º0ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´)
        cap = cv2.VideoCapture(0) 
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº 0ï¼Œå°è¯•ç›¸æœº 2...")
            cap = cv2.VideoCapture(2)
            if not cap.isOpened():
                print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº")
                return

        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

        # ç”¨é¦–å¸§åšä¸€æ¬¡Kç¼©æ”¾/è¯Šæ–­
        ret0, frame0 = cap.read()
        if ret0:
            self._maybe_scale_K_to_frame(frame0)
        
        print("\nğŸ® æ§åˆ¶è¯´æ˜:")
        print("  'f' - å¼€å¯/å…³é—­ è·Ÿéšæ¨¡å¼")
        print("  'h' - æœºæ¢°è‡‚å›ä¸­")
        print("  'q' - é€€å‡º")
        print("="*60)
        
        following = False
        gain = 0.8
        step_limit = 0.080

        last_print_t = 0.0
        print_interval_s = 0.2
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                display = frame.copy()
                
                # æ£€æµ‹è“è‰²åœ†å½¢
                success, center, radius_px, mask = self.detect_blue_circle(frame)
                
                if success:
                    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
                    cv2.circle(display, center, radius_px, (0, 255, 0), 2)
                    cv2.circle(display, center, 3, (0, 255, 255), -1)
                    
                    # 1. è®¡ç®—ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®
                    pos_cam = self.estimate_pose_from_circle(center, radius_px)
                    pos_cam_mm = pos_cam * 1000

                    now = time.time()
                    do_print = (now - last_print_t) >= print_interval_s
                    if do_print:
                        last_print_t = now
                        print(f"Cam:  [{pos_cam_mm[0]:7.1f}, {pos_cam_mm[1]:7.1f}, {pos_cam_mm[2]:7.1f}] mm")
                    
                    cv2.putText(display, f"PnP(Cam): [{pos_cam_mm[0]:.0f}, {pos_cam_mm[1]:.0f}, {pos_cam_mm[2]:.0f}]", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    
                    # 2. è½¬æ¢åˆ°åŸºåº§åæ ‡ç³» (Eye-to-Hand)
                    if self.T_cam_base is not None:
                        # P_base = T_handeye @ P_cam
                        pos_cam_homo = np.append(pos_cam, 1.0)
                        pos_target_base = (self.T_cam_base @ pos_cam_homo)[:3]
                        pos_target_mm = pos_target_base * 1000

                        if do_print:
                            print(f"Base: [{pos_target_mm[0]:7.1f}, {pos_target_mm[1]:7.1f}, {pos_target_mm[2]:7.1f}] mm")
                        
                        cv2.putText(display, f"Target(Base): [{pos_target_mm[0]:.0f}, {pos_target_mm[1]:.0f}, {pos_target_mm[2]:.0f}]", 
                                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                        
                        if following:
                            # è·å–å½“å‰æœºæ¢°è‡‚ä½ç½®
                            T_gripper_base, q_curr = self.read_robot_pose()
                            pos_gripper_base = T_gripper_base[:3, 3]
                            
                            # è®¡ç®—è¯¯å·®å‘é‡
                            error_base = pos_target_base - pos_gripper_base
                            
                            # æ§åˆ¶é‡
                            delta_base = error_base * gain
                            if np.linalg.norm(delta_base) > step_limit:
                                delta_base = delta_base / np.linalg.norm(delta_base) * step_limit
                            
                            pos_gripper_des = pos_gripper_base + delta_base
                            
                            # IKæ±‚è§£
                            R_gripper_des = T_gripper_base[:3, :3] # ä¿æŒå½“å‰å§¿æ€
                            T_gripper_base_des = np.eye(4)
                            T_gripper_base_des[:3, :3] = R_gripper_des
                            T_gripper_base_des[:3, 3] = pos_gripper_des
                            
                            ik_res = self.robot.ikine_LM(
                                T_gripper_base_des, 
                                q0=q_curr,
                                mask=np.array([1, 1, 1, 0.5, 0.5, 0])
                            )
                            
                            if ik_res.success:
                                q_new = ik_res.q
                                if np.linalg.norm(q_new - q_curr) < 1.5:
                                    targets = self.robot.q_to_servo_targets(q_new)
                                    self.controller.fast_move_to_pose(targets, speed=200)
                                    cv2.putText(display, "TRACKING", (10, 90), 
                                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    else:
                        cv2.putText(display, "No Calibration", (10, 60), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # çŠ¶æ€æ˜¾ç¤º
                status = "FOLLOW ON" if following else "FOLLOW OFF (Press 'f')"
                cv2.putText(display, status, (10, display.shape[0]-30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0) if following else (0, 255, 255), 2)
                
                cv2.imshow("Eye-to-Hand Tracking", display)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('f'):
                    following = not following
                    print(f"Follow mode: {following}")
                elif key == ord('h'):
                    self.controller.move_all_home()
                    following = False
                    
        finally:
            cap.release()
            cv2.destroyAllWindows()


def main():
    tracker = BlueCircleTrackerEyeToHand(circle_diameter=0.05)
    tracker.run()


if __name__ == "__main__":
    main()
