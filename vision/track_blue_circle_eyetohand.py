#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è“è‰²åœ†å½¢æŠ“å–è„šæœ¬ (çœ¼åœ¨æ‰‹å¤– Eye-to-Hand)
=====================================
åŠŸèƒ½:
  1. æ£€æµ‹è“è‰²åœ†å½¢
  2. ä½¿ç”¨PnPè®¡ç®—åœ†å½¢åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®
  3. ç»“åˆçœ¼åœ¨æ‰‹å¤–æ ‡å®šç»“æœï¼Œè®¡ç®—åœ†å½¢åœ¨åŸºåº§åæ ‡ç³»ä¸‹çš„ä½ç½®
  4. æ§åˆ¶æœºæ¢°è‡‚æœ«ç«¯é è¿‘è“è‰²åœ†å½¢

ä½¿ç”¨æ–¹æ³•:
  python vision/track_blue_circle_eyetohand.py
"""

import sys
import os
import cv2
import numpy as np
import time
import yaml

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from driver.ftservo_controller import ServoController
from ik.robot import create_so101_5dof_gripper


class BlueCircleTrackerEyeToHand:
    """è“è‰²åœ†å½¢è·Ÿè¸ªå™¨ï¼ˆçœ¼åœ¨æ‰‹å¤–ç‰ˆï¼‰"""
    
    def __init__(self, circle_diameter=0.05):
        """
        Parameters
        ----------
        circle_diameter : float
            åœ†å½¢ç›´å¾„ (ç±³)ï¼Œé»˜è®¤ 5cm
        """
        self.circle_diameter = circle_diameter
        self.circle_radius = circle_diameter / 2.0
        
        # è·¯å¾„é…ç½®
        self.config_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config_data')
        self.intrinsic_file = os.path.join(self.config_dir, 'camera_intrinsics_environment.yaml')
        self.extrinsic_file = os.path.join(self.config_dir, 'handeye_result_envir.npy')
        
        # åŠ è½½å‚æ•°
        self.load_camera_intrinsics(self.intrinsic_file)
        self.load_handeye_calibration(self.extrinsic_file)
        
        # åˆå§‹åŒ–æœºå™¨äºº
        self.robot = None
        self.controller = None
        
        # HSV è“è‰²èŒƒå›´
        self.hsv_lower1 = np.array([100, 80, 80])
        self.hsv_upper1 = np.array([120, 255, 255])
        
        print("="*60)
        print("ğŸ”µ è“è‰²åœ†å½¢æŠ“å–ç³»ç»Ÿ (Eye-to-Hand)")
        print("="*60)
        print(f"åœ†å½¢ç›´å¾„: {circle_diameter*1000:.0f} mm")
        print("="*60)
    
    def load_camera_intrinsics(self, yaml_path):
        """åŠ è½½ç›¸æœºå†…å‚"""
        if not os.path.exists(yaml_path):
            print(f"âŒ æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
            # å°è¯•ä½¿ç”¨é»˜è®¤å€¼æˆ–æŠ›å‡ºå¼‚å¸¸
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç›¸æœºå†…å‚æ–‡ä»¶: {yaml_path}")
        
        try:
            fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
            
            # å°è¯•ä¸åŒçš„é”®å
            camera_matrix_node = fs.getNode('camera_matrix')
            if camera_matrix_node.empty():
                camera_matrix_node = fs.getNode('K')
            
            dist_coeffs_node = fs.getNode('distortion_coefficients')
            if dist_coeffs_node.empty():
                dist_coeffs_node = fs.getNode('distCoeffs')
            
            self.K = camera_matrix_node.mat()
            self.dist = dist_coeffs_node.mat().flatten()
            fs.release()
            
            print(f"ğŸ“· å·²åŠ è½½ç›¸æœºå†…å‚: {os.path.basename(yaml_path)}")
            print(f"   fx={self.K[0,0]:.1f}, fy={self.K[1,1]:.1f}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç›¸æœºå†…å‚å¤±è´¥: {e}")
            raise
    
    def load_handeye_calibration(self, npy_path):
        """åŠ è½½æ‰‹çœ¼æ ‡å®šç»“æœ (T_cam_base)"""
        if not os.path.exists(npy_path):
            print(f"âš ï¸ æœªæ‰¾åˆ°æ‰‹çœ¼æ ‡å®šæ–‡ä»¶: {npy_path}")
            print("   å°†æ— æ³•è¿›è¡Œåæ ‡è½¬æ¢")
            self.T_cam_base = None
        else:
            try:
                self.T_cam_base = np.load(npy_path)
                print(f"âœ… å·²åŠ è½½æ‰‹çœ¼æ ‡å®šå‚æ•°: {os.path.basename(npy_path)}")
                print(f"   T_cam_base:\n{self.T_cam_base}")
            except Exception as e:
                print(f"âŒ åŠ è½½æ‰‹çœ¼æ ‡å®šå‚æ•°å¤±è´¥: {e}")
                self.T_cam_base = None
    
    def init_robot(self, port="/dev/left_arm", baudrate=1_000_000):
        """åˆå§‹åŒ–æœºå™¨äºº"""
        print("\nğŸ¤– åˆå§‹åŒ–æœºå™¨äºº...")
        try:
            self.controller = ServoController(
                port=port, 
                baudrate=baudrate, 
                config_path=os.path.join(os.path.dirname(__file__), "../driver/servo_config.json")
            )
            self.robot = create_so101_5dof_gripper()
            self.robot.set_servo_controller(self.controller)
            print("âœ… æœºå™¨äººåˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ æœºå™¨äººåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def detect_blue_circle(self, frame):
        """æ£€æµ‹è“è‰²åœ†å½¢"""
        # è½¬æ¢åˆ°HSV
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # è“è‰²æ©ç 
        mask = cv2.inRange(hsv, self.hsv_lower1, self.hsv_upper1)
        
        # å½¢æ€å­¦å¤„ç†
        kernel = np.ones((7, 7), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        
        # éœå¤«åœ†æ£€æµ‹
        blurred = cv2.GaussianBlur(mask, (9, 9), 2)
        circles = cv2.HoughCircles(
            blurred,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=50,
            param1=50,
            param2=30,
            minRadius=15,
            maxRadius=250
        )
        
        if circles is not None:
            circles = np.round(circles[0, :]).astype(int)
            best_circle = max(circles, key=lambda c: c[2])
            center = (best_circle[0], best_circle[1])
            radius = best_circle[2]
            return True, center, radius, mask
        
        # è½®å»“æ£€æµ‹å¤‡é€‰
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)
            if area > 800:
                perimeter = cv2.arcLength(largest_contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    if circularity > 0.7:
                        (cx, cy), radius = cv2.minEnclosingCircle(largest_contour)
                        return True, (int(cx), int(cy)), int(radius), mask
        
        return False, None, None, mask
    
    def estimate_pose_from_circle(self, center, radius_px):
        """ä»åœ†å½¢çš„åƒç´ åæ ‡å’ŒåŠå¾„ä¼°ç®—3Dä½ç½® (ç›¸æœºåæ ‡ç³»)"""
        fx = self.K[0, 0]
        fy = self.K[1, 1]
        cx = self.K[0, 2]
        cy = self.K[1, 2]
        
        # ä¼°ç®—æ·±åº¦ Z = f * R / r
        f = (fx + fy) / 2
        Z = f * self.circle_radius / radius_px
        
        # åæŠ•å½±
        u, v = center
        X = (u - cx) * Z / fx
        Y = (v - cy) * Z / fy
        
        return np.array([X, Y, Z])
    
    def read_robot_pose(self, verbose=False):
        """è¯»å–æœºå™¨äººå½“å‰æœ«ç«¯ä½å§¿"""
        q = self.robot.read_joint_angles(
            joint_names=self.robot.joint_names,
            verbose=verbose
        )
        T_gripper_base = self.robot.fkine(q)
        return T_gripper_base, q
    
    def run(self):
        """è¿è¡Œè·Ÿè¸ªå¾ªç¯"""
        if not self.init_robot():
            return
        
        # æ‰“å¼€ç›¸æœº (é€šå¸¸çœ¼åœ¨æ‰‹å¤–ä½¿ç”¨ä¸åŒçš„ç›¸æœºIDï¼Œè¿™é‡Œå‡è®¾ä¸º0ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´)
        cap = cv2.VideoCapture(0) 
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº 0ï¼Œå°è¯•ç›¸æœº 2...")
            cap = cv2.VideoCapture(2)
            if not cap.isOpened():
                print("âŒ æ— æ³•æ‰“å¼€ç›¸æœº")
                return

        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        print("\nğŸ® æ§åˆ¶è¯´æ˜:")
        print("  'f' - å¼€å¯/å…³é—­ è·Ÿéšæ¨¡å¼")
        print("  'h' - æœºæ¢°è‡‚å›ä¸­")
        print("  'q' - é€€å‡º")
        print("="*60)
        
        following = False
        gain = 0.8
        step_limit = 0.080
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                display = frame.copy()
                
                # æ£€æµ‹è“è‰²åœ†å½¢
                success, center, radius_px, mask = self.detect_blue_circle(frame)
                
                if success:
                    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
                    cv2.circle(display, center, radius_px, (0, 255, 0), 2)
                    cv2.circle(display, center, 3, (0, 255, 255), -1)
                    
                    # 1. è®¡ç®—ç›¸æœºåæ ‡ç³»ä¸‹çš„ä½ç½®
                    pos_cam = self.estimate_pose_from_circle(center, radius_px)
                    pos_cam_mm = pos_cam * 1000
                    
                    # æ‰“å° PnP ç»“æœ
                    print(f"PnP (Cam): [{pos_cam_mm[0]:6.1f}, {pos_cam_mm[1]:6.1f}, {pos_cam_mm[2]:6.1f}] mm")
                    
                    cv2.putText(display, f"PnP(Cam): [{pos_cam_mm[0]:.0f}, {pos_cam_mm[1]:.0f}, {pos_cam_mm[2]:.0f}]", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    
                    # 2. è½¬æ¢åˆ°åŸºåº§åæ ‡ç³» (Eye-to-Hand)
                    if self.T_cam_base is not None:
                        # P_base = T_cam_base @ P_cam
                        pos_cam_homo = np.append(pos_cam, 1.0)
                        pos_target_base = (self.T_cam_base @ pos_cam_homo)[:3]
                        pos_target_mm = pos_target_base * 1000
                        
                        # æ‰“å°ç›®æ ‡ä½å§¿
                        print(f"Target (Base): [{pos_target_mm[0]:6.1f}, {pos_target_mm[1]:6.1f}, {pos_target_mm[2]:6.1f}] mm")
                        
                        cv2.putText(display, f"Target(Base): [{pos_target_mm[0]:.0f}, {pos_target_mm[1]:.0f}, {pos_target_mm[2]:.0f}]", 
                                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                        
                        if following:
                            # è·å–å½“å‰æœºæ¢°è‡‚ä½ç½®
                            T_gripper_base, q_curr = self.read_robot_pose()
                            pos_gripper_base = T_gripper_base[:3, 3]
                            
                            # è®¡ç®—è¯¯å·®å‘é‡
                            error_base = pos_target_base - pos_gripper_base
                            
                            # æ§åˆ¶é‡
                            delta_base = error_base * gain
                            if np.linalg.norm(delta_base) > step_limit:
                                delta_base = delta_base / np.linalg.norm(delta_base) * step_limit
                            
                            pos_gripper_des = pos_gripper_base + delta_base
                            
                            # IKæ±‚è§£
                            R_gripper_des = T_gripper_base[:3, :3] # ä¿æŒå½“å‰å§¿æ€
                            T_gripper_base_des = np.eye(4)
                            T_gripper_base_des[:3, :3] = R_gripper_des
                            T_gripper_base_des[:3, 3] = pos_gripper_des
                            
                            ik_res = self.robot.ikine_LM(
                                T_gripper_base_des, 
                                q0=q_curr,
                                mask=np.array([1, 1, 1, 0.5, 0.5, 0])
                            )
                            
                            if ik_res.success:
                                q_new = ik_res.q
                                if np.linalg.norm(q_new - q_curr) < 1.5:
                                    targets = self.robot.q_to_servo_targets(q_new)
                                    self.controller.fast_move_to_pose(targets, speed=200)
                                    cv2.putText(display, "TRACKING", (10, 90), 
                                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    else:
                        cv2.putText(display, "No Calibration", (10, 60), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # çŠ¶æ€æ˜¾ç¤º
                status = "FOLLOW ON" if following else "FOLLOW OFF (Press 'f')"
                cv2.putText(display, status, (10, display.shape[0]-30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0) if following else (0, 255, 255), 2)
                
                cv2.imshow("Eye-to-Hand Tracking", display)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('f'):
                    following = not following
                    print(f"Follow mode: {following}")
                elif key == ord('h'):
                    self.controller.move_all_home()
                    following = False
                    
        finally:
            cap.release()
            cv2.destroyAllWindows()


def main():
    tracker = BlueCircleTrackerEyeToHand(circle_diameter=0.05)
    tracker.run()


if __name__ == "__main__":
    main()
